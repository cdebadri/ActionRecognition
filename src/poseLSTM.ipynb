{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "poseLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2NPWvSqPlaR",
        "colab_type": "code",
        "outputId": "04023f40-bc4d-44d8-a4fd-e9029d423db0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!ls \"/content/drive/My Drive\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n",
            " BFS.cpp\t\t\t     'poseLSTM2 (2).index'\n",
            " BI2\t\t\t\t     'poseLSTM2 (2).meta'\n",
            " checkpoint\t\t\t      poseLSTM2.data-00000-of-00001\n",
            "'checkpoint (1)'\t\t      poseLSTM2.index\n",
            "'checkpoint (2)'\t\t      poseLSTM2.meta\n",
            "'Colab Notebooks'\t\t      poseLSTM2.pb\n",
            " data3\t\t\t\t      poseLSTM.data-00000-of-00001\n",
            " eyes\t\t\t\t      poseLSTM_model\n",
            "'poseLSTM2 (1).data-00000-of-00001'   RNN-HAR-2D-Pose-database.zip\n",
            "'poseLSTM2 (1).index'\t\t      train_supervised_pose.csv\n",
            "'poseLSTM2 (1).meta'\t\t      train_unsupervised_pose.tsv\n",
            "'poseLSTM2 (1).pb'\t\t      validation_supervised_pose.csv\n",
            "'poseLSTM2 (2).data-00000-of-00001'   validation_unsupervised_pose.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0unkoS2kR3FD",
        "colab_type": "code",
        "outputId": "72bec4e6-61ea-4e6e-fb6d-22db1daf8ebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "!unzip \"/content/drive/My Drive/RNN-HAR-2D-Pose-database.zip\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/RNN-HAR-2D-Pose-database.zip\n",
            "   creating: RNN-HAR-2D-Pose-database/\n",
            "  inflating: RNN-HAR-2D-Pose-database/README.md  \n",
            "  inflating: RNN-HAR-2D-Pose-database/X_val2.txt  \n",
            "  inflating: RNN-HAR-2D-Pose-database/X_val.txt  \n",
            "  inflating: RNN-HAR-2D-Pose-database/Y_train.txt  \n",
            "  inflating: RNN-HAR-2D-Pose-database/Y_test.txt  \n",
            "  inflating: RNN-HAR-2D-Pose-database/X_train.txt  \n",
            "  inflating: RNN-HAR-2D-Pose-database/X_test.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRjgZ8IHPPpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
        "from sklearn import metrics\n",
        "import random\n",
        "from random import randint\n",
        "import time\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ghEY_0oPPp1",
        "colab_type": "text"
      },
      "source": [
        "## Preparing dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1I1wTMxPPp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Useful Constants\n",
        "\n",
        "# Output classes to learn how to classify\n",
        "LABELS = [    \n",
        "    \"JUMPING\",\n",
        "    \"JUMPING_JACKS\",\n",
        "    \"BOXING\",\n",
        "    \"WAVING_2HANDS\",\n",
        "    \"WAVING_1HAND\",\n",
        "    \"CLAPPING_HANDS\"\n",
        "\n",
        "] \n",
        "DATASET_PATH = \"RNN-HAR-2D-Pose-database/\"\n",
        "\n",
        "X_train_path = DATASET_PATH + \"X_train.txt\"\n",
        "X_test_path = DATASET_PATH + \"X_test.txt\"\n",
        "\n",
        "y_train_path = DATASET_PATH + \"Y_train.txt\"\n",
        "y_test_path = DATASET_PATH + \"Y_test.txt\"\n",
        "\n",
        "n_steps = 32 # 32 timesteps per series"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piJXHD9iPPp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Load the networks inputs\n",
        "\n",
        "def load_X(X_path):\n",
        "    file = open(X_path, 'r')\n",
        "    X_ = np.array(\n",
        "        [elem for elem in [\n",
        "            row.split(',') for row in file\n",
        "        ]], \n",
        "        dtype=np.float32\n",
        "    )\n",
        "    file.close()\n",
        "    blocks = int(len(X_) / n_steps)\n",
        "    \n",
        "    X_ = np.array(np.split(X_,blocks))\n",
        "\n",
        "    return X_ \n",
        "\n",
        "# Load the networks outputs\n",
        "\n",
        "def load_y(y_path):\n",
        "    file = open(y_path, 'r')\n",
        "    y_ = np.array(\n",
        "        [elem for elem in [\n",
        "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "        ]], \n",
        "        dtype=np.int32\n",
        "    )\n",
        "    file.close()\n",
        "    \n",
        "    # for 0-based indexing \n",
        "    return y_ - 1\n",
        "\n",
        "X_train = load_X(X_train_path)\n",
        "X_test = load_X(X_test_path)\n",
        "#print X_test\n",
        "\n",
        "y_train = load_y(y_train_path)\n",
        "y_test = load_y(y_test_path)\n",
        "# proof that it actually works for the skeptical: replace labelled classes with random classes to train on\n",
        "#for i in range(len(y_train)):\n",
        "#    y_train[i] = randint(0, 5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tqk7CrbPPqD",
        "colab_type": "text"
      },
      "source": [
        "## Set Parameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N28TvyJPPPqF",
        "colab_type": "code",
        "outputId": "e091ad05-353d-426c-b618-d310ba8c2cbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "# Input Data \n",
        "\n",
        "training_data_count = len(X_train)  # 4519 training series (with 50% overlap between each serie)\n",
        "test_data_count = len(X_test)  # 1197 test series\n",
        "n_input = len(X_train[0][0])  # num input parameters per timestep\n",
        "\n",
        "n_hidden = 34 # Hidden layer num of features\n",
        "n_classes = 6 \n",
        "\n",
        "#updated for learning-rate decay\n",
        "# calculated as: decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
        "decaying_learning_rate = True\n",
        "learning_rate = 0.0025 #used if decaying_learning_rate set to False\n",
        "init_learning_rate = 0.005\n",
        "decay_rate = 0.96 #the base of the exponential in the decay\n",
        "decay_steps = 100000 #used in decay every 60000 steps with a base of 0.96\n",
        "\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "lambda_loss_amount = 0.0015\n",
        "\n",
        "training_iters = training_data_count *300  # Loop 300 times on the dataset, ie 300 epochs\n",
        "batch_size = 512\n",
        "display_iter = batch_size*8  # To show test set accuracy during training\n",
        "\n",
        "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
        "print(X_train.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
        "print(\"\\nThe dataset has not been preprocessed, is not normalised etc\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "(X shape, y shape, every X's mean, every X's standard deviation)\n",
            "((22625, 32, 36), (5751, 1), 251.01117, 126.12204)\n",
            "\n",
            "The dataset has not been preprocessed, is not normalised etc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-K-KEVWPPqQ",
        "colab_type": "text"
      },
      "source": [
        "## Utility functions for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpz7E9pzPPqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LSTM_RNN(_X, _weights, _biases):\n",
        "    # model architecture based on \"guillaume-chevalier\" and \"aymericdamien\" under the MIT license.\n",
        "\n",
        "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
        "    _X = tf.reshape(_X, [-1, n_input])   \n",
        "    # Rectifies Linear Unit activation function used\n",
        "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
        "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
        "    _X = tf.split(_X, n_steps, 0) \n",
        "\n",
        "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
        "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
        "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
        "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
        "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
        "\n",
        "    # A single output is produced, in style of \"many to one\" classifier, refer to http://karpathy.github.io/2015/05/21/rnn-effectiveness/ for details\n",
        "    lstm_last_output = outputs[-1]\n",
        "    \n",
        "    # Linear activation\n",
        "    return tf.add(tf.matmul(lstm_last_output, _weights['out']), _biases['out'], name='output')\n",
        "\n",
        "\n",
        "def extract_batch_size(_train, _labels, _unsampled, batch_size):\n",
        "    # Fetch a \"batch_size\" amount of data and labels from \"(X|y)_train\" data. \n",
        "    # Elements of each batch are chosen randomly, without replacement, from X_train with corresponding label from Y_train\n",
        "    # unsampled_indices keeps track of sampled data ensuring non-replacement. Resets when remaining datapoints < batch_size    \n",
        "    \n",
        "    shape = list(_train.shape)\n",
        "    shape[0] = batch_size\n",
        "    batch_s = np.empty(shape)\n",
        "    batch_labels = np.empty((batch_size,1)) \n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Loop index\n",
        "        # index = random sample from _unsampled (indices)\n",
        "        index = random.choice(_unsampled)\n",
        "        batch_s[i] = _train[index] \n",
        "        batch_labels[i] = _labels[index]\n",
        "        _unsampled.remove(index)\n",
        "\n",
        "\n",
        "    return batch_s, batch_labels, _unsampled\n",
        "\n",
        "\n",
        "def one_hot(y_):\n",
        "    # One hot encoding of the network outputs\n",
        "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
        "    \n",
        "    y_ = y_.reshape(len(y_))\n",
        "    n_values = int(np.max(y_)) + 1\n",
        "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M93XcSfwPPqY",
        "colab_type": "text"
      },
      "source": [
        "## Build the network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D05nnl_OPPqa",
        "colab_type": "code",
        "outputId": "647e29dc-b74b-456f-b07d-31dc67d2d60b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "source": [
        "\n",
        "# Graph input/output\n",
        "x = tf.placeholder(tf.float32, [None, n_steps, n_input], name='input')\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "# Graph weights\n",
        "weights = {\n",
        "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
        "}\n",
        "biases = {\n",
        "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "pred = LSTM_RNN(x, weights, biases)\n",
        "\n",
        "# Loss, optimizer and evaluation\n",
        "l2 = lambda_loss_amount * sum(\n",
        "    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
        ") # L2 loss prevents this overkill neural network to overfit the data\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\n",
        "if decaying_learning_rate:\n",
        "    learning_rate = tf.train.exponential_decay(init_learning_rate, global_step*batch_size, decay_steps, decay_rate, staircase=True)\n",
        "\n",
        "\n",
        "#decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps) #exponentially decayed learning rate\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step=global_step) # Adam Optimizer\n",
        "\n",
        "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1), name='prediction')\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-7-8dee4e0301f6>:12: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-7-8dee4e0301f6>:14: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-7-8dee4e0301f6>:15: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From <ipython-input-8-f213a2d7b153>:20: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfBvVBk8PPqg",
        "colab_type": "text"
      },
      "source": [
        "## Train the network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "1rJE_CEePPqi",
        "colab_type": "code",
        "outputId": "75eff57d-22d4-4ae4-f7b8-902ccd32f745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56440
        }
      },
      "source": [
        "test_losses = []\n",
        "test_accuracies = []\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "# Perform Training steps with \"batch_size\" amount of data at each loop. \n",
        "# Elements of each batch are chosen randomly, without replacement, from X_train, \n",
        "# restarting when remaining datapoints < batch_size\n",
        "step = 1\n",
        "time_start = time.time()\n",
        "unsampled_indices = range(0,len(X_train))\n",
        "\n",
        "while step * batch_size <= training_iters:\n",
        "    #print (sess.run(learning_rate)) #decaying learning rate\n",
        "    #print (sess.run(global_step)) # global number of iterations\n",
        "    if len(unsampled_indices) < batch_size:\n",
        "        unsampled_indices = range(0,len(X_train)) \n",
        "    batch_xs, raw_labels, unsampled_indicies = extract_batch_size(X_train, y_train, unsampled_indices, batch_size)\n",
        "    batch_ys = one_hot(raw_labels)\n",
        "    # check that encoded output is same length as num_classes, if not, pad it \n",
        "    if len(batch_ys[0]) < n_classes:\n",
        "        temp_ys = np.zeros((batch_size, n_classes))\n",
        "        temp_ys[:batch_ys.shape[0],:batch_ys.shape[1]] = batch_ys\n",
        "        batch_ys = temp_ys\n",
        "       \n",
        "    \n",
        "\n",
        "    # Fit training using batch data\n",
        "    _, loss, acc = sess.run(\n",
        "        [optimizer, cost, accuracy],\n",
        "        feed_dict={\n",
        "            x: batch_xs, \n",
        "            y: batch_ys\n",
        "        }\n",
        "    )\n",
        "    train_losses.append(loss)\n",
        "    train_accuracies.append(acc)\n",
        "    \n",
        "    # Evaluate network only at some steps for faster training: \n",
        "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
        "        \n",
        "        # To not spam console, show training accuracy/loss in this \"if\"\n",
        "        print(\"Iter #\" + str(step*batch_size) + \\\n",
        "              \":  Learning rate = \" + \"{:.6f}\".format(sess.run(learning_rate)) + \\\n",
        "              \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
        "              \", Accuracy = {}\".format(acc))\n",
        "        \n",
        "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
        "        loss, acc = sess.run(\n",
        "            [cost, accuracy], \n",
        "            feed_dict={\n",
        "                x: X_test,\n",
        "                y: one_hot(y_test)\n",
        "            }\n",
        "        )\n",
        "        test_losses.append(loss)\n",
        "        test_accuracies.append(acc)\n",
        "        print(\"PERFORMANCE ON TEST SET:             \" + \\\n",
        "              \"Batch Loss = {}\".format(loss) + \\\n",
        "              \", Accuracy = {}\".format(acc))\n",
        "\n",
        "    step += 1\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "\n",
        "# Accuracy for test data\n",
        "\n",
        "one_hot_predictions, accuracy, final_loss = sess.run(\n",
        "    [pred, accuracy, cost],\n",
        "    feed_dict={\n",
        "        x: X_test,\n",
        "        y: one_hot(y_test)\n",
        "    }\n",
        ")\n",
        "\n",
        "test_losses.append(final_loss)\n",
        "test_accuracies.append(accuracy)\n",
        "\n",
        "print(\"FINAL RESULT: \" + \\\n",
        "      \"Batch Loss = {}\".format(final_loss) + \\\n",
        "      \", Accuracy = {}\".format(accuracy))\n",
        "time_stop = time.time()\n",
        "print(\"TOTAL TIME:  {}\".format(time_stop - time_start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter #512:  Learning rate = 0.005000:   Batch Loss = 4.242367, Accuracy = 0.13671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 3.44947385788, Accuracy = 0.160319939256\n",
            "Iter #4096:  Learning rate = 0.005000:   Batch Loss = 2.985750, Accuracy = 0.287109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 3.07006788254, Accuracy = 0.25699877739\n",
            "Iter #8192:  Learning rate = 0.005000:   Batch Loss = 2.888960, Accuracy = 0.326171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.88224387169, Accuracy = 0.371065914631\n",
            "Iter #12288:  Learning rate = 0.005000:   Batch Loss = 2.830805, Accuracy = 0.306640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.84467601776, Accuracy = 0.290558159351\n",
            "Iter #16384:  Learning rate = 0.005000:   Batch Loss = 2.647563, Accuracy = 0.447265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.6444606781, Accuracy = 0.432098776102\n",
            "Iter #20480:  Learning rate = 0.005000:   Batch Loss = 2.513466, Accuracy = 0.462890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.55735301971, Accuracy = 0.427751690149\n",
            "Iter #24576:  Learning rate = 0.005000:   Batch Loss = 2.437603, Accuracy = 0.50390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.57814002037, Accuracy = 0.413319408894\n",
            "Iter #28672:  Learning rate = 0.005000:   Batch Loss = 2.321351, Accuracy = 0.513671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.3308672905, Accuracy = 0.482350885868\n",
            "Iter #32768:  Learning rate = 0.005000:   Batch Loss = 2.217325, Accuracy = 0.51953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.31229782104, Accuracy = 0.505998969078\n",
            "Iter #36864:  Learning rate = 0.005000:   Batch Loss = 2.556655, Accuracy = 0.400390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.51740312576, Accuracy = 0.447574347258\n",
            "Iter #40960:  Learning rate = 0.005000:   Batch Loss = 2.414219, Accuracy = 0.435546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.4278087616, Accuracy = 0.440097361803\n",
            "Iter #45056:  Learning rate = 0.005000:   Batch Loss = 2.156863, Accuracy = 0.55859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.20695662498, Accuracy = 0.503390729427\n",
            "Iter #49152:  Learning rate = 0.005000:   Batch Loss = 2.111361, Accuracy = 0.55859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.09223413467, Accuracy = 0.56459748745\n",
            "Iter #53248:  Learning rate = 0.005000:   Batch Loss = 1.924275, Accuracy = 0.603515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.88803744316, Accuracy = 0.620761632919\n",
            "Iter #57344:  Learning rate = 0.005000:   Batch Loss = 2.057607, Accuracy = 0.521484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.86708188057, Accuracy = 0.63293337822\n",
            "Iter #61440:  Learning rate = 0.005000:   Batch Loss = 1.903608, Accuracy = 0.625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.95951724052, Accuracy = 0.619196653366\n",
            "Iter #65536:  Learning rate = 0.005000:   Batch Loss = 2.390187, Accuracy = 0.419921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.67693161964, Accuracy = 0.4293166399\n",
            "Iter #69632:  Learning rate = 0.005000:   Batch Loss = 2.198395, Accuracy = 0.50390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.15735578537, Accuracy = 0.501304149628\n",
            "Iter #73728:  Learning rate = 0.005000:   Batch Loss = 2.021396, Accuracy = 0.5703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.03150320053, Accuracy = 0.543035984039\n",
            "Iter #77824:  Learning rate = 0.005000:   Batch Loss = 1.885471, Accuracy = 0.6015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.90397024155, Accuracy = 0.614849567413\n",
            "Iter #81920:  Learning rate = 0.005000:   Batch Loss = 1.931477, Accuracy = 0.603515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.92202246189, Accuracy = 0.589462697506\n",
            "Iter #86016:  Learning rate = 0.005000:   Batch Loss = 2.026143, Accuracy = 0.51953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.90101194382, Accuracy = 0.581290185452\n",
            "Iter #90112:  Learning rate = 0.005000:   Batch Loss = 2.157836, Accuracy = 0.43359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.26175069809, Accuracy = 0.420274734497\n",
            "Iter #94208:  Learning rate = 0.005000:   Batch Loss = 1.866058, Accuracy = 0.6171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.88163971901, Accuracy = 0.601634502411\n",
            "Iter #98304:  Learning rate = 0.005000:   Batch Loss = 1.663836, Accuracy = 0.658203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.69526934624, Accuracy = 0.651017189026\n",
            "Iter #102400:  Learning rate = 0.004800:   Batch Loss = 1.651183, Accuracy = 0.669921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.08424949646, Accuracy = 0.499913066626\n",
            "Iter #106496:  Learning rate = 0.004800:   Batch Loss = 1.696405, Accuracy = 0.619140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.73253893852, Accuracy = 0.636237204075\n",
            "Iter #110592:  Learning rate = 0.004800:   Batch Loss = 1.994980, Accuracy = 0.564453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.82219719887, Accuracy = 0.594157516956\n",
            "Iter #114688:  Learning rate = 0.004800:   Batch Loss = 1.779035, Accuracy = 0.62109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.77788710594, Accuracy = 0.621631026268\n",
            "Iter #118784:  Learning rate = 0.004800:   Batch Loss = 1.675571, Accuracy = 0.654296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.65503239632, Accuracy = 0.65310382843\n",
            "Iter #122880:  Learning rate = 0.004800:   Batch Loss = 1.600163, Accuracy = 0.693359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.63637602329, Accuracy = 0.669274926186\n",
            "Iter #126976:  Learning rate = 0.004800:   Batch Loss = 1.552112, Accuracy = 0.689453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.54772758484, Accuracy = 0.718831479549\n",
            "Iter #131072:  Learning rate = 0.004800:   Batch Loss = 1.986201, Accuracy = 0.578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.17330479622, Accuracy = 0.555381655693\n",
            "Iter #135168:  Learning rate = 0.004800:   Batch Loss = 1.882881, Accuracy = 0.541015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.84675312042, Accuracy = 0.59363591671\n",
            "Iter #139264:  Learning rate = 0.004800:   Batch Loss = 1.910949, Accuracy = 0.5390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.80513870716, Accuracy = 0.620413839817\n",
            "Iter #143360:  Learning rate = 0.004800:   Batch Loss = 1.703063, Accuracy = 0.662109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.70006132126, Accuracy = 0.646496236324\n",
            "Iter #147456:  Learning rate = 0.004800:   Batch Loss = 1.651125, Accuracy = 0.65625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.61138248444, Accuracy = 0.681968331337\n",
            "Iter #151552:  Learning rate = 0.004800:   Batch Loss = 1.551593, Accuracy = 0.669921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 3.31734704971, Accuracy = 0.339593112469\n",
            "Iter #155648:  Learning rate = 0.004800:   Batch Loss = 1.812881, Accuracy = 0.55078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.86474704742, Accuracy = 0.568075120449\n",
            "Iter #159744:  Learning rate = 0.004800:   Batch Loss = 2.115825, Accuracy = 0.439453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.98218274117, Accuracy = 0.500260829926\n",
            "Iter #163840:  Learning rate = 0.004800:   Batch Loss = 1.774225, Accuracy = 0.603515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.82638335228, Accuracy = 0.553121209145\n",
            "Iter #167936:  Learning rate = 0.004800:   Batch Loss = 1.698678, Accuracy = 0.59765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.62152171135, Accuracy = 0.654321014881\n",
            "Iter #172032:  Learning rate = 0.004800:   Batch Loss = 1.554278, Accuracy = 0.673828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.53796744347, Accuracy = 0.69170576334\n",
            "Iter #176128:  Learning rate = 0.004800:   Batch Loss = 1.618948, Accuracy = 0.65234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.65112400055, Accuracy = 0.622500419617\n",
            "Iter #180224:  Learning rate = 0.004800:   Batch Loss = 1.681467, Accuracy = 0.615234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.83457362652, Accuracy = 0.57398712635\n",
            "Iter #184320:  Learning rate = 0.004800:   Batch Loss = 1.676969, Accuracy = 0.60546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.68703043461, Accuracy = 0.600591182709\n",
            "Iter #188416:  Learning rate = 0.004800:   Batch Loss = 1.587571, Accuracy = 0.64453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.53006792068, Accuracy = 0.679707884789\n",
            "Iter #192512:  Learning rate = 0.004800:   Batch Loss = 1.559365, Accuracy = 0.623046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.49271535873, Accuracy = 0.676925778389\n",
            "Iter #196608:  Learning rate = 0.004800:   Batch Loss = 1.452676, Accuracy = 0.6953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4764111042, Accuracy = 0.692401349545\n",
            "Iter #200704:  Learning rate = 0.004608:   Batch Loss = 1.353261, Accuracy = 0.751953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.42137360573, Accuracy = 0.718483746052\n",
            "Iter #204800:  Learning rate = 0.004608:   Batch Loss = 1.354682, Accuracy = 0.71875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.34580397606, Accuracy = 0.745435595512\n",
            "Iter #208896:  Learning rate = 0.004608:   Batch Loss = 1.336572, Accuracy = 0.701171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.39271116257, Accuracy = 0.722135305405\n",
            "Iter #212992:  Learning rate = 0.004608:   Batch Loss = 1.273628, Accuracy = 0.767578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.29230117798, Accuracy = 0.760737240314\n",
            "Iter #217088:  Learning rate = 0.004608:   Batch Loss = 1.276049, Accuracy = 0.796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.27284753323, Accuracy = 0.797948181629\n",
            "Iter #221184:  Learning rate = 0.004608:   Batch Loss = 1.279032, Accuracy = 0.76171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.21533966064, Accuracy = 0.784037530422\n",
            "Iter #225280:  Learning rate = 0.004608:   Batch Loss = 1.194344, Accuracy = 0.818359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.22109937668, Accuracy = 0.806294560432\n",
            "Iter #229376:  Learning rate = 0.004608:   Batch Loss = 1.101163, Accuracy = 0.830078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.19918704033, Accuracy = 0.788036882877\n",
            "Iter #233472:  Learning rate = 0.004608:   Batch Loss = 1.202758, Accuracy = 0.779296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.19315099716, Accuracy = 0.804034054279\n",
            "Iter #237568:  Learning rate = 0.004608:   Batch Loss = 1.154832, Accuracy = 0.837890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.13023757935, Accuracy = 0.840375602245\n",
            "Iter #241664:  Learning rate = 0.004608:   Batch Loss = 1.100253, Accuracy = 0.84765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1801431179, Accuracy = 0.807859480381\n",
            "Iter #245760:  Learning rate = 0.004608:   Batch Loss = 1.097060, Accuracy = 0.837890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.22238326073, Accuracy = 0.757955133915\n",
            "Iter #249856:  Learning rate = 0.004608:   Batch Loss = 1.090356, Accuracy = 0.841796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.21550309658, Accuracy = 0.800904214382\n",
            "Iter #253952:  Learning rate = 0.004608:   Batch Loss = 1.062110, Accuracy = 0.845703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.12480330467, Accuracy = 0.817075312138\n",
            "Iter #258048:  Learning rate = 0.004608:   Batch Loss = 1.117810, Accuracy = 0.826171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.11370944977, Accuracy = 0.828377664089\n",
            "Iter #262144:  Learning rate = 0.004608:   Batch Loss = 1.231640, Accuracy = 0.751953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.16572284698, Accuracy = 0.781777083874\n",
            "Iter #266240:  Learning rate = 0.004608:   Batch Loss = 1.066343, Accuracy = 0.8515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.06639957428, Accuracy = 0.837941229343\n",
            "Iter #270336:  Learning rate = 0.004608:   Batch Loss = 0.994494, Accuracy = 0.873046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.05390226841, Accuracy = 0.847330927849\n",
            "Iter #274432:  Learning rate = 0.004608:   Batch Loss = 1.026102, Accuracy = 0.865234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.049908638, Accuracy = 0.865066945553\n",
            "Iter #278528:  Learning rate = 0.004608:   Batch Loss = 1.023549, Accuracy = 0.84765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.09243106842, Accuracy = 0.827856004238\n",
            "Iter #282624:  Learning rate = 0.004608:   Batch Loss = 1.070055, Accuracy = 0.859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.00379061699, Accuracy = 0.873587191105\n",
            "Iter #286720:  Learning rate = 0.004608:   Batch Loss = 1.159481, Accuracy = 0.806640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.33702754974, Accuracy = 0.739349663258\n",
            "Iter #290816:  Learning rate = 0.004608:   Batch Loss = 1.129097, Accuracy = 0.8203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.15812933445, Accuracy = 0.80664229393\n",
            "Iter #294912:  Learning rate = 0.004608:   Batch Loss = 1.097351, Accuracy = 0.830078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.07575511932, Accuracy = 0.828377664089\n",
            "Iter #299008:  Learning rate = 0.004608:   Batch Loss = 0.990638, Accuracy = 0.87890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.10622000694, Accuracy = 0.840897262096\n",
            "Iter #303104:  Learning rate = 0.004424:   Batch Loss = 0.974445, Accuracy = 0.880859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.096981287, Accuracy = 0.837941229343\n",
            "Iter #307200:  Learning rate = 0.004424:   Batch Loss = 1.099511, Accuracy = 0.806640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.17384707928, Accuracy = 0.786993563175\n",
            "Iter #311296:  Learning rate = 0.004424:   Batch Loss = 1.076048, Accuracy = 0.85546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.07875955105, Accuracy = 0.832898616791\n",
            "Iter #315392:  Learning rate = 0.004424:   Batch Loss = 0.977923, Accuracy = 0.880859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.00063931942, Accuracy = 0.874282717705\n",
            "Iter #319488:  Learning rate = 0.004424:   Batch Loss = 0.926784, Accuracy = 0.904296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.00832939148, Accuracy = 0.871674478054\n",
            "Iter #323584:  Learning rate = 0.004424:   Batch Loss = 0.955308, Accuracy = 0.890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.01396012306, Accuracy = 0.8621109128\n",
            "Iter #327680:  Learning rate = 0.004424:   Batch Loss = 0.973324, Accuracy = 0.8671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1534409523, Accuracy = 0.83237695694\n",
            "Iter #331776:  Learning rate = 0.004424:   Batch Loss = 0.951865, Accuracy = 0.90234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.984623908997, Accuracy = 0.881411910057\n",
            "Iter #335872:  Learning rate = 0.004424:   Batch Loss = 0.894833, Accuracy = 0.9140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.980202674866, Accuracy = 0.862284839153\n",
            "Iter #339968:  Learning rate = 0.004424:   Batch Loss = 0.989031, Accuracy = 0.87109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.01636123657, Accuracy = 0.866805791855\n",
            "Iter #344064:  Learning rate = 0.004424:   Batch Loss = 0.899623, Accuracy = 0.8984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.959689736366, Accuracy = 0.873239457607\n",
            "Iter #348160:  Learning rate = 0.004424:   Batch Loss = 0.921366, Accuracy = 0.88671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.973463058472, Accuracy = 0.869240105152\n",
            "Iter #352256:  Learning rate = 0.004424:   Batch Loss = 0.908970, Accuracy = 0.900390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.943603336811, Accuracy = 0.88436794281\n",
            "Iter #356352:  Learning rate = 0.004424:   Batch Loss = 0.981182, Accuracy = 0.873046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.953048110008, Accuracy = 0.880890250206\n",
            "Iter #360448:  Learning rate = 0.004424:   Batch Loss = 0.882738, Accuracy = 0.9140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.913032889366, Accuracy = 0.900712907314\n",
            "Iter #364544:  Learning rate = 0.004424:   Batch Loss = 0.911958, Accuracy = 0.8984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.7071647644, Accuracy = 0.68701094389\n",
            "Iter #368640:  Learning rate = 0.004424:   Batch Loss = 1.199200, Accuracy = 0.78515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.40724313259, Accuracy = 0.727003991604\n",
            "Iter #372736:  Learning rate = 0.004424:   Batch Loss = 1.021499, Accuracy = 0.83203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.09065926075, Accuracy = 0.802121341228\n",
            "Iter #376832:  Learning rate = 0.004424:   Batch Loss = 0.938414, Accuracy = 0.892578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.974402785301, Accuracy = 0.874804377556\n",
            "Iter #380928:  Learning rate = 0.004424:   Batch Loss = 0.946503, Accuracy = 0.875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.976883888245, Accuracy = 0.875673770905\n",
            "Iter #385024:  Learning rate = 0.004424:   Batch Loss = 0.875743, Accuracy = 0.9140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.913198053837, Accuracy = 0.894279241562\n",
            "Iter #389120:  Learning rate = 0.004424:   Batch Loss = 0.977598, Accuracy = 0.8515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.01722788811, Accuracy = 0.849069714546\n",
            "Iter #393216:  Learning rate = 0.004424:   Batch Loss = 0.962707, Accuracy = 0.884765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.967044830322, Accuracy = 0.868196845055\n",
            "Iter #397312:  Learning rate = 0.004424:   Batch Loss = 0.983070, Accuracy = 0.896484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.939595401287, Accuracy = 0.894800901413\n",
            "Iter #401408:  Learning rate = 0.004247:   Batch Loss = 0.900346, Accuracy = 0.921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.964485824108, Accuracy = 0.876717090607\n",
            "Iter #405504:  Learning rate = 0.004247:   Batch Loss = 0.901199, Accuracy = 0.896484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.97322165966, Accuracy = 0.871500611305\n",
            "Iter #409600:  Learning rate = 0.004247:   Batch Loss = 0.957526, Accuracy = 0.900390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.913677692413, Accuracy = 0.893583714962\n",
            "Iter #413696:  Learning rate = 0.004247:   Batch Loss = 0.848507, Accuracy = 0.921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.877633333206, Accuracy = 0.904190599918\n",
            "Iter #417792:  Learning rate = 0.004247:   Batch Loss = 0.817161, Accuracy = 0.931640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.859270691872, Accuracy = 0.902451753616\n",
            "Iter #421888:  Learning rate = 0.004247:   Batch Loss = 0.854121, Accuracy = 0.923828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.86966574192, Accuracy = 0.904886126518\n",
            "Iter #425984:  Learning rate = 0.004247:   Batch Loss = 0.818851, Accuracy = 0.921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.868648052216, Accuracy = 0.900539040565\n",
            "Iter #430080:  Learning rate = 0.004247:   Batch Loss = 0.793964, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.863219618797, Accuracy = 0.909233152866\n",
            "Iter #434176:  Learning rate = 0.004247:   Batch Loss = 0.799067, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.859152376652, Accuracy = 0.902277886868\n",
            "Iter #438272:  Learning rate = 0.004247:   Batch Loss = 0.825813, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.854576408863, Accuracy = 0.901408433914\n",
            "Iter #442368:  Learning rate = 0.004247:   Batch Loss = 0.794639, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.841975748539, Accuracy = 0.90940707922\n",
            "Iter #446464:  Learning rate = 0.004247:   Batch Loss = 0.792012, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.841584205627, Accuracy = 0.915319085121\n",
            "Iter #450560:  Learning rate = 0.004247:   Batch Loss = 0.869888, Accuracy = 0.90625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.885861754417, Accuracy = 0.899495720863\n",
            "Iter #454656:  Learning rate = 0.004247:   Batch Loss = 0.807011, Accuracy = 0.91015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.830274283886, Accuracy = 0.915840744972\n",
            "Iter #458752:  Learning rate = 0.004247:   Batch Loss = 0.761064, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8040599823, Accuracy = 0.925056517124\n",
            "Iter #462848:  Learning rate = 0.004247:   Batch Loss = 0.767402, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.857463419437, Accuracy = 0.901234567165\n",
            "Iter #466944:  Learning rate = 0.004247:   Batch Loss = 0.746569, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.944237232208, Accuracy = 0.868718504906\n",
            "Iter #471040:  Learning rate = 0.004247:   Batch Loss = 0.893064, Accuracy = 0.888671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.05147123337, Accuracy = 0.830812036991\n",
            "Iter #475136:  Learning rate = 0.004247:   Batch Loss = 0.857410, Accuracy = 0.912109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.916008651257, Accuracy = 0.875326037407\n",
            "Iter #479232:  Learning rate = 0.004247:   Batch Loss = 0.905958, Accuracy = 0.880859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.856829941273, Accuracy = 0.900712907314\n",
            "Iter #483328:  Learning rate = 0.004247:   Batch Loss = 0.788767, Accuracy = 0.9296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.795583724976, Accuracy = 0.91949224472\n",
            "Iter #487424:  Learning rate = 0.004247:   Batch Loss = 0.803176, Accuracy = 0.916015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.814348816872, Accuracy = 0.915666818619\n",
            "Iter #491520:  Learning rate = 0.004247:   Batch Loss = 0.829922, Accuracy = 0.908203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.795936584473, Accuracy = 0.919666171074\n",
            "Iter #495616:  Learning rate = 0.004247:   Batch Loss = 0.803710, Accuracy = 0.916015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.772567212582, Accuracy = 0.929055809975\n",
            "Iter #499712:  Learning rate = 0.004247:   Batch Loss = 0.731733, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.78675609827, Accuracy = 0.927490890026\n",
            "Iter #503808:  Learning rate = 0.004077:   Batch Loss = 0.717209, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.77822124958, Accuracy = 0.929055809975\n",
            "Iter #507904:  Learning rate = 0.004077:   Batch Loss = 0.711266, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.816429376602, Accuracy = 0.91010260582\n",
            "Iter #512000:  Learning rate = 0.004077:   Batch Loss = 0.720378, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.834199488163, Accuracy = 0.902103960514\n",
            "Iter #516096:  Learning rate = 0.004077:   Batch Loss = 0.756082, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.761196494102, Accuracy = 0.930794656277\n",
            "Iter #520192:  Learning rate = 0.004077:   Batch Loss = 0.761248, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.756764769554, Accuracy = 0.926273703575\n",
            "Iter #524288:  Learning rate = 0.004077:   Batch Loss = 0.745862, Accuracy = 0.919921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.756025016308, Accuracy = 0.925752043724\n",
            "Iter #528384:  Learning rate = 0.004077:   Batch Loss = 0.743531, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.779412865639, Accuracy = 0.925925910473\n",
            "Iter #532480:  Learning rate = 0.004077:   Batch Loss = 0.721313, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.763073265553, Accuracy = 0.935315608978\n",
            "Iter #536576:  Learning rate = 0.004077:   Batch Loss = 1.142170, Accuracy = 0.83203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.949162125587, Accuracy = 0.881411910057\n",
            "Iter #540672:  Learning rate = 0.004077:   Batch Loss = 0.851198, Accuracy = 0.884765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.873461484909, Accuracy = 0.874282717705\n",
            "Iter #544768:  Learning rate = 0.004077:   Batch Loss = 0.805222, Accuracy = 0.9140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.850709974766, Accuracy = 0.896191954613\n",
            "Iter #548864:  Learning rate = 0.004077:   Batch Loss = 0.770948, Accuracy = 0.927734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.812282383442, Accuracy = 0.907146573067\n",
            "Iter #552960:  Learning rate = 0.004077:   Batch Loss = 0.782606, Accuracy = 0.927734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.788592338562, Accuracy = 0.913754105568\n",
            "Iter #557056:  Learning rate = 0.004077:   Batch Loss = 0.689907, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.787496268749, Accuracy = 0.912015318871\n",
            "Iter #561152:  Learning rate = 0.004077:   Batch Loss = 0.761617, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.804670214653, Accuracy = 0.91618847847\n",
            "Iter #565248:  Learning rate = 0.004077:   Batch Loss = 0.720339, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.767434120178, Accuracy = 0.927490890026\n",
            "Iter #569344:  Learning rate = 0.004077:   Batch Loss = 0.733100, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.757161200047, Accuracy = 0.921231091022\n",
            "Iter #573440:  Learning rate = 0.004077:   Batch Loss = 0.716145, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.735073328018, Accuracy = 0.934620082378\n",
            "Iter #577536:  Learning rate = 0.004077:   Batch Loss = 0.794482, Accuracy = 0.916015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.796587944031, Accuracy = 0.914275765419\n",
            "Iter #581632:  Learning rate = 0.004077:   Batch Loss = 0.733138, Accuracy = 0.9296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.736713707447, Accuracy = 0.931837916374\n",
            "Iter #585728:  Learning rate = 0.004077:   Batch Loss = 0.708370, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.729976832867, Accuracy = 0.935315608978\n",
            "Iter #589824:  Learning rate = 0.004077:   Batch Loss = 0.710023, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.726203560829, Accuracy = 0.933055102825\n",
            "Iter #593920:  Learning rate = 0.004077:   Batch Loss = 0.802413, Accuracy = 0.916015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.834039330482, Accuracy = 0.893409848213\n",
            "Iter #598016:  Learning rate = 0.004077:   Batch Loss = 0.713211, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.764401495457, Accuracy = 0.923143804073\n",
            "Iter #602112:  Learning rate = 0.003914:   Batch Loss = 0.736644, Accuracy = 0.923828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.808931946754, Accuracy = 0.893583714962\n",
            "Iter #606208:  Learning rate = 0.003914:   Batch Loss = 0.766430, Accuracy = 0.919921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.746661305428, Accuracy = 0.925056517124\n",
            "Iter #610304:  Learning rate = 0.003914:   Batch Loss = 0.715730, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.777122974396, Accuracy = 0.914275765419\n",
            "Iter #614400:  Learning rate = 0.003914:   Batch Loss = 0.733680, Accuracy = 0.927734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.741981387138, Accuracy = 0.927664756775\n",
            "Iter #618496:  Learning rate = 0.003914:   Batch Loss = 0.711140, Accuracy = 0.931640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.789107203484, Accuracy = 0.899147987366\n",
            "Iter #622592:  Learning rate = 0.003914:   Batch Loss = 0.770580, Accuracy = 0.916015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.73151409626, Accuracy = 0.925925910473\n",
            "Iter #626688:  Learning rate = 0.003914:   Batch Loss = 0.686606, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.74172437191, Accuracy = 0.924534857273\n",
            "Iter #630784:  Learning rate = 0.003914:   Batch Loss = 0.719633, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.716243565083, Accuracy = 0.929751336575\n",
            "Iter #634880:  Learning rate = 0.003914:   Batch Loss = 0.763624, Accuracy = 0.919921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.717132568359, Accuracy = 0.934793949127\n",
            "Iter #638976:  Learning rate = 0.003914:   Batch Loss = 0.681515, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.769026756287, Accuracy = 0.918796718121\n",
            "Iter #643072:  Learning rate = 0.003914:   Batch Loss = 0.739423, Accuracy = 0.9296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.732138574123, Accuracy = 0.920013904572\n",
            "Iter #647168:  Learning rate = 0.003914:   Batch Loss = 0.670536, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.730415821075, Accuracy = 0.918970584869\n",
            "Iter #651264:  Learning rate = 0.003914:   Batch Loss = 0.622544, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.695031940937, Accuracy = 0.938793241978\n",
            "Iter #655360:  Learning rate = 0.003914:   Batch Loss = 0.659437, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.698232710361, Accuracy = 0.934272289276\n",
            "Iter #659456:  Learning rate = 0.003914:   Batch Loss = 0.675741, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.710863947868, Accuracy = 0.931490182877\n",
            "Iter #663552:  Learning rate = 0.003914:   Batch Loss = 0.687069, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.755038380623, Accuracy = 0.91757953167\n",
            "Iter #667648:  Learning rate = 0.003914:   Batch Loss = 0.668784, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.732350170612, Accuracy = 0.922448277473\n",
            "Iter #671744:  Learning rate = 0.003914:   Batch Loss = 0.671756, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.691520571709, Accuracy = 0.936185002327\n",
            "Iter #675840:  Learning rate = 0.003914:   Batch Loss = 0.673868, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.705791473389, Accuracy = 0.931316316128\n",
            "Iter #679936:  Learning rate = 0.003914:   Batch Loss = 0.671812, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.731256425381, Accuracy = 0.921057224274\n",
            "Iter #684032:  Learning rate = 0.003914:   Batch Loss = 0.653959, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.697719871998, Accuracy = 0.937054395676\n",
            "Iter #688128:  Learning rate = 0.003914:   Batch Loss = 0.629657, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.705066859722, Accuracy = 0.930620789528\n",
            "Iter #692224:  Learning rate = 0.003914:   Batch Loss = 0.627586, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.693339705467, Accuracy = 0.934793949127\n",
            "Iter #696320:  Learning rate = 0.003914:   Batch Loss = 0.644251, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.768902897835, Accuracy = 0.904538333416\n",
            "Iter #700416:  Learning rate = 0.003757:   Batch Loss = 0.625842, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.761414647102, Accuracy = 0.914971292019\n",
            "Iter #704512:  Learning rate = 0.003757:   Batch Loss = 0.664381, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.807474076748, Accuracy = 0.892888188362\n",
            "Iter #708608:  Learning rate = 0.003757:   Batch Loss = 0.657765, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.681888520718, Accuracy = 0.935489475727\n",
            "Iter #712704:  Learning rate = 0.003757:   Batch Loss = 0.668802, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.680590927601, Accuracy = 0.935141742229\n",
            "Iter #716800:  Learning rate = 0.003757:   Batch Loss = 0.636756, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.654715299606, Accuracy = 0.942618668079\n",
            "Iter #720896:  Learning rate = 0.003757:   Batch Loss = 0.652047, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.67189002037, Accuracy = 0.93914103508\n",
            "Iter #724992:  Learning rate = 0.003757:   Batch Loss = 0.608354, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.658202409744, Accuracy = 0.945053040981\n",
            "Iter #729088:  Learning rate = 0.003757:   Batch Loss = 0.604023, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.647301614285, Accuracy = 0.94731348753\n",
            "Iter #733184:  Learning rate = 0.003757:   Batch Loss = 0.611152, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.640096068382, Accuracy = 0.952182233334\n",
            "Iter #737280:  Learning rate = 0.003757:   Batch Loss = 0.612749, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.641672194004, Accuracy = 0.950095653534\n",
            "Iter #741376:  Learning rate = 0.003757:   Batch Loss = 0.606768, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.660540521145, Accuracy = 0.93983656168\n",
            "Iter #745472:  Learning rate = 0.003757:   Batch Loss = 0.621432, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.673298537731, Accuracy = 0.938619375229\n",
            "Iter #749568:  Learning rate = 0.003757:   Batch Loss = 0.623368, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.68052738905, Accuracy = 0.938097715378\n",
            "Iter #753664:  Learning rate = 0.003757:   Batch Loss = 0.613976, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.722302794456, Accuracy = 0.915666818619\n",
            "Iter #757760:  Learning rate = 0.003757:   Batch Loss = 0.633674, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.656458854675, Accuracy = 0.941401481628\n",
            "Iter #761856:  Learning rate = 0.003757:   Batch Loss = 0.637878, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.686925292015, Accuracy = 0.936706662178\n",
            "Iter #765952:  Learning rate = 0.003757:   Batch Loss = 0.737313, Accuracy = 0.919921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.770479083061, Accuracy = 0.922100484371\n",
            "Iter #770048:  Learning rate = 0.003757:   Batch Loss = 0.651745, Accuracy = 0.931640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.675332427025, Accuracy = 0.931142389774\n",
            "Iter #774144:  Learning rate = 0.003757:   Batch Loss = 0.601481, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.63844794035, Accuracy = 0.945574700832\n",
            "Iter #778240:  Learning rate = 0.003757:   Batch Loss = 0.730197, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.69838398695, Accuracy = 0.925230383873\n",
            "Iter #782336:  Learning rate = 0.003757:   Batch Loss = 0.628802, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.658992528915, Accuracy = 0.941575407982\n",
            "Iter #786432:  Learning rate = 0.003757:   Batch Loss = 0.624721, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.660360634327, Accuracy = 0.938097715378\n",
            "Iter #790528:  Learning rate = 0.003757:   Batch Loss = 0.606340, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.642815351486, Accuracy = 0.947139620781\n",
            "Iter #794624:  Learning rate = 0.003757:   Batch Loss = 0.599407, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.625622153282, Accuracy = 0.954094946384\n",
            "Iter #798720:  Learning rate = 0.003757:   Batch Loss = 0.627924, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.652098894119, Accuracy = 0.938793241978\n",
            "Iter #802816:  Learning rate = 0.003607:   Batch Loss = 0.618826, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.686361312866, Accuracy = 0.932011842728\n",
            "Iter #806912:  Learning rate = 0.003607:   Batch Loss = 0.619121, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.675638794899, Accuracy = 0.924013197422\n",
            "Iter #811008:  Learning rate = 0.003607:   Batch Loss = 0.592331, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.666722118855, Accuracy = 0.940184295177\n",
            "Iter #815104:  Learning rate = 0.003607:   Batch Loss = 0.623226, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.643478214741, Accuracy = 0.946444094181\n",
            "Iter #819200:  Learning rate = 0.003607:   Batch Loss = 0.626298, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.637773215771, Accuracy = 0.942444801331\n",
            "Iter #823296:  Learning rate = 0.003607:   Batch Loss = 0.583270, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.831687450409, Accuracy = 0.873761057854\n",
            "Iter #827392:  Learning rate = 0.003607:   Batch Loss = 0.639747, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.698843240738, Accuracy = 0.916536271572\n",
            "Iter #831488:  Learning rate = 0.003607:   Batch Loss = 0.633370, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.661244750023, Accuracy = 0.935315608978\n",
            "Iter #835584:  Learning rate = 0.003607:   Batch Loss = 0.596723, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.660433411598, Accuracy = 0.933924555779\n",
            "Iter #839680:  Learning rate = 0.003607:   Batch Loss = 0.595942, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.685247659683, Accuracy = 0.924013197422\n",
            "Iter #843776:  Learning rate = 0.003607:   Batch Loss = 0.631709, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.716993451118, Accuracy = 0.91271084547\n",
            "Iter #847872:  Learning rate = 0.003607:   Batch Loss = 0.588479, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.69483602047, Accuracy = 0.928360283375\n",
            "Iter #851968:  Learning rate = 0.003607:   Batch Loss = 0.612295, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.633783221245, Accuracy = 0.94053208828\n",
            "Iter #856064:  Learning rate = 0.003607:   Batch Loss = 0.603366, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.659728527069, Accuracy = 0.938097715378\n",
            "Iter #860160:  Learning rate = 0.003607:   Batch Loss = 0.589641, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.645628094673, Accuracy = 0.934793949127\n",
            "Iter #864256:  Learning rate = 0.003607:   Batch Loss = 0.607586, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.654412150383, Accuracy = 0.94122761488\n",
            "Iter #868352:  Learning rate = 0.003607:   Batch Loss = 0.590092, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.636251807213, Accuracy = 0.94053208828\n",
            "Iter #872448:  Learning rate = 0.003607:   Batch Loss = 0.599126, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.618695497513, Accuracy = 0.947835147381\n",
            "Iter #876544:  Learning rate = 0.003607:   Batch Loss = 0.566021, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.618361711502, Accuracy = 0.949052333832\n",
            "Iter #880640:  Learning rate = 0.003607:   Batch Loss = 0.582136, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.657027959824, Accuracy = 0.936358869076\n",
            "Iter #884736:  Learning rate = 0.003607:   Batch Loss = 0.582039, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.63711732626, Accuracy = 0.94053208828\n",
            "Iter #888832:  Learning rate = 0.003607:   Batch Loss = 0.576012, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.619584798813, Accuracy = 0.946270227432\n",
            "Iter #892928:  Learning rate = 0.003607:   Batch Loss = 0.564650, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.589951038361, Accuracy = 0.956355392933\n",
            "Iter #897024:  Learning rate = 0.003607:   Batch Loss = 0.620976, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.635994195938, Accuracy = 0.94053208828\n",
            "Iter #901120:  Learning rate = 0.003463:   Batch Loss = 0.595611, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.610255002975, Accuracy = 0.951486706734\n",
            "Iter #905216:  Learning rate = 0.003463:   Batch Loss = 0.551726, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.649809956551, Accuracy = 0.938793241978\n",
            "Iter #909312:  Learning rate = 0.003463:   Batch Loss = 0.562073, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.62675011158, Accuracy = 0.94314032793\n",
            "Iter #913408:  Learning rate = 0.003463:   Batch Loss = 0.615422, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.608287930489, Accuracy = 0.947835147381\n",
            "Iter #917504:  Learning rate = 0.003463:   Batch Loss = 0.604501, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.65950345993, Accuracy = 0.926447570324\n",
            "Iter #921600:  Learning rate = 0.003463:   Batch Loss = 0.625596, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.646602988243, Accuracy = 0.941575407982\n",
            "Iter #925696:  Learning rate = 0.003463:   Batch Loss = 0.647807, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.649677634239, Accuracy = 0.930794656277\n",
            "Iter #929792:  Learning rate = 0.003463:   Batch Loss = 0.574319, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.623407006264, Accuracy = 0.939662694931\n",
            "Iter #933888:  Learning rate = 0.003463:   Batch Loss = 0.610054, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.643517494202, Accuracy = 0.93774998188\n",
            "Iter #937984:  Learning rate = 0.003463:   Batch Loss = 0.610741, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.610361218452, Accuracy = 0.943488061428\n",
            "Iter #942080:  Learning rate = 0.003463:   Batch Loss = 0.615054, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.611545979977, Accuracy = 0.944705247879\n",
            "Iter #946176:  Learning rate = 0.003463:   Batch Loss = 0.554313, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.615289330482, Accuracy = 0.942792534828\n",
            "Iter #950272:  Learning rate = 0.003463:   Batch Loss = 0.570431, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.602652430534, Accuracy = 0.94992172718\n",
            "Iter #954368:  Learning rate = 0.003463:   Batch Loss = 0.553537, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.632362723351, Accuracy = 0.94383585453\n",
            "Iter #958464:  Learning rate = 0.003463:   Batch Loss = 0.528845, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.596924245358, Accuracy = 0.950269520283\n",
            "Iter #962560:  Learning rate = 0.003463:   Batch Loss = 0.551369, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.622521162033, Accuracy = 0.94383585453\n",
            "Iter #966656:  Learning rate = 0.003463:   Batch Loss = 0.533553, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.603947281837, Accuracy = 0.953921079636\n",
            "Iter #970752:  Learning rate = 0.003463:   Batch Loss = 0.521163, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.61712449789, Accuracy = 0.937923848629\n",
            "Iter #974848:  Learning rate = 0.003463:   Batch Loss = 0.565698, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.610860586166, Accuracy = 0.942444801331\n",
            "Iter #978944:  Learning rate = 0.003463:   Batch Loss = 0.558229, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.60641759634, Accuracy = 0.94731348753\n",
            "Iter #983040:  Learning rate = 0.003463:   Batch Loss = 0.527409, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.581643342972, Accuracy = 0.954616606236\n",
            "Iter #987136:  Learning rate = 0.003463:   Batch Loss = 0.544680, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.61263692379, Accuracy = 0.946444094181\n",
            "Iter #991232:  Learning rate = 0.003463:   Batch Loss = 0.542882, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.617993593216, Accuracy = 0.947661280632\n",
            "Iter #995328:  Learning rate = 0.003463:   Batch Loss = 0.550481, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.595387279987, Accuracy = 0.947835147381\n",
            "Iter #999424:  Learning rate = 0.003463:   Batch Loss = 0.550916, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.60619366169, Accuracy = 0.94314032793\n",
            "Iter #1003520:  Learning rate = 0.003324:   Batch Loss = 0.583179, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.607173085213, Accuracy = 0.950443387032\n",
            "Iter #1007616:  Learning rate = 0.003324:   Batch Loss = 0.549316, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.609371781349, Accuracy = 0.941053748131\n",
            "Iter #1011712:  Learning rate = 0.003324:   Batch Loss = 0.552449, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.592252135277, Accuracy = 0.949226200581\n",
            "Iter #1015808:  Learning rate = 0.003324:   Batch Loss = 0.550570, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.584064483643, Accuracy = 0.950269520283\n",
            "Iter #1019904:  Learning rate = 0.003324:   Batch Loss = 0.527788, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.590117454529, Accuracy = 0.947661280632\n",
            "Iter #1024000:  Learning rate = 0.003324:   Batch Loss = 0.564697, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.604472637177, Accuracy = 0.94383585453\n",
            "Iter #1028096:  Learning rate = 0.003324:   Batch Loss = 0.519882, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.571928918362, Accuracy = 0.955659866333\n",
            "Iter #1032192:  Learning rate = 0.003324:   Batch Loss = 0.505450, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.572403073311, Accuracy = 0.956181526184\n",
            "Iter #1036288:  Learning rate = 0.003324:   Batch Loss = 0.534728, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.560310721397, Accuracy = 0.959659218788\n",
            "Iter #1040384:  Learning rate = 0.003324:   Batch Loss = 0.538445, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.562755048275, Accuracy = 0.958442032337\n",
            "Iter #1044480:  Learning rate = 0.003324:   Batch Loss = 0.555178, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.578426837921, Accuracy = 0.952703893185\n",
            "Iter #1048576:  Learning rate = 0.003324:   Batch Loss = 0.531920, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.578451812267, Accuracy = 0.952877759933\n",
            "Iter #1052672:  Learning rate = 0.003324:   Batch Loss = 0.561432, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.713246822357, Accuracy = 0.905233860016\n",
            "Iter #1056768:  Learning rate = 0.003324:   Batch Loss = 0.628352, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.69313621521, Accuracy = 0.916362345219\n",
            "Iter #1060864:  Learning rate = 0.003324:   Batch Loss = 0.639120, Accuracy = 0.923828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.714707255363, Accuracy = 0.90940707922\n",
            "Iter #1064960:  Learning rate = 0.003324:   Batch Loss = 0.730676, Accuracy = 0.9140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.615843832493, Accuracy = 0.938793241978\n",
            "Iter #1069056:  Learning rate = 0.003324:   Batch Loss = 0.567729, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.616283178329, Accuracy = 0.937054395676\n",
            "Iter #1073152:  Learning rate = 0.003324:   Batch Loss = 0.570426, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.612954974174, Accuracy = 0.935837268829\n",
            "Iter #1077248:  Learning rate = 0.003324:   Batch Loss = 0.571789, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.609691977501, Accuracy = 0.937923848629\n",
            "Iter #1081344:  Learning rate = 0.003324:   Batch Loss = 0.564069, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.570471048355, Accuracy = 0.950791180134\n",
            "Iter #1085440:  Learning rate = 0.003324:   Batch Loss = 0.552014, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.559715390205, Accuracy = 0.956529319286\n",
            "Iter #1089536:  Learning rate = 0.003324:   Batch Loss = 0.722612, Accuracy = 0.880859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.613201737404, Accuracy = 0.935315608978\n",
            "Iter #1093632:  Learning rate = 0.003324:   Batch Loss = 0.605313, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.680552005768, Accuracy = 0.913754105568\n",
            "Iter #1097728:  Learning rate = 0.003324:   Batch Loss = 0.559170, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.604909837246, Accuracy = 0.94453138113\n",
            "Iter #1101824:  Learning rate = 0.003191:   Batch Loss = 0.575219, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.564294576645, Accuracy = 0.957050919533\n",
            "Iter #1105920:  Learning rate = 0.003191:   Batch Loss = 0.523477, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.599896192551, Accuracy = 0.945748567581\n",
            "Iter #1110016:  Learning rate = 0.003191:   Batch Loss = 0.554680, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.608553826809, Accuracy = 0.938271582127\n",
            "Iter #1114112:  Learning rate = 0.003191:   Batch Loss = 0.531712, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.60903441906, Accuracy = 0.943314194679\n",
            "Iter #1118208:  Learning rate = 0.003191:   Batch Loss = 0.537211, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.557911515236, Accuracy = 0.955138266087\n",
            "Iter #1122304:  Learning rate = 0.003191:   Batch Loss = 0.540665, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.580034732819, Accuracy = 0.949573993683\n",
            "Iter #1126400:  Learning rate = 0.003191:   Batch Loss = 0.538036, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.572999060154, Accuracy = 0.954268813133\n",
            "Iter #1130496:  Learning rate = 0.003191:   Batch Loss = 0.490281, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.571672916412, Accuracy = 0.953573286533\n",
            "Iter #1134592:  Learning rate = 0.003191:   Batch Loss = 0.550567, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.554816901684, Accuracy = 0.960354745388\n",
            "Iter #1138688:  Learning rate = 0.003191:   Batch Loss = 0.520396, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.545369982719, Accuracy = 0.960006952286\n",
            "Iter #1142784:  Learning rate = 0.003191:   Batch Loss = 0.489095, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.549103915691, Accuracy = 0.957224845886\n",
            "Iter #1146880:  Learning rate = 0.003191:   Batch Loss = 0.541767, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.546131253242, Accuracy = 0.958094239235\n",
            "Iter #1150976:  Learning rate = 0.003191:   Batch Loss = 0.514747, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.52945971489, Accuracy = 0.962267458439\n",
            "Iter #1155072:  Learning rate = 0.003191:   Batch Loss = 0.463183, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.548855245113, Accuracy = 0.955833792686\n",
            "Iter #1159168:  Learning rate = 0.003191:   Batch Loss = 0.503980, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.554238915443, Accuracy = 0.956007659435\n",
            "Iter #1163264:  Learning rate = 0.003191:   Batch Loss = 0.499431, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.621803939342, Accuracy = 0.943488061428\n",
            "Iter #1167360:  Learning rate = 0.003191:   Batch Loss = 0.568528, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.565687477589, Accuracy = 0.950617313385\n",
            "Iter #1171456:  Learning rate = 0.003191:   Batch Loss = 0.528598, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.556761860847, Accuracy = 0.955485999584\n",
            "Iter #1175552:  Learning rate = 0.003191:   Batch Loss = 0.489725, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.551038146019, Accuracy = 0.956181526184\n",
            "Iter #1179648:  Learning rate = 0.003191:   Batch Loss = 0.516608, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.575881361961, Accuracy = 0.951834440231\n",
            "Iter #1183744:  Learning rate = 0.003191:   Batch Loss = 0.493308, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.573315620422, Accuracy = 0.952182233334\n",
            "Iter #1187840:  Learning rate = 0.003191:   Batch Loss = 0.559939, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.548264443874, Accuracy = 0.955659866333\n",
            "Iter #1191936:  Learning rate = 0.003191:   Batch Loss = 0.517275, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.560761868954, Accuracy = 0.949573993683\n",
            "Iter #1196032:  Learning rate = 0.003191:   Batch Loss = 0.534370, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.528017580509, Accuracy = 0.960006952286\n",
            "Iter #1200128:  Learning rate = 0.003064:   Batch Loss = 0.487483, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.538127601147, Accuracy = 0.959137558937\n",
            "Iter #1204224:  Learning rate = 0.003064:   Batch Loss = 0.492332, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.586328029633, Accuracy = 0.946965754032\n",
            "Iter #1208320:  Learning rate = 0.003064:   Batch Loss = 0.529818, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.573155045509, Accuracy = 0.94661796093\n",
            "Iter #1212416:  Learning rate = 0.003064:   Batch Loss = 0.502265, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.542127788067, Accuracy = 0.956007659435\n",
            "Iter #1216512:  Learning rate = 0.003064:   Batch Loss = 0.536471, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.588795304298, Accuracy = 0.944879174232\n",
            "Iter #1220608:  Learning rate = 0.003064:   Batch Loss = 0.512356, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.560793459415, Accuracy = 0.950617313385\n",
            "Iter #1224704:  Learning rate = 0.003064:   Batch Loss = 0.500576, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.547197043896, Accuracy = 0.958615899086\n",
            "Iter #1228800:  Learning rate = 0.003064:   Batch Loss = 0.499320, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.536871910095, Accuracy = 0.956355392933\n",
            "Iter #1232896:  Learning rate = 0.003064:   Batch Loss = 0.484942, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.561746835709, Accuracy = 0.954268813133\n",
            "Iter #1236992:  Learning rate = 0.003064:   Batch Loss = 0.504922, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.565609097481, Accuracy = 0.950269520283\n",
            "Iter #1241088:  Learning rate = 0.003064:   Batch Loss = 0.517201, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.547395944595, Accuracy = 0.955138266087\n",
            "Iter #1245184:  Learning rate = 0.003064:   Batch Loss = 0.503681, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.528286218643, Accuracy = 0.955485999584\n",
            "Iter #1249280:  Learning rate = 0.003064:   Batch Loss = 0.478936, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.526309132576, Accuracy = 0.959833085537\n",
            "Iter #1253376:  Learning rate = 0.003064:   Batch Loss = 0.473842, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.587162017822, Accuracy = 0.941923141479\n",
            "Iter #1257472:  Learning rate = 0.003064:   Batch Loss = 0.526636, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.548067808151, Accuracy = 0.955659866333\n",
            "Iter #1261568:  Learning rate = 0.003064:   Batch Loss = 0.493972, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.545171141624, Accuracy = 0.954268813133\n",
            "Iter #1265664:  Learning rate = 0.003064:   Batch Loss = 0.511857, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.521303772926, Accuracy = 0.960180819035\n",
            "Iter #1269760:  Learning rate = 0.003064:   Batch Loss = 0.514012, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.542696833611, Accuracy = 0.957920372486\n",
            "Iter #1273856:  Learning rate = 0.003064:   Batch Loss = 0.652276, Accuracy = 0.90234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.745668292046, Accuracy = 0.89045381546\n",
            "Iter #1277952:  Learning rate = 0.003064:   Batch Loss = 0.581817, Accuracy = 0.9296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.643827676773, Accuracy = 0.918796718121\n",
            "Iter #1282048:  Learning rate = 0.003064:   Batch Loss = 0.539189, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.592692613602, Accuracy = 0.928534150124\n",
            "Iter #1286144:  Learning rate = 0.003064:   Batch Loss = 0.506331, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.582862496376, Accuracy = 0.93844550848\n",
            "Iter #1290240:  Learning rate = 0.003064:   Batch Loss = 0.500657, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.558016061783, Accuracy = 0.949400126934\n",
            "Iter #1294336:  Learning rate = 0.003064:   Batch Loss = 0.482895, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.522979259491, Accuracy = 0.958615899086\n",
            "Iter #1298432:  Learning rate = 0.003064:   Batch Loss = 0.527592, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.592096090317, Accuracy = 0.932011842728\n",
            "Iter #1302528:  Learning rate = 0.002941:   Batch Loss = 0.480207, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.57414495945, Accuracy = 0.940879821777\n",
            "Iter #1306624:  Learning rate = 0.002941:   Batch Loss = 0.561027, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.523482978344, Accuracy = 0.956703186035\n",
            "Iter #1310720:  Learning rate = 0.002941:   Batch Loss = 0.495214, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.56751948595, Accuracy = 0.94314032793\n",
            "Iter #1314816:  Learning rate = 0.002941:   Batch Loss = 0.496167, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.538900375366, Accuracy = 0.952182233334\n",
            "Iter #1318912:  Learning rate = 0.002941:   Batch Loss = 0.490436, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.508151233196, Accuracy = 0.962267458439\n",
            "Iter #1323008:  Learning rate = 0.002941:   Batch Loss = 0.514394, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.509553313255, Accuracy = 0.963310718536\n",
            "Iter #1327104:  Learning rate = 0.002941:   Batch Loss = 0.518532, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.538659393787, Accuracy = 0.952356100082\n",
            "Iter #1331200:  Learning rate = 0.002941:   Batch Loss = 0.480502, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.552431762218, Accuracy = 0.952182233334\n",
            "Iter #1335296:  Learning rate = 0.002941:   Batch Loss = 0.541306, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.521960616112, Accuracy = 0.958094239235\n",
            "Iter #1339392:  Learning rate = 0.002941:   Batch Loss = 0.475799, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.522622704506, Accuracy = 0.959485292435\n",
            "Iter #1343488:  Learning rate = 0.002941:   Batch Loss = 0.467601, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.519540727139, Accuracy = 0.960528612137\n",
            "Iter #1347584:  Learning rate = 0.002941:   Batch Loss = 0.520923, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.522387564182, Accuracy = 0.954442679882\n",
            "Iter #1351680:  Learning rate = 0.002941:   Batch Loss = 0.525887, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.592803299427, Accuracy = 0.938271582127\n",
            "Iter #1355776:  Learning rate = 0.002941:   Batch Loss = 0.517668, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.580508589745, Accuracy = 0.934446156025\n",
            "Iter #1359872:  Learning rate = 0.002941:   Batch Loss = 0.516812, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.579644143581, Accuracy = 0.933402895927\n",
            "Iter #1363968:  Learning rate = 0.002941:   Batch Loss = 0.487999, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.546564221382, Accuracy = 0.948356807232\n",
            "Iter #1368064:  Learning rate = 0.002941:   Batch Loss = 0.515676, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.50398337841, Accuracy = 0.961571872234\n",
            "Iter #1372160:  Learning rate = 0.002941:   Batch Loss = 0.488562, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.510261178017, Accuracy = 0.962093532085\n",
            "Iter #1376256:  Learning rate = 0.002941:   Batch Loss = 0.468906, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.514434635639, Accuracy = 0.962267458439\n",
            "Iter #1380352:  Learning rate = 0.002941:   Batch Loss = 0.490395, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.532227873802, Accuracy = 0.953573286533\n",
            "Iter #1384448:  Learning rate = 0.002941:   Batch Loss = 0.489248, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.50270408392, Accuracy = 0.962615191936\n",
            "Iter #1388544:  Learning rate = 0.002941:   Batch Loss = 0.526048, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.548417329788, Accuracy = 0.949226200581\n",
            "Iter #1392640:  Learning rate = 0.002941:   Batch Loss = 0.484991, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.577070713043, Accuracy = 0.941401481628\n",
            "Iter #1396736:  Learning rate = 0.002941:   Batch Loss = 0.505290, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.540748357773, Accuracy = 0.953399419785\n",
            "Iter #1400832:  Learning rate = 0.002823:   Batch Loss = 0.466885, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.508754491806, Accuracy = 0.959311425686\n",
            "Iter #1404928:  Learning rate = 0.002823:   Batch Loss = 0.475512, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.499899923801, Accuracy = 0.963310718536\n",
            "Iter #1409024:  Learning rate = 0.002823:   Batch Loss = 0.483177, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.505065858364, Accuracy = 0.959137558937\n",
            "Iter #1413120:  Learning rate = 0.002823:   Batch Loss = 0.449150, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.493748694658, Accuracy = 0.964354038239\n",
            "Iter #1417216:  Learning rate = 0.002823:   Batch Loss = 0.469874, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.492656081915, Accuracy = 0.966962277889\n",
            "Iter #1421312:  Learning rate = 0.002823:   Batch Loss = 0.496802, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.506087005138, Accuracy = 0.962789058685\n",
            "Iter #1425408:  Learning rate = 0.002823:   Batch Loss = 0.520432, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.510696530342, Accuracy = 0.957920372486\n",
            "Iter #1429504:  Learning rate = 0.002823:   Batch Loss = 0.467386, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.528047502041, Accuracy = 0.955312132835\n",
            "Iter #1433600:  Learning rate = 0.002823:   Batch Loss = 0.473617, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.541496574879, Accuracy = 0.954268813133\n",
            "Iter #1437696:  Learning rate = 0.002823:   Batch Loss = 0.444223, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49168035388, Accuracy = 0.963658511639\n",
            "Iter #1441792:  Learning rate = 0.002823:   Batch Loss = 0.469986, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.51117169857, Accuracy = 0.957920372486\n",
            "Iter #1445888:  Learning rate = 0.002823:   Batch Loss = 0.475470, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.493370860815, Accuracy = 0.96418017149\n",
            "Iter #1449984:  Learning rate = 0.002823:   Batch Loss = 0.486606, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.558398842812, Accuracy = 0.94661796093\n",
            "Iter #1454080:  Learning rate = 0.002823:   Batch Loss = 0.497040, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.520426928997, Accuracy = 0.952356100082\n",
            "Iter #1458176:  Learning rate = 0.002823:   Batch Loss = 0.480412, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.516815841198, Accuracy = 0.960702478886\n",
            "Iter #1462272:  Learning rate = 0.002823:   Batch Loss = 0.460035, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.519446372986, Accuracy = 0.958789765835\n",
            "Iter #1466368:  Learning rate = 0.002823:   Batch Loss = 0.480931, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.512494027615, Accuracy = 0.959485292435\n",
            "Iter #1470464:  Learning rate = 0.002823:   Batch Loss = 0.443392, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.521613299847, Accuracy = 0.959659218788\n",
            "Iter #1474560:  Learning rate = 0.002823:   Batch Loss = 0.445994, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.519583642483, Accuracy = 0.958963632584\n",
            "Iter #1478656:  Learning rate = 0.002823:   Batch Loss = 0.445402, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.515877306461, Accuracy = 0.960702478886\n",
            "Iter #1482752:  Learning rate = 0.002823:   Batch Loss = 0.474084, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.494711786509, Accuracy = 0.959485292435\n",
            "Iter #1486848:  Learning rate = 0.002823:   Batch Loss = 0.465663, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.483369767666, Accuracy = 0.966614484787\n",
            "Iter #1490944:  Learning rate = 0.002823:   Batch Loss = 0.463409, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.510925292969, Accuracy = 0.957920372486\n",
            "Iter #1495040:  Learning rate = 0.002823:   Batch Loss = 0.468006, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5121281147, Accuracy = 0.960180819035\n",
            "Iter #1499136:  Learning rate = 0.002823:   Batch Loss = 0.428726, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.500655055046, Accuracy = 0.961571872234\n",
            "Iter #1503232:  Learning rate = 0.002710:   Batch Loss = 0.423071, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.480138838291, Accuracy = 0.971830964088\n",
            "Iter #1507328:  Learning rate = 0.002710:   Batch Loss = 0.434723, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49515658617, Accuracy = 0.962441325188\n",
            "Iter #1511424:  Learning rate = 0.002710:   Batch Loss = 0.431780, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.494729101658, Accuracy = 0.962789058685\n",
            "Iter #1515520:  Learning rate = 0.002710:   Batch Loss = 0.479387, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.643736600876, Accuracy = 0.913058578968\n",
            "Iter #1519616:  Learning rate = 0.002710:   Batch Loss = 0.638559, Accuracy = 0.91015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.553603947163, Accuracy = 0.940010428429\n",
            "Iter #1523712:  Learning rate = 0.002710:   Batch Loss = 0.564648, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.545682311058, Accuracy = 0.93844550848\n",
            "Iter #1527808:  Learning rate = 0.002710:   Batch Loss = 0.513239, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.618199706078, Accuracy = 0.915666818619\n",
            "Iter #1531904:  Learning rate = 0.002710:   Batch Loss = 0.572772, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.588550150394, Accuracy = 0.927838623524\n",
            "Iter #1536000:  Learning rate = 0.002710:   Batch Loss = 0.497322, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.56818062067, Accuracy = 0.937402188778\n",
            "Iter #1540096:  Learning rate = 0.002710:   Batch Loss = 0.472397, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.513336479664, Accuracy = 0.955312132835\n",
            "Iter #1544192:  Learning rate = 0.002710:   Batch Loss = 0.467540, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.506761550903, Accuracy = 0.958789765835\n",
            "Iter #1548288:  Learning rate = 0.002710:   Batch Loss = 0.505714, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.509892106056, Accuracy = 0.955833792686\n",
            "Iter #1552384:  Learning rate = 0.002710:   Batch Loss = 0.466777, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.554771006107, Accuracy = 0.941053748131\n",
            "Iter #1556480:  Learning rate = 0.002710:   Batch Loss = 0.486356, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.521079182625, Accuracy = 0.950791180134\n",
            "Iter #1560576:  Learning rate = 0.002710:   Batch Loss = 0.483117, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49584621191, Accuracy = 0.960702478886\n",
            "Iter #1564672:  Learning rate = 0.002710:   Batch Loss = 0.475804, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.475197255611, Accuracy = 0.966092824936\n",
            "Iter #1568768:  Learning rate = 0.002710:   Batch Loss = 0.474440, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.501687705517, Accuracy = 0.952356100082\n",
            "Iter #1572864:  Learning rate = 0.002710:   Batch Loss = 0.486835, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.558005928993, Accuracy = 0.938793241978\n",
            "Iter #1576960:  Learning rate = 0.002710:   Batch Loss = 0.469200, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.50470995903, Accuracy = 0.956355392933\n",
            "Iter #1581056:  Learning rate = 0.002710:   Batch Loss = 0.444952, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.507713198662, Accuracy = 0.952877759933\n",
            "Iter #1585152:  Learning rate = 0.002710:   Batch Loss = 0.487820, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.492689609528, Accuracy = 0.960528612137\n",
            "Iter #1589248:  Learning rate = 0.002710:   Batch Loss = 0.499380, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.551940739155, Accuracy = 0.931837916374\n",
            "Iter #1593344:  Learning rate = 0.002710:   Batch Loss = 0.518214, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.553633868694, Accuracy = 0.94053208828\n",
            "Iter #1597440:  Learning rate = 0.002710:   Batch Loss = 0.448314, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49258941412, Accuracy = 0.961050271988\n",
            "Iter #1601536:  Learning rate = 0.002602:   Batch Loss = 0.455410, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.498923778534, Accuracy = 0.956703186035\n",
            "Iter #1605632:  Learning rate = 0.002602:   Batch Loss = 0.484535, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48211979866, Accuracy = 0.964701771736\n",
            "Iter #1609728:  Learning rate = 0.002602:   Batch Loss = 0.431467, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.479354381561, Accuracy = 0.965397298336\n",
            "Iter #1613824:  Learning rate = 0.002602:   Batch Loss = 0.434371, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.462723851204, Accuracy = 0.969918251038\n",
            "Iter #1617920:  Learning rate = 0.002602:   Batch Loss = 0.464170, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.482680618763, Accuracy = 0.961745798588\n",
            "Iter #1622016:  Learning rate = 0.002602:   Batch Loss = 0.507576, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.557091355324, Accuracy = 0.940705955029\n",
            "Iter #1626112:  Learning rate = 0.002602:   Batch Loss = 0.467752, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4832431674, Accuracy = 0.963136851788\n",
            "Iter #1630208:  Learning rate = 0.002602:   Batch Loss = 0.460897, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4779971838, Accuracy = 0.964701771736\n",
            "Iter #1634304:  Learning rate = 0.002602:   Batch Loss = 0.441493, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48888283968, Accuracy = 0.958789765835\n",
            "Iter #1638400:  Learning rate = 0.002602:   Batch Loss = 0.442194, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.477939486504, Accuracy = 0.963484585285\n",
            "Iter #1642496:  Learning rate = 0.002602:   Batch Loss = 0.413936, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.480969309807, Accuracy = 0.96487569809\n",
            "Iter #1646592:  Learning rate = 0.002602:   Batch Loss = 0.441268, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.475444555283, Accuracy = 0.964701771736\n",
            "Iter #1650688:  Learning rate = 0.002602:   Batch Loss = 0.453712, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.501993656158, Accuracy = 0.956181526184\n",
            "Iter #1654784:  Learning rate = 0.002602:   Batch Loss = 0.460573, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.490850389004, Accuracy = 0.959833085537\n",
            "Iter #1658880:  Learning rate = 0.002602:   Batch Loss = 0.428475, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.495340645313, Accuracy = 0.954964339733\n",
            "Iter #1662976:  Learning rate = 0.002602:   Batch Loss = 0.423653, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.507166445255, Accuracy = 0.952008366585\n",
            "Iter #1667072:  Learning rate = 0.002602:   Batch Loss = 0.436824, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.479840278625, Accuracy = 0.962962985039\n",
            "Iter #1671168:  Learning rate = 0.002602:   Batch Loss = 0.424455, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.506644427776, Accuracy = 0.956529319286\n",
            "Iter #1675264:  Learning rate = 0.002602:   Batch Loss = 0.498425, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.534477472305, Accuracy = 0.944357514381\n",
            "Iter #1679360:  Learning rate = 0.002602:   Batch Loss = 0.453172, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.487063467503, Accuracy = 0.962267458439\n",
            "Iter #1683456:  Learning rate = 0.002602:   Batch Loss = 0.473468, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.513069391251, Accuracy = 0.952182233334\n",
            "Iter #1687552:  Learning rate = 0.002602:   Batch Loss = 0.509194, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49755102396, Accuracy = 0.959137558937\n",
            "Iter #1691648:  Learning rate = 0.002602:   Batch Loss = 0.462404, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.508365035057, Accuracy = 0.950443387032\n",
            "Iter #1695744:  Learning rate = 0.002602:   Batch Loss = 0.466498, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.478648871183, Accuracy = 0.960006952286\n",
            "Iter #1699840:  Learning rate = 0.002602:   Batch Loss = 0.450208, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.479762852192, Accuracy = 0.961919665337\n",
            "Iter #1703936:  Learning rate = 0.002498:   Batch Loss = 0.477684, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48469877243, Accuracy = 0.959485292435\n",
            "Iter #1708032:  Learning rate = 0.002498:   Batch Loss = 0.444453, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.509193122387, Accuracy = 0.958615899086\n",
            "Iter #1712128:  Learning rate = 0.002498:   Batch Loss = 0.482078, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.507371485233, Accuracy = 0.950269520283\n",
            "Iter #1716224:  Learning rate = 0.002498:   Batch Loss = 0.431458, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.481115877628, Accuracy = 0.958442032337\n",
            "Iter #1720320:  Learning rate = 0.002498:   Batch Loss = 0.433741, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.479590266943, Accuracy = 0.962962985039\n",
            "Iter #1724416:  Learning rate = 0.002498:   Batch Loss = 0.436154, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.470572054386, Accuracy = 0.962615191936\n",
            "Iter #1728512:  Learning rate = 0.002498:   Batch Loss = 0.442756, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.476042866707, Accuracy = 0.962962985039\n",
            "Iter #1732608:  Learning rate = 0.002498:   Batch Loss = 0.467857, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.466216146946, Accuracy = 0.966440618038\n",
            "Iter #1736704:  Learning rate = 0.002498:   Batch Loss = 0.427508, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.478882700205, Accuracy = 0.961745798588\n",
            "Iter #1740800:  Learning rate = 0.002498:   Batch Loss = 0.399335, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.451991766691, Accuracy = 0.968527197838\n",
            "Iter #1744896:  Learning rate = 0.002498:   Batch Loss = 0.427081, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.456655949354, Accuracy = 0.968005537987\n",
            "Iter #1748992:  Learning rate = 0.002498:   Batch Loss = 0.446807, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.471276402473, Accuracy = 0.965223431587\n",
            "Iter #1753088:  Learning rate = 0.002498:   Batch Loss = 0.408813, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.464509546757, Accuracy = 0.964354038239\n",
            "Iter #1757184:  Learning rate = 0.002498:   Batch Loss = 0.434642, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.463969230652, Accuracy = 0.965049564838\n",
            "Iter #1761280:  Learning rate = 0.002498:   Batch Loss = 0.431752, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46309158206, Accuracy = 0.965223431587\n",
            "Iter #1765376:  Learning rate = 0.002498:   Batch Loss = 0.472378, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.525273382664, Accuracy = 0.94383585453\n",
            "Iter #1769472:  Learning rate = 0.002498:   Batch Loss = 0.460656, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.481605947018, Accuracy = 0.955312132835\n",
            "Iter #1773568:  Learning rate = 0.002498:   Batch Loss = 0.505185, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.489731669426, Accuracy = 0.954442679882\n",
            "Iter #1777664:  Learning rate = 0.002498:   Batch Loss = 0.441659, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.483249485493, Accuracy = 0.953573286533\n",
            "Iter #1781760:  Learning rate = 0.002498:   Batch Loss = 0.414408, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.491748571396, Accuracy = 0.957050919533\n",
            "Iter #1785856:  Learning rate = 0.002498:   Batch Loss = 0.442492, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.475239932537, Accuracy = 0.958789765835\n",
            "Iter #1789952:  Learning rate = 0.002498:   Batch Loss = 0.418423, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.472107023001, Accuracy = 0.959311425686\n",
            "Iter #1794048:  Learning rate = 0.002498:   Batch Loss = 0.391865, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.460855603218, Accuracy = 0.965918958187\n",
            "Iter #1798144:  Learning rate = 0.002498:   Batch Loss = 0.436878, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.465166002512, Accuracy = 0.960528612137\n",
            "Iter #1802240:  Learning rate = 0.002398:   Batch Loss = 0.431330, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.462534785271, Accuracy = 0.96418017149\n",
            "Iter #1806336:  Learning rate = 0.002398:   Batch Loss = 0.419370, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.486728191376, Accuracy = 0.956007659435\n",
            "Iter #1810432:  Learning rate = 0.002398:   Batch Loss = 0.412198, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.488979935646, Accuracy = 0.956529319286\n",
            "Iter #1814528:  Learning rate = 0.002398:   Batch Loss = 0.435618, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.464261263609, Accuracy = 0.962615191936\n",
            "Iter #1818624:  Learning rate = 0.002398:   Batch Loss = 0.425366, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47727316618, Accuracy = 0.958615899086\n",
            "Iter #1822720:  Learning rate = 0.002398:   Batch Loss = 0.417575, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.458390057087, Accuracy = 0.964006245136\n",
            "Iter #1826816:  Learning rate = 0.002398:   Batch Loss = 0.475330, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.510584115982, Accuracy = 0.94522690773\n",
            "Iter #1830912:  Learning rate = 0.002398:   Batch Loss = 0.420732, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.453256160021, Accuracy = 0.96748393774\n",
            "Iter #1835008:  Learning rate = 0.002398:   Batch Loss = 0.553250, Accuracy = 0.923828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.664271712303, Accuracy = 0.889410555363\n",
            "Iter #1839104:  Learning rate = 0.002398:   Batch Loss = 0.444418, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.476848900318, Accuracy = 0.954790472984\n",
            "Iter #1843200:  Learning rate = 0.002398:   Batch Loss = 0.529555, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.554061472416, Accuracy = 0.929229676723\n",
            "Iter #1847296:  Learning rate = 0.002398:   Batch Loss = 0.601096, Accuracy = 0.912109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.686967372894, Accuracy = 0.873761057854\n",
            "Iter #1851392:  Learning rate = 0.002398:   Batch Loss = 0.546691, Accuracy = 0.923828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.597860217094, Accuracy = 0.900017380714\n",
            "Iter #1855488:  Learning rate = 0.002398:   Batch Loss = 0.490854, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.518971502781, Accuracy = 0.942966461182\n",
            "Iter #1859584:  Learning rate = 0.002398:   Batch Loss = 0.487211, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.499787032604, Accuracy = 0.945574700832\n",
            "Iter #1863680:  Learning rate = 0.002398:   Batch Loss = 0.428551, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.491039872169, Accuracy = 0.953399419785\n",
            "Iter #1867776:  Learning rate = 0.002398:   Batch Loss = 0.443661, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.483885735273, Accuracy = 0.956529319286\n",
            "Iter #1871872:  Learning rate = 0.002398:   Batch Loss = 0.412859, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.503945171833, Accuracy = 0.956007659435\n",
            "Iter #1875968:  Learning rate = 0.002398:   Batch Loss = 0.441849, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.500067830086, Accuracy = 0.956877052784\n",
            "Iter #1880064:  Learning rate = 0.002398:   Batch Loss = 0.430192, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.482148706913, Accuracy = 0.957224845886\n",
            "Iter #1884160:  Learning rate = 0.002398:   Batch Loss = 0.433408, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.474854826927, Accuracy = 0.958615899086\n",
            "Iter #1888256:  Learning rate = 0.002398:   Batch Loss = 0.410915, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.480014264584, Accuracy = 0.955833792686\n",
            "Iter #1892352:  Learning rate = 0.002398:   Batch Loss = 0.404024, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.463991701603, Accuracy = 0.962267458439\n",
            "Iter #1896448:  Learning rate = 0.002398:   Batch Loss = 0.440969, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.507285833359, Accuracy = 0.950791180134\n",
            "Iter #1900544:  Learning rate = 0.002302:   Batch Loss = 0.448365, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.53965473175, Accuracy = 0.944705247879\n",
            "Iter #1904640:  Learning rate = 0.002302:   Batch Loss = 0.433902, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.477730989456, Accuracy = 0.955659866333\n",
            "Iter #1908736:  Learning rate = 0.002302:   Batch Loss = 0.403532, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.461465418339, Accuracy = 0.961571872234\n",
            "Iter #1912832:  Learning rate = 0.002302:   Batch Loss = 0.408765, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.454737305641, Accuracy = 0.963484585285\n",
            "Iter #1916928:  Learning rate = 0.002302:   Batch Loss = 0.423612, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.457932978868, Accuracy = 0.965571224689\n",
            "Iter #1921024:  Learning rate = 0.002302:   Batch Loss = 0.392668, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.466780841351, Accuracy = 0.960006952286\n",
            "Iter #1925120:  Learning rate = 0.002302:   Batch Loss = 0.410201, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.439288318157, Accuracy = 0.96887499094\n",
            "Iter #1929216:  Learning rate = 0.002302:   Batch Loss = 0.403570, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.452841043472, Accuracy = 0.966962277889\n",
            "Iter #1933312:  Learning rate = 0.002302:   Batch Loss = 0.395834, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.430237799883, Accuracy = 0.972526490688\n",
            "Iter #1937408:  Learning rate = 0.002302:   Batch Loss = 0.412050, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.454198092222, Accuracy = 0.96418017149\n",
            "Iter #1941504:  Learning rate = 0.002302:   Batch Loss = 0.403575, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.441647738218, Accuracy = 0.970092177391\n",
            "Iter #1945600:  Learning rate = 0.002302:   Batch Loss = 0.430799, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.447923481464, Accuracy = 0.967831671238\n",
            "Iter #1949696:  Learning rate = 0.002302:   Batch Loss = 0.440736, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.448019266129, Accuracy = 0.96748393774\n",
            "Iter #1953792:  Learning rate = 0.002302:   Batch Loss = 0.429373, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.443089812994, Accuracy = 0.969396650791\n",
            "Iter #1957888:  Learning rate = 0.002302:   Batch Loss = 0.401511, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.435811698437, Accuracy = 0.972178757191\n",
            "Iter #1961984:  Learning rate = 0.002302:   Batch Loss = 0.416101, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.447515130043, Accuracy = 0.966962277889\n",
            "Iter #1966080:  Learning rate = 0.002302:   Batch Loss = 0.407946, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.463423073292, Accuracy = 0.962962985039\n",
            "Iter #1970176:  Learning rate = 0.002302:   Batch Loss = 0.424012, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.458807110786, Accuracy = 0.965745091438\n",
            "Iter #1974272:  Learning rate = 0.002302:   Batch Loss = 0.424312, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.468412697315, Accuracy = 0.959833085537\n",
            "Iter #1978368:  Learning rate = 0.002302:   Batch Loss = 0.447044, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.443868994713, Accuracy = 0.96957051754\n",
            "Iter #1982464:  Learning rate = 0.002302:   Batch Loss = 0.411926, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.436907202005, Accuracy = 0.968701124191\n",
            "Iter #1986560:  Learning rate = 0.002302:   Batch Loss = 0.423088, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45669350028, Accuracy = 0.962267458439\n",
            "Iter #1990656:  Learning rate = 0.002302:   Batch Loss = 0.393671, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46054494381, Accuracy = 0.958615899086\n",
            "Iter #1994752:  Learning rate = 0.002302:   Batch Loss = 0.394631, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.429547220469, Accuracy = 0.971309363842\n",
            "Iter #1998848:  Learning rate = 0.002302:   Batch Loss = 0.388412, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.427625060081, Accuracy = 0.97235262394\n",
            "Iter #2002944:  Learning rate = 0.002210:   Batch Loss = 0.397519, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.418624579906, Accuracy = 0.97565639019\n",
            "Iter #2007040:  Learning rate = 0.002210:   Batch Loss = 0.376553, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.433506637812, Accuracy = 0.968701124191\n",
            "Iter #2011136:  Learning rate = 0.002210:   Batch Loss = 0.421420, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.452499598265, Accuracy = 0.961571872234\n",
            "Iter #2015232:  Learning rate = 0.002210:   Batch Loss = 0.396664, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.434418350458, Accuracy = 0.970613777637\n",
            "Iter #2019328:  Learning rate = 0.002210:   Batch Loss = 0.395204, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.445869266987, Accuracy = 0.964354038239\n",
            "Iter #2023424:  Learning rate = 0.002210:   Batch Loss = 0.376174, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.439110308886, Accuracy = 0.965571224689\n",
            "Iter #2027520:  Learning rate = 0.002210:   Batch Loss = 0.412561, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.435859799385, Accuracy = 0.96748393774\n",
            "Iter #2031616:  Learning rate = 0.002210:   Batch Loss = 0.395725, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.423592448235, Accuracy = 0.97096157074\n",
            "Iter #2035712:  Learning rate = 0.002210:   Batch Loss = 0.400298, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.434908807278, Accuracy = 0.96817946434\n",
            "Iter #2039808:  Learning rate = 0.002210:   Batch Loss = 0.388257, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.435519784689, Accuracy = 0.970092177391\n",
            "Iter #2043904:  Learning rate = 0.002210:   Batch Loss = 0.388021, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.451671421528, Accuracy = 0.962615191936\n",
            "Iter #2048000:  Learning rate = 0.002210:   Batch Loss = 0.380424, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.451433956623, Accuracy = 0.960876345634\n",
            "Iter #2052096:  Learning rate = 0.002210:   Batch Loss = 0.394611, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.448858350515, Accuracy = 0.966092824936\n",
            "Iter #2056192:  Learning rate = 0.002210:   Batch Loss = 0.401184, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.421865642071, Accuracy = 0.970787703991\n",
            "Iter #2060288:  Learning rate = 0.002210:   Batch Loss = 0.393905, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.453035205603, Accuracy = 0.962267458439\n",
            "Iter #2064384:  Learning rate = 0.002210:   Batch Loss = 0.416047, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.468180298805, Accuracy = 0.959137558937\n",
            "Iter #2068480:  Learning rate = 0.002210:   Batch Loss = 0.398316, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.439956843853, Accuracy = 0.965918958187\n",
            "Iter #2072576:  Learning rate = 0.002210:   Batch Loss = 0.388862, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.424970775843, Accuracy = 0.970613777637\n",
            "Iter #2076672:  Learning rate = 0.002210:   Batch Loss = 0.370218, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.422488570213, Accuracy = 0.97096157074\n",
            "Iter #2080768:  Learning rate = 0.002210:   Batch Loss = 0.417846, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.431357383728, Accuracy = 0.971135437489\n",
            "Iter #2084864:  Learning rate = 0.002210:   Batch Loss = 0.379449, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.422099530697, Accuracy = 0.970092177391\n",
            "Iter #2088960:  Learning rate = 0.002210:   Batch Loss = 0.399788, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.427482366562, Accuracy = 0.971135437489\n",
            "Iter #2093056:  Learning rate = 0.002210:   Batch Loss = 0.397137, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.424999773502, Accuracy = 0.968701124191\n",
            "Iter #2097152:  Learning rate = 0.002210:   Batch Loss = 0.385818, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.431246876717, Accuracy = 0.96887499094\n",
            "Iter #2101248:  Learning rate = 0.002122:   Batch Loss = 0.380640, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.420631110668, Accuracy = 0.971309363842\n",
            "Iter #2105344:  Learning rate = 0.002122:   Batch Loss = 0.384505, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.416962772608, Accuracy = 0.971830964088\n",
            "Iter #2109440:  Learning rate = 0.002122:   Batch Loss = 0.413343, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.433911651373, Accuracy = 0.969396650791\n",
            "Iter #2113536:  Learning rate = 0.002122:   Batch Loss = 0.384770, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.433271706104, Accuracy = 0.965049564838\n",
            "Iter #2117632:  Learning rate = 0.002122:   Batch Loss = 0.401707, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.434768855572, Accuracy = 0.966440618038\n",
            "Iter #2121728:  Learning rate = 0.002122:   Batch Loss = 0.383038, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.461560308933, Accuracy = 0.954790472984\n",
            "Iter #2125824:  Learning rate = 0.002122:   Batch Loss = 0.398074, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.421505123377, Accuracy = 0.970439910889\n",
            "Iter #2129920:  Learning rate = 0.002122:   Batch Loss = 0.403758, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.424757301807, Accuracy = 0.968701124191\n",
            "Iter #2134016:  Learning rate = 0.002122:   Batch Loss = 0.379623, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.431182444096, Accuracy = 0.968527197838\n",
            "Iter #2138112:  Learning rate = 0.002122:   Batch Loss = 0.384422, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.417054831982, Accuracy = 0.972004890442\n",
            "Iter #2142208:  Learning rate = 0.002122:   Batch Loss = 0.375601, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41602280736, Accuracy = 0.97356981039\n",
            "Iter #2146304:  Learning rate = 0.002122:   Batch Loss = 0.386475, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.413032263517, Accuracy = 0.97496086359\n",
            "Iter #2150400:  Learning rate = 0.002122:   Batch Loss = 0.402042, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.450481712818, Accuracy = 0.961050271988\n",
            "Iter #2154496:  Learning rate = 0.002122:   Batch Loss = 0.414009, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.451721966267, Accuracy = 0.96418017149\n",
            "Iter #2158592:  Learning rate = 0.002122:   Batch Loss = 0.416220, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.463879704475, Accuracy = 0.957050919533\n",
            "Iter #2162688:  Learning rate = 0.002122:   Batch Loss = 0.407508, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.436128824949, Accuracy = 0.964527904987\n",
            "Iter #2166784:  Learning rate = 0.002122:   Batch Loss = 0.396687, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.423342168331, Accuracy = 0.969396650791\n",
            "Iter #2170880:  Learning rate = 0.002122:   Batch Loss = 0.373692, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.433943271637, Accuracy = 0.965049564838\n",
            "Iter #2174976:  Learning rate = 0.002122:   Batch Loss = 0.395006, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.431035339832, Accuracy = 0.966962277889\n",
            "Iter #2179072:  Learning rate = 0.002122:   Batch Loss = 0.368976, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.413257300854, Accuracy = 0.972700417042\n",
            "Iter #2183168:  Learning rate = 0.002122:   Batch Loss = 0.380026, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.425464034081, Accuracy = 0.966614484787\n",
            "Iter #2187264:  Learning rate = 0.002122:   Batch Loss = 0.356204, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.442054003477, Accuracy = 0.963832378387\n",
            "Iter #2191360:  Learning rate = 0.002122:   Batch Loss = 0.380434, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.469312787056, Accuracy = 0.954616606236\n",
            "Iter #2195456:  Learning rate = 0.002122:   Batch Loss = 0.369587, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.428749442101, Accuracy = 0.968701124191\n",
            "Iter #2199552:  Learning rate = 0.002122:   Batch Loss = 0.392616, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.425896376371, Accuracy = 0.968701124191\n",
            "Iter #2203648:  Learning rate = 0.002037:   Batch Loss = 0.364619, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.413450479507, Accuracy = 0.973395943642\n",
            "Iter #2207744:  Learning rate = 0.002037:   Batch Loss = 0.383997, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.40610525012, Accuracy = 0.97356981039\n",
            "Iter #2211840:  Learning rate = 0.002037:   Batch Loss = 0.418140, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.417777389288, Accuracy = 0.970787703991\n",
            "Iter #2215936:  Learning rate = 0.002037:   Batch Loss = 0.380900, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.418761372566, Accuracy = 0.96957051754\n",
            "Iter #2220032:  Learning rate = 0.002037:   Batch Loss = 0.365723, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43367382884, Accuracy = 0.966614484787\n",
            "Iter #2224128:  Learning rate = 0.002037:   Batch Loss = 0.414894, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.426306128502, Accuracy = 0.968005537987\n",
            "Iter #2228224:  Learning rate = 0.002037:   Batch Loss = 0.370521, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.410564690828, Accuracy = 0.973395943642\n",
            "Iter #2232320:  Learning rate = 0.002037:   Batch Loss = 0.409631, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.418619662523, Accuracy = 0.967657804489\n",
            "Iter #2236416:  Learning rate = 0.002037:   Batch Loss = 0.445039, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.515637874603, Accuracy = 0.937402188778\n",
            "Iter #2240512:  Learning rate = 0.002037:   Batch Loss = 0.428896, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.487974584103, Accuracy = 0.948704600334\n",
            "Iter #2244608:  Learning rate = 0.002037:   Batch Loss = 0.435983, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.450056552887, Accuracy = 0.954616606236\n",
            "Iter #2248704:  Learning rate = 0.002037:   Batch Loss = 0.370457, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.426783502102, Accuracy = 0.961050271988\n",
            "Iter #2252800:  Learning rate = 0.002037:   Batch Loss = 0.380385, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.419357210398, Accuracy = 0.96887499094\n",
            "Iter #2256896:  Learning rate = 0.002037:   Batch Loss = 0.380550, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.419193685055, Accuracy = 0.969222724438\n",
            "Iter #2260992:  Learning rate = 0.002037:   Batch Loss = 0.370327, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.433300673962, Accuracy = 0.963832378387\n",
            "Iter #2265088:  Learning rate = 0.002037:   Batch Loss = 0.364485, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.435742229223, Accuracy = 0.959137558937\n",
            "Iter #2269184:  Learning rate = 0.002037:   Batch Loss = 0.381888, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.455920010805, Accuracy = 0.962093532085\n",
            "Iter #2273280:  Learning rate = 0.002037:   Batch Loss = 0.441908, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.50057309866, Accuracy = 0.94522690773\n",
            "Iter #2277376:  Learning rate = 0.002037:   Batch Loss = 0.424232, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.472481966019, Accuracy = 0.953747153282\n",
            "Iter #2281472:  Learning rate = 0.002037:   Batch Loss = 0.395980, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.428866535425, Accuracy = 0.964527904987\n",
            "Iter #2285568:  Learning rate = 0.002037:   Batch Loss = 0.367549, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.410006731749, Accuracy = 0.971830964088\n",
            "Iter #2289664:  Learning rate = 0.002037:   Batch Loss = 0.404882, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.430842041969, Accuracy = 0.96678841114\n",
            "Iter #2293760:  Learning rate = 0.002037:   Batch Loss = 0.396824, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.417545348406, Accuracy = 0.968353331089\n",
            "Iter #2297856:  Learning rate = 0.002037:   Batch Loss = 0.404571, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.416582942009, Accuracy = 0.969396650791\n",
            "Iter #2301952:  Learning rate = 0.001955:   Batch Loss = 0.379290, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.422596633434, Accuracy = 0.967831671238\n",
            "Iter #2306048:  Learning rate = 0.001955:   Batch Loss = 0.358042, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.412950128317, Accuracy = 0.970787703991\n",
            "Iter #2310144:  Learning rate = 0.001955:   Batch Loss = 0.375359, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.39901086688, Accuracy = 0.975482523441\n",
            "Iter #2314240:  Learning rate = 0.001955:   Batch Loss = 0.370601, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.412731230259, Accuracy = 0.97165709734\n",
            "Iter #2318336:  Learning rate = 0.001955:   Batch Loss = 0.395114, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.431547343731, Accuracy = 0.963484585285\n",
            "Iter #2322432:  Learning rate = 0.001955:   Batch Loss = 0.370132, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.408371448517, Accuracy = 0.970787703991\n",
            "Iter #2326528:  Learning rate = 0.001955:   Batch Loss = 0.390608, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.417089015245, Accuracy = 0.968701124191\n",
            "Iter #2330624:  Learning rate = 0.001955:   Batch Loss = 0.349529, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.420966148376, Accuracy = 0.968353331089\n",
            "Iter #2334720:  Learning rate = 0.001955:   Batch Loss = 0.349883, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.411563545465, Accuracy = 0.970092177391\n",
            "Iter #2338816:  Learning rate = 0.001955:   Batch Loss = 0.371974, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.40349856019, Accuracy = 0.970439910889\n",
            "Iter #2342912:  Learning rate = 0.001955:   Batch Loss = 0.358229, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.406071335077, Accuracy = 0.973048150539\n",
            "Iter #2347008:  Learning rate = 0.001955:   Batch Loss = 0.377047, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.399378091097, Accuracy = 0.972874283791\n",
            "Iter #2351104:  Learning rate = 0.001955:   Batch Loss = 0.354679, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.416768312454, Accuracy = 0.96748393774\n",
            "Iter #2355200:  Learning rate = 0.001955:   Batch Loss = 0.372591, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.400670617819, Accuracy = 0.97235262394\n",
            "Iter #2359296:  Learning rate = 0.001955:   Batch Loss = 0.363209, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.426866889, Accuracy = 0.963484585285\n",
            "Iter #2363392:  Learning rate = 0.001955:   Batch Loss = 0.376388, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.439688622952, Accuracy = 0.959485292435\n",
            "Iter #2367488:  Learning rate = 0.001955:   Batch Loss = 0.385785, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43168002367, Accuracy = 0.963136851788\n",
            "Iter #2371584:  Learning rate = 0.001955:   Batch Loss = 0.392604, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4634552598, Accuracy = 0.954616606236\n",
            "Iter #2375680:  Learning rate = 0.001955:   Batch Loss = 0.358316, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.421065241098, Accuracy = 0.962789058685\n",
            "Iter #2379776:  Learning rate = 0.001955:   Batch Loss = 0.419138, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.423555284739, Accuracy = 0.966092824936\n",
            "Iter #2383872:  Learning rate = 0.001955:   Batch Loss = 0.359012, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.447295159101, Accuracy = 0.957398712635\n",
            "Iter #2387968:  Learning rate = 0.001955:   Batch Loss = 0.404623, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.436815142632, Accuracy = 0.960702478886\n",
            "Iter #2392064:  Learning rate = 0.001955:   Batch Loss = 0.399985, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.433606445789, Accuracy = 0.963136851788\n",
            "Iter #2396160:  Learning rate = 0.001955:   Batch Loss = 0.394587, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.409995555878, Accuracy = 0.96748393774\n",
            "Iter #2400256:  Learning rate = 0.001877:   Batch Loss = 0.380982, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.411398768425, Accuracy = 0.967310011387\n",
            "Iter #2404352:  Learning rate = 0.001877:   Batch Loss = 0.364217, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.399380147457, Accuracy = 0.974613130093\n",
            "Iter #2408448:  Learning rate = 0.001877:   Batch Loss = 0.358396, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.413220256567, Accuracy = 0.969222724438\n",
            "Iter #2412544:  Learning rate = 0.001877:   Batch Loss = 0.371308, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.405398488045, Accuracy = 0.969222724438\n",
            "Iter #2416640:  Learning rate = 0.001877:   Batch Loss = 0.357157, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.440313816071, Accuracy = 0.957398712635\n",
            "Iter #2420736:  Learning rate = 0.001877:   Batch Loss = 0.356505, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.393554538488, Accuracy = 0.973395943642\n",
            "Iter #2424832:  Learning rate = 0.001877:   Batch Loss = 0.373780, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.397170245647, Accuracy = 0.971483230591\n",
            "Iter #2428928:  Learning rate = 0.001877:   Batch Loss = 0.376659, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.40302759409, Accuracy = 0.971830964088\n",
            "Iter #2433024:  Learning rate = 0.001877:   Batch Loss = 0.354061, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.395852446556, Accuracy = 0.973917603493\n",
            "Iter #2437120:  Learning rate = 0.001877:   Batch Loss = 0.383439, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.418888390064, Accuracy = 0.96817946434\n",
            "Iter #2441216:  Learning rate = 0.001877:   Batch Loss = 0.348810, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.427592754364, Accuracy = 0.964701771736\n",
            "Iter #2445312:  Learning rate = 0.001877:   Batch Loss = 0.370501, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.418048411608, Accuracy = 0.968353331089\n",
            "Iter #2449408:  Learning rate = 0.001877:   Batch Loss = 0.408474, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4184692204, Accuracy = 0.965571224689\n",
            "Iter #2453504:  Learning rate = 0.001877:   Batch Loss = 0.356340, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.409770131111, Accuracy = 0.971309363842\n",
            "Iter #2457600:  Learning rate = 0.001877:   Batch Loss = 0.362500, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.416153907776, Accuracy = 0.963832378387\n",
            "Iter #2461696:  Learning rate = 0.001877:   Batch Loss = 0.357569, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.39131385088, Accuracy = 0.975308656693\n",
            "Iter #2465792:  Learning rate = 0.001877:   Batch Loss = 0.369281, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.417873382568, Accuracy = 0.965049564838\n",
            "Iter #2469888:  Learning rate = 0.001877:   Batch Loss = 0.346095, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.396736294031, Accuracy = 0.973395943642\n",
            "Iter #2473984:  Learning rate = 0.001877:   Batch Loss = 0.416727, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.415301799774, Accuracy = 0.968527197838\n",
            "Iter #2478080:  Learning rate = 0.001877:   Batch Loss = 0.374376, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.40934535861, Accuracy = 0.96487569809\n",
            "Iter #2482176:  Learning rate = 0.001877:   Batch Loss = 0.375179, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.434316277504, Accuracy = 0.962789058685\n",
            "Iter #2486272:  Learning rate = 0.001877:   Batch Loss = 0.346566, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.392976999283, Accuracy = 0.972178757191\n",
            "Iter #2490368:  Learning rate = 0.001877:   Batch Loss = 0.364990, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.412261873484, Accuracy = 0.969744384289\n",
            "Iter #2494464:  Learning rate = 0.001877:   Batch Loss = 0.353828, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.409300625324, Accuracy = 0.96887499094\n",
            "Iter #2498560:  Learning rate = 0.001877:   Batch Loss = 0.346928, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41073936224, Accuracy = 0.971135437489\n",
            "Iter #2502656:  Learning rate = 0.001802:   Batch Loss = 0.341073, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.404696017504, Accuracy = 0.97096157074\n",
            "Iter #2506752:  Learning rate = 0.001802:   Batch Loss = 0.361745, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.399912625551, Accuracy = 0.972178757191\n",
            "Iter #2510848:  Learning rate = 0.001802:   Batch Loss = 0.360077, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.394435077906, Accuracy = 0.975830316544\n",
            "Iter #2514944:  Learning rate = 0.001802:   Batch Loss = 0.344843, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.399820894003, Accuracy = 0.970092177391\n",
            "Iter #2519040:  Learning rate = 0.001802:   Batch Loss = 0.336409, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.402312994003, Accuracy = 0.971309363842\n",
            "Iter #2523136:  Learning rate = 0.001802:   Batch Loss = 0.356973, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.396092951298, Accuracy = 0.969918251038\n",
            "Iter #2527232:  Learning rate = 0.001802:   Batch Loss = 0.347515, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.392690330744, Accuracy = 0.973048150539\n",
            "Iter #2531328:  Learning rate = 0.001802:   Batch Loss = 0.338479, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.399046361446, Accuracy = 0.970092177391\n",
            "Iter #2535424:  Learning rate = 0.001802:   Batch Loss = 0.335416, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.40144598484, Accuracy = 0.973048150539\n",
            "Iter #2539520:  Learning rate = 0.001802:   Batch Loss = 0.330595, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.408322513103, Accuracy = 0.97496086359\n",
            "Iter #2543616:  Learning rate = 0.001802:   Batch Loss = 0.385950, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.418959408998, Accuracy = 0.964006245136\n",
            "Iter #2547712:  Learning rate = 0.001802:   Batch Loss = 0.355630, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.411669701338, Accuracy = 0.969222724438\n",
            "Iter #2551808:  Learning rate = 0.001802:   Batch Loss = 0.333152, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.412540495396, Accuracy = 0.965571224689\n",
            "Iter #2555904:  Learning rate = 0.001802:   Batch Loss = 0.369942, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.408281028271, Accuracy = 0.970787703991\n",
            "Iter #2560000:  Learning rate = 0.001802:   Batch Loss = 0.358418, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.391255170107, Accuracy = 0.973048150539\n",
            "Iter #2564096:  Learning rate = 0.001802:   Batch Loss = 0.358431, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.428265869617, Accuracy = 0.96748393774\n",
            "Iter #2568192:  Learning rate = 0.001802:   Batch Loss = 0.352914, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.398512721062, Accuracy = 0.970613777637\n",
            "Iter #2572288:  Learning rate = 0.001802:   Batch Loss = 0.363838, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.440023958683, Accuracy = 0.959833085537\n",
            "Iter #2576384:  Learning rate = 0.001802:   Batch Loss = 0.366432, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.394012510777, Accuracy = 0.97235262394\n",
            "Iter #2580480:  Learning rate = 0.001802:   Batch Loss = 0.394253, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.421066015959, Accuracy = 0.964354038239\n",
            "Iter #2584576:  Learning rate = 0.001802:   Batch Loss = 0.365571, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.412097752094, Accuracy = 0.963310718536\n",
            "Iter #2588672:  Learning rate = 0.001802:   Batch Loss = 0.616673, Accuracy = 0.88671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.506295859814, Accuracy = 0.933402895927\n",
            "Iter #2592768:  Learning rate = 0.001802:   Batch Loss = 0.413512, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.486771583557, Accuracy = 0.941575407982\n",
            "Iter #2596864:  Learning rate = 0.001802:   Batch Loss = 0.420405, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.434861660004, Accuracy = 0.961919665337\n",
            "Iter #2600960:  Learning rate = 0.001730:   Batch Loss = 0.411781, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.450975090265, Accuracy = 0.958789765835\n",
            "Iter #2605056:  Learning rate = 0.001730:   Batch Loss = 0.358187, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.436757087708, Accuracy = 0.959833085537\n",
            "Iter #2609152:  Learning rate = 0.001730:   Batch Loss = 0.379623, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.420028328896, Accuracy = 0.970787703991\n",
            "Iter #2613248:  Learning rate = 0.001730:   Batch Loss = 0.379076, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.412792742252, Accuracy = 0.967310011387\n",
            "Iter #2617344:  Learning rate = 0.001730:   Batch Loss = 0.367615, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.433738857508, Accuracy = 0.960354745388\n",
            "Iter #2621440:  Learning rate = 0.001730:   Batch Loss = 0.404870, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.439745873213, Accuracy = 0.961224138737\n",
            "Iter #2625536:  Learning rate = 0.001730:   Batch Loss = 0.348002, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.40011882782, Accuracy = 0.970092177391\n",
            "Iter #2629632:  Learning rate = 0.001730:   Batch Loss = 0.335111, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.395606696606, Accuracy = 0.970439910889\n",
            "Iter #2633728:  Learning rate = 0.001730:   Batch Loss = 0.363191, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.394126087427, Accuracy = 0.969918251038\n",
            "Iter #2637824:  Learning rate = 0.001730:   Batch Loss = 0.340702, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.418941617012, Accuracy = 0.966614484787\n",
            "Iter #2641920:  Learning rate = 0.001730:   Batch Loss = 0.357054, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.417563319206, Accuracy = 0.970613777637\n",
            "Iter #2646016:  Learning rate = 0.001730:   Batch Loss = 0.363914, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.425999492407, Accuracy = 0.965745091438\n",
            "Iter #2650112:  Learning rate = 0.001730:   Batch Loss = 0.331648, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.397011220455, Accuracy = 0.971483230591\n",
            "Iter #2654208:  Learning rate = 0.001730:   Batch Loss = 0.343088, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.382546842098, Accuracy = 0.97235262394\n",
            "Iter #2658304:  Learning rate = 0.001730:   Batch Loss = 0.353071, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.399414092302, Accuracy = 0.97026604414\n",
            "Iter #2662400:  Learning rate = 0.001730:   Batch Loss = 0.346413, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.420635670424, Accuracy = 0.962962985039\n",
            "Iter #2666496:  Learning rate = 0.001730:   Batch Loss = 0.345989, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.390082597733, Accuracy = 0.97356981039\n",
            "Iter #2670592:  Learning rate = 0.001730:   Batch Loss = 0.347146, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.389469534159, Accuracy = 0.972700417042\n",
            "Iter #2674688:  Learning rate = 0.001730:   Batch Loss = 0.341970, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.39459630847, Accuracy = 0.97165709734\n",
            "Iter #2678784:  Learning rate = 0.001730:   Batch Loss = 0.343998, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.419993907213, Accuracy = 0.961919665337\n",
            "Iter #2682880:  Learning rate = 0.001730:   Batch Loss = 0.333814, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.400365233421, Accuracy = 0.96887499094\n",
            "Iter #2686976:  Learning rate = 0.001730:   Batch Loss = 0.347566, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.397194057703, Accuracy = 0.969048857689\n",
            "Iter #2691072:  Learning rate = 0.001730:   Batch Loss = 0.340348, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.387040585279, Accuracy = 0.973917603493\n",
            "Iter #2695168:  Learning rate = 0.001730:   Batch Loss = 0.337607, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.387572526932, Accuracy = 0.973917603493\n",
            "Iter #2699264:  Learning rate = 0.001730:   Batch Loss = 0.324514, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.386890798807, Accuracy = 0.972700417042\n",
            "Iter #2703360:  Learning rate = 0.001661:   Batch Loss = 0.329022, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.391834139824, Accuracy = 0.969918251038\n",
            "Iter #2707456:  Learning rate = 0.001661:   Batch Loss = 0.323772, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.401954084635, Accuracy = 0.969396650791\n",
            "Iter #2711552:  Learning rate = 0.001661:   Batch Loss = 0.344134, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.395889371634, Accuracy = 0.972004890442\n",
            "Iter #2715648:  Learning rate = 0.001661:   Batch Loss = 0.325750, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.383603215218, Accuracy = 0.97565639019\n",
            "Iter #2719744:  Learning rate = 0.001661:   Batch Loss = 0.339066, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.384837716818, Accuracy = 0.972874283791\n",
            "Iter #2723840:  Learning rate = 0.001661:   Batch Loss = 0.330916, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.398109614849, Accuracy = 0.969744384289\n",
            "Iter #2727936:  Learning rate = 0.001661:   Batch Loss = 0.461730, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.454812765121, Accuracy = 0.948530673981\n",
            "Iter #2732032:  Learning rate = 0.001661:   Batch Loss = 0.363047, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.403760910034, Accuracy = 0.96418017149\n",
            "Iter #2736128:  Learning rate = 0.001661:   Batch Loss = 0.371180, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.390400022268, Accuracy = 0.96887499094\n",
            "Iter #2740224:  Learning rate = 0.001661:   Batch Loss = 0.356800, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.384617894888, Accuracy = 0.97356981039\n",
            "Iter #2744320:  Learning rate = 0.001661:   Batch Loss = 0.363588, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.391562283039, Accuracy = 0.970787703991\n",
            "Iter #2748416:  Learning rate = 0.001661:   Batch Loss = 0.337117, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.393669188023, Accuracy = 0.967831671238\n",
            "Iter #2752512:  Learning rate = 0.001661:   Batch Loss = 0.356676, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.389825314283, Accuracy = 0.970787703991\n",
            "Iter #2756608:  Learning rate = 0.001661:   Batch Loss = 0.345609, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.383285045624, Accuracy = 0.973048150539\n",
            "Iter #2760704:  Learning rate = 0.001661:   Batch Loss = 0.345166, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.37908449769, Accuracy = 0.97565639019\n",
            "Iter #2764800:  Learning rate = 0.001661:   Batch Loss = 0.321281, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.375912666321, Accuracy = 0.974439203739\n",
            "Iter #2768896:  Learning rate = 0.001661:   Batch Loss = 0.329195, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.373217523098, Accuracy = 0.97565639019\n",
            "Iter #2772992:  Learning rate = 0.001661:   Batch Loss = 0.329009, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.40047454834, Accuracy = 0.96678841114\n",
            "Iter #2777088:  Learning rate = 0.001661:   Batch Loss = 0.341605, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.381311446428, Accuracy = 0.974613130093\n",
            "Iter #2781184:  Learning rate = 0.001661:   Batch Loss = 0.339466, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.403043478727, Accuracy = 0.965049564838\n",
            "Iter #2785280:  Learning rate = 0.001661:   Batch Loss = 0.347428, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.382811009884, Accuracy = 0.972526490688\n",
            "Iter #2789376:  Learning rate = 0.001661:   Batch Loss = 0.358944, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.393577069044, Accuracy = 0.972004890442\n",
            "Iter #2793472:  Learning rate = 0.001661:   Batch Loss = 0.356406, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.378008633852, Accuracy = 0.975482523441\n",
            "Iter #2797568:  Learning rate = 0.001661:   Batch Loss = 0.343081, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.403255939484, Accuracy = 0.970092177391\n",
            "Iter #2801664:  Learning rate = 0.001594:   Batch Loss = 0.353704, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.404518485069, Accuracy = 0.96678841114\n",
            "Iter #2805760:  Learning rate = 0.001594:   Batch Loss = 0.381587, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.399031162262, Accuracy = 0.96957051754\n",
            "Iter #2809856:  Learning rate = 0.001594:   Batch Loss = 0.350624, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.390468925238, Accuracy = 0.971309363842\n",
            "Iter #2813952:  Learning rate = 0.001594:   Batch Loss = 0.350082, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.388966858387, Accuracy = 0.971309363842\n",
            "Iter #2818048:  Learning rate = 0.001594:   Batch Loss = 0.324716, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41689324379, Accuracy = 0.963484585285\n",
            "Iter #2822144:  Learning rate = 0.001594:   Batch Loss = 0.333764, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.391179531813, Accuracy = 0.96817946434\n",
            "Iter #2826240:  Learning rate = 0.001594:   Batch Loss = 0.330159, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.371997177601, Accuracy = 0.974613130093\n",
            "Iter #2830336:  Learning rate = 0.001594:   Batch Loss = 0.321363, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.383120208979, Accuracy = 0.971135437489\n",
            "Iter #2834432:  Learning rate = 0.001594:   Batch Loss = 0.339396, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.38868522644, Accuracy = 0.97235262394\n",
            "Iter #2838528:  Learning rate = 0.001594:   Batch Loss = 0.348218, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.374263077974, Accuracy = 0.97426533699\n",
            "Iter #2842624:  Learning rate = 0.001594:   Batch Loss = 0.358712, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.376545488834, Accuracy = 0.976873576641\n",
            "Iter #2846720:  Learning rate = 0.001594:   Batch Loss = 0.332803, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.391830563545, Accuracy = 0.969048857689\n",
            "Iter #2850816:  Learning rate = 0.001594:   Batch Loss = 0.400606, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.447430342436, Accuracy = 0.954964339733\n",
            "Iter #2854912:  Learning rate = 0.001594:   Batch Loss = 0.334533, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.456754088402, Accuracy = 0.962267458439\n",
            "Iter #2859008:  Learning rate = 0.001594:   Batch Loss = 0.334364, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.377420276403, Accuracy = 0.970787703991\n",
            "Iter #2863104:  Learning rate = 0.001594:   Batch Loss = 0.364378, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.381593436003, Accuracy = 0.974786996841\n",
            "Iter #2867200:  Learning rate = 0.001594:   Batch Loss = 0.325590, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.417877614498, Accuracy = 0.960702478886\n",
            "Iter #2871296:  Learning rate = 0.001594:   Batch Loss = 0.318222, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.414246857166, Accuracy = 0.966440618038\n",
            "Iter #2875392:  Learning rate = 0.001594:   Batch Loss = 0.329910, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.373777329922, Accuracy = 0.97565639019\n",
            "Iter #2879488:  Learning rate = 0.001594:   Batch Loss = 0.325598, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.368442982435, Accuracy = 0.977221369743\n",
            "Iter #2883584:  Learning rate = 0.001594:   Batch Loss = 0.445738, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.540144205093, Accuracy = 0.94800901413\n",
            "Iter #2887680:  Learning rate = 0.001594:   Batch Loss = 0.413438, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.437665700912, Accuracy = 0.950443387032\n",
            "Iter #2891776:  Learning rate = 0.001594:   Batch Loss = 0.356751, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.431218326092, Accuracy = 0.950791180134\n",
            "Iter #2895872:  Learning rate = 0.001594:   Batch Loss = 0.385133, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.422661721706, Accuracy = 0.960702478886\n",
            "Iter #2899968:  Learning rate = 0.001594:   Batch Loss = 0.362368, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.381503522396, Accuracy = 0.972178757191\n",
            "Iter #2904064:  Learning rate = 0.001531:   Batch Loss = 0.353374, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.392624557018, Accuracy = 0.971483230591\n",
            "Iter #2908160:  Learning rate = 0.001531:   Batch Loss = 0.481719, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.486424714327, Accuracy = 0.939488768578\n",
            "Iter #2912256:  Learning rate = 0.001531:   Batch Loss = 0.363617, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.411679834127, Accuracy = 0.966962277889\n",
            "Iter #2916352:  Learning rate = 0.001531:   Batch Loss = 0.346162, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.400818914175, Accuracy = 0.970787703991\n",
            "Iter #2920448:  Learning rate = 0.001531:   Batch Loss = 0.337226, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.393640637398, Accuracy = 0.971483230591\n",
            "Iter #2924544:  Learning rate = 0.001531:   Batch Loss = 0.340673, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.395576655865, Accuracy = 0.97096157074\n",
            "Iter #2928640:  Learning rate = 0.001531:   Batch Loss = 0.346776, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.383364289999, Accuracy = 0.97096157074\n",
            "Iter #2932736:  Learning rate = 0.001531:   Batch Loss = 0.338254, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.382034778595, Accuracy = 0.972874283791\n",
            "Iter #2936832:  Learning rate = 0.001531:   Batch Loss = 0.331659, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.380660891533, Accuracy = 0.974091470242\n",
            "Iter #2940928:  Learning rate = 0.001531:   Batch Loss = 0.332036, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.377349555492, Accuracy = 0.973743677139\n",
            "Iter #2945024:  Learning rate = 0.001531:   Batch Loss = 0.320914, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.366276264191, Accuracy = 0.975830316544\n",
            "Iter #2949120:  Learning rate = 0.001531:   Batch Loss = 0.316087, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.379622370005, Accuracy = 0.975830316544\n",
            "Iter #2953216:  Learning rate = 0.001531:   Batch Loss = 0.326486, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.38909727335, Accuracy = 0.96957051754\n",
            "Iter #2957312:  Learning rate = 0.001531:   Batch Loss = 0.338071, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.398292273283, Accuracy = 0.969222724438\n",
            "Iter #2961408:  Learning rate = 0.001531:   Batch Loss = 0.319516, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.389313429594, Accuracy = 0.972526490688\n",
            "Iter #2965504:  Learning rate = 0.001531:   Batch Loss = 0.342302, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.368098199368, Accuracy = 0.976178050041\n",
            "Iter #2969600:  Learning rate = 0.001531:   Batch Loss = 0.316623, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.381161570549, Accuracy = 0.972526490688\n",
            "Iter #2973696:  Learning rate = 0.001531:   Batch Loss = 0.311037, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.367252290249, Accuracy = 0.976004183292\n",
            "Iter #2977792:  Learning rate = 0.001531:   Batch Loss = 0.309084, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.361844837666, Accuracy = 0.977916896343\n",
            "Iter #2981888:  Learning rate = 0.001531:   Batch Loss = 0.309635, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.38207384944, Accuracy = 0.97496086359\n",
            "Iter #2985984:  Learning rate = 0.001531:   Batch Loss = 0.321415, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.36883559823, Accuracy = 0.977221369743\n",
            "Iter #2990080:  Learning rate = 0.001531:   Batch Loss = 0.321282, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.368069320917, Accuracy = 0.977743029594\n",
            "Iter #2994176:  Learning rate = 0.001531:   Batch Loss = 0.326677, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.374102920294, Accuracy = 0.973917603493\n",
            "Iter #2998272:  Learning rate = 0.001531:   Batch Loss = 0.320812, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.391253709793, Accuracy = 0.969222724438\n",
            "Iter #3002368:  Learning rate = 0.001469:   Batch Loss = 0.326439, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.382120400667, Accuracy = 0.973048150539\n",
            "Iter #3006464:  Learning rate = 0.001469:   Batch Loss = 0.323391, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.371435046196, Accuracy = 0.975308656693\n",
            "Iter #3010560:  Learning rate = 0.001469:   Batch Loss = 0.337112, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.376964360476, Accuracy = 0.972178757191\n",
            "Iter #3014656:  Learning rate = 0.001469:   Batch Loss = 0.335308, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.378087878227, Accuracy = 0.972874283791\n",
            "Iter #3018752:  Learning rate = 0.001469:   Batch Loss = 0.311499, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.362962603569, Accuracy = 0.975134730339\n",
            "Iter #3022848:  Learning rate = 0.001469:   Batch Loss = 0.317756, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.358332127333, Accuracy = 0.979134082794\n",
            "Iter #3026944:  Learning rate = 0.001469:   Batch Loss = 0.315340, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.360550224781, Accuracy = 0.976178050041\n",
            "Iter #3031040:  Learning rate = 0.001469:   Batch Loss = 0.331216, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.379035383463, Accuracy = 0.97356981039\n",
            "Iter #3035136:  Learning rate = 0.001469:   Batch Loss = 0.318261, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.375813543797, Accuracy = 0.973743677139\n",
            "Iter #3039232:  Learning rate = 0.001469:   Batch Loss = 0.323080, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.385957449675, Accuracy = 0.969222724438\n",
            "Iter #3043328:  Learning rate = 0.001469:   Batch Loss = 0.299002, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.374308496714, Accuracy = 0.973222076893\n",
            "Iter #3047424:  Learning rate = 0.001469:   Batch Loss = 0.318913, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.366778403521, Accuracy = 0.976873576641\n",
            "Iter #3051520:  Learning rate = 0.001469:   Batch Loss = 0.332767, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.389256119728, Accuracy = 0.968005537987\n",
            "Iter #3055616:  Learning rate = 0.001469:   Batch Loss = 0.324115, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.375310242176, Accuracy = 0.972004890442\n",
            "Iter #3059712:  Learning rate = 0.001469:   Batch Loss = 0.347971, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.379183858633, Accuracy = 0.97235262394\n",
            "Iter #3063808:  Learning rate = 0.001469:   Batch Loss = 0.318106, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.374205827713, Accuracy = 0.97426533699\n",
            "Iter #3067904:  Learning rate = 0.001469:   Batch Loss = 0.347997, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.381346464157, Accuracy = 0.973395943642\n",
            "Iter #3072000:  Learning rate = 0.001469:   Batch Loss = 0.308498, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.376812636852, Accuracy = 0.976004183292\n",
            "Iter #3076096:  Learning rate = 0.001469:   Batch Loss = 0.317922, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.359431177378, Accuracy = 0.97704744339\n",
            "Iter #3080192:  Learning rate = 0.001469:   Batch Loss = 0.330409, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.371535480022, Accuracy = 0.974786996841\n",
            "Iter #3084288:  Learning rate = 0.001469:   Batch Loss = 0.328604, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.376467168331, Accuracy = 0.974439203739\n",
            "Iter #3088384:  Learning rate = 0.001469:   Batch Loss = 0.320775, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.402007222176, Accuracy = 0.972526490688\n",
            "Iter #3092480:  Learning rate = 0.001469:   Batch Loss = 0.328674, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.356012403965, Accuracy = 0.980177342892\n",
            "Iter #3096576:  Learning rate = 0.001469:   Batch Loss = 0.323332, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.3581404984, Accuracy = 0.977221369743\n",
            "Iter #3100672:  Learning rate = 0.001411:   Batch Loss = 0.317362, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.366143614054, Accuracy = 0.977221369743\n",
            "Iter #3104768:  Learning rate = 0.001411:   Batch Loss = 0.324858, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.368435591459, Accuracy = 0.976873576641\n",
            "Iter #3108864:  Learning rate = 0.001411:   Batch Loss = 0.324055, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.369387477636, Accuracy = 0.974091470242\n",
            "Iter #3112960:  Learning rate = 0.001411:   Batch Loss = 0.313888, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.368558675051, Accuracy = 0.974786996841\n",
            "Iter #3117056:  Learning rate = 0.001411:   Batch Loss = 0.306937, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.372579455376, Accuracy = 0.976699709892\n",
            "Iter #3121152:  Learning rate = 0.001411:   Batch Loss = 0.324709, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.382666200399, Accuracy = 0.970092177391\n",
            "Iter #3125248:  Learning rate = 0.001411:   Batch Loss = 0.298609, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.355693697929, Accuracy = 0.977395236492\n",
            "Iter #3129344:  Learning rate = 0.001411:   Batch Loss = 0.324851, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.360828816891, Accuracy = 0.976525843143\n",
            "Iter #3133440:  Learning rate = 0.001411:   Batch Loss = 0.306627, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.379256248474, Accuracy = 0.973395943642\n",
            "Iter #3137536:  Learning rate = 0.001411:   Batch Loss = 0.318076, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.372827142477, Accuracy = 0.975482523441\n",
            "Iter #3141632:  Learning rate = 0.001411:   Batch Loss = 0.325485, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.368246406317, Accuracy = 0.975308656693\n",
            "Iter #3145728:  Learning rate = 0.001411:   Batch Loss = 0.319891, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.363541066647, Accuracy = 0.974786996841\n",
            "Iter #3149824:  Learning rate = 0.001411:   Batch Loss = 0.337614, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.377700656652, Accuracy = 0.975830316544\n",
            "Iter #3153920:  Learning rate = 0.001411:   Batch Loss = 0.306691, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.36238181591, Accuracy = 0.97635191679\n",
            "Iter #3158016:  Learning rate = 0.001411:   Batch Loss = 0.331918, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.380841851234, Accuracy = 0.973743677139\n",
            "Iter #3162112:  Learning rate = 0.001411:   Batch Loss = 0.344557, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.397535175085, Accuracy = 0.966962277889\n",
            "Iter #3166208:  Learning rate = 0.001411:   Batch Loss = 0.321506, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.380178630352, Accuracy = 0.969744384289\n",
            "Iter #3170304:  Learning rate = 0.001411:   Batch Loss = 0.380163, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.399609565735, Accuracy = 0.966440618038\n",
            "Iter #3174400:  Learning rate = 0.001411:   Batch Loss = 0.312767, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.376081556082, Accuracy = 0.973048150539\n",
            "Iter #3178496:  Learning rate = 0.001411:   Batch Loss = 0.323592, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.365363836288, Accuracy = 0.97496086359\n",
            "Iter #3182592:  Learning rate = 0.001411:   Batch Loss = 0.306136, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.374826014042, Accuracy = 0.971135437489\n",
            "Iter #3186688:  Learning rate = 0.001411:   Batch Loss = 0.317594, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.38019451499, Accuracy = 0.970439910889\n",
            "Iter #3190784:  Learning rate = 0.001411:   Batch Loss = 0.313588, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.364430308342, Accuracy = 0.974786996841\n",
            "Iter #3194880:  Learning rate = 0.001411:   Batch Loss = 0.311386, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.378087878227, Accuracy = 0.970092177391\n",
            "Iter #3198976:  Learning rate = 0.001411:   Batch Loss = 0.310084, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.36483964324, Accuracy = 0.97565639019\n",
            "Iter #3203072:  Learning rate = 0.001354:   Batch Loss = 0.338423, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.40252995491, Accuracy = 0.964527904987\n",
            "Iter #3207168:  Learning rate = 0.001354:   Batch Loss = 0.338763, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.380594730377, Accuracy = 0.970613777637\n",
            "Iter #3211264:  Learning rate = 0.001354:   Batch Loss = 0.315013, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.370949327946, Accuracy = 0.975134730339\n",
            "Iter #3215360:  Learning rate = 0.001354:   Batch Loss = 0.312457, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.37006983161, Accuracy = 0.975482523441\n",
            "Iter #3219456:  Learning rate = 0.001354:   Batch Loss = 0.298953, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.375471711159, Accuracy = 0.971309363842\n",
            "Iter #3223552:  Learning rate = 0.001354:   Batch Loss = 0.314484, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.359920442104, Accuracy = 0.97426533699\n",
            "Iter #3227648:  Learning rate = 0.001354:   Batch Loss = 0.300442, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.354550868273, Accuracy = 0.97704744339\n",
            "Iter #3231744:  Learning rate = 0.001354:   Batch Loss = 0.319122, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.37014901638, Accuracy = 0.974613130093\n",
            "Iter #3235840:  Learning rate = 0.001354:   Batch Loss = 0.310674, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.383154720068, Accuracy = 0.969744384289\n",
            "Iter #3239936:  Learning rate = 0.001354:   Batch Loss = 0.326636, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.379285395145, Accuracy = 0.971135437489\n",
            "Iter #3244032:  Learning rate = 0.001354:   Batch Loss = 0.317379, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.3845692873, Accuracy = 0.964527904987\n",
            "Iter #3248128:  Learning rate = 0.001354:   Batch Loss = 0.321725, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.368590056896, Accuracy = 0.972874283791\n",
            "Iter #3252224:  Learning rate = 0.001354:   Batch Loss = 0.300721, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.369082570076, Accuracy = 0.972874283791\n",
            "Iter #3256320:  Learning rate = 0.001354:   Batch Loss = 0.302001, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.360616087914, Accuracy = 0.977743029594\n",
            "Iter #3260416:  Learning rate = 0.001354:   Batch Loss = 0.320818, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.371203899384, Accuracy = 0.973743677139\n",
            "Iter #3264512:  Learning rate = 0.001354:   Batch Loss = 0.320644, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.358615666628, Accuracy = 0.977221369743\n",
            "Iter #3268608:  Learning rate = 0.001354:   Batch Loss = 0.294746, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.365302652121, Accuracy = 0.978612422943\n",
            "Iter #3272704:  Learning rate = 0.001354:   Batch Loss = 0.302761, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.364043414593, Accuracy = 0.976699709892\n",
            "Iter #3276800:  Learning rate = 0.001354:   Batch Loss = 0.325565, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.368290066719, Accuracy = 0.976525843143\n",
            "Iter #3280896:  Learning rate = 0.001354:   Batch Loss = 0.295255, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.361550718546, Accuracy = 0.976525843143\n",
            "Iter #3284992:  Learning rate = 0.001354:   Batch Loss = 0.312952, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.375355988741, Accuracy = 0.974439203739\n",
            "Iter #3289088:  Learning rate = 0.001354:   Batch Loss = 0.324391, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.359277248383, Accuracy = 0.97635191679\n",
            "Iter #3293184:  Learning rate = 0.001354:   Batch Loss = 0.299764, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.361672341824, Accuracy = 0.975830316544\n",
            "Iter #3297280:  Learning rate = 0.001354:   Batch Loss = 0.306286, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.377019941807, Accuracy = 0.972178757191\n",
            "Iter #3301376:  Learning rate = 0.001300:   Batch Loss = 0.334615, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.389388680458, Accuracy = 0.964006245136\n",
            "Iter #3305472:  Learning rate = 0.001300:   Batch Loss = 0.338147, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.372577667236, Accuracy = 0.970439910889\n",
            "Iter #3309568:  Learning rate = 0.001300:   Batch Loss = 0.341634, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4022102952, Accuracy = 0.965918958187\n",
            "Iter #3313664:  Learning rate = 0.001300:   Batch Loss = 0.333192, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.376847088337, Accuracy = 0.969048857689\n",
            "Iter #3317760:  Learning rate = 0.001300:   Batch Loss = 0.309864, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.364866614342, Accuracy = 0.972178757191\n",
            "Iter #3321856:  Learning rate = 0.001300:   Batch Loss = 0.307519, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.363511383533, Accuracy = 0.97635191679\n",
            "Iter #3325952:  Learning rate = 0.001300:   Batch Loss = 0.300863, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.35366910696, Accuracy = 0.976699709892\n",
            "Iter #3330048:  Learning rate = 0.001300:   Batch Loss = 0.296344, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.386909663677, Accuracy = 0.972874283791\n",
            "Iter #3334144:  Learning rate = 0.001300:   Batch Loss = 0.315978, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.364121794701, Accuracy = 0.97356981039\n",
            "Iter #3338240:  Learning rate = 0.001300:   Batch Loss = 0.303660, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.361448407173, Accuracy = 0.974439203739\n",
            "Iter #3342336:  Learning rate = 0.001300:   Batch Loss = 0.315786, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.358848839998, Accuracy = 0.97635191679\n",
            "Iter #3346432:  Learning rate = 0.001300:   Batch Loss = 0.317597, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.358639150858, Accuracy = 0.977569103241\n",
            "Iter #3350528:  Learning rate = 0.001300:   Batch Loss = 0.329075, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.38000485301, Accuracy = 0.96678841114\n",
            "Iter #3354624:  Learning rate = 0.001300:   Batch Loss = 0.320503, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.374307066202, Accuracy = 0.970092177391\n",
            "Iter #3358720:  Learning rate = 0.001300:   Batch Loss = 0.304912, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.35756534338, Accuracy = 0.976525843143\n",
            "Iter #3362816:  Learning rate = 0.001300:   Batch Loss = 0.324694, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.374152928591, Accuracy = 0.97026604414\n",
            "Iter #3366912:  Learning rate = 0.001300:   Batch Loss = 0.316756, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.36374554038, Accuracy = 0.972874283791\n",
            "Iter #3371008:  Learning rate = 0.001300:   Batch Loss = 0.308238, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.356182217598, Accuracy = 0.975482523441\n",
            "Iter #3375104:  Learning rate = 0.001300:   Batch Loss = 0.317641, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.362931787968, Accuracy = 0.974439203739\n",
            "Iter #3379200:  Learning rate = 0.001300:   Batch Loss = 0.298219, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.363008856773, Accuracy = 0.973743677139\n",
            "Iter #3383296:  Learning rate = 0.001300:   Batch Loss = 0.297310, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.365239590406, Accuracy = 0.976004183292\n",
            "Iter #3387392:  Learning rate = 0.001300:   Batch Loss = 0.293469, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.357734560966, Accuracy = 0.974439203739\n",
            "Iter #3391488:  Learning rate = 0.001300:   Batch Loss = 0.308596, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.360773652792, Accuracy = 0.97356981039\n",
            "Iter #3395584:  Learning rate = 0.001300:   Batch Loss = 0.317842, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.372274756432, Accuracy = 0.96817946434\n",
            "Iter #3399680:  Learning rate = 0.001300:   Batch Loss = 0.307030, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.372675508261, Accuracy = 0.972004890442\n",
            "Iter #3403776:  Learning rate = 0.001248:   Batch Loss = 0.296557, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.366043955088, Accuracy = 0.974613130093\n",
            "Iter #3407872:  Learning rate = 0.001248:   Batch Loss = 0.322717, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.362601697445, Accuracy = 0.976699709892\n",
            "Iter #3411968:  Learning rate = 0.001248:   Batch Loss = 0.303934, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.361948907375, Accuracy = 0.976004183292\n",
            "Iter #3416064:  Learning rate = 0.001248:   Batch Loss = 0.378407, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.429017066956, Accuracy = 0.951834440231\n",
            "Iter #3420160:  Learning rate = 0.001248:   Batch Loss = 0.313603, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.368606775999, Accuracy = 0.968353331089\n",
            "Iter #3424256:  Learning rate = 0.001248:   Batch Loss = 0.301366, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.354251623154, Accuracy = 0.976525843143\n",
            "Iter #3428352:  Learning rate = 0.001248:   Batch Loss = 0.319600, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.372236222029, Accuracy = 0.969918251038\n",
            "Iter #3432448:  Learning rate = 0.001248:   Batch Loss = 0.314654, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.352721452713, Accuracy = 0.973917603493\n",
            "Iter #3436544:  Learning rate = 0.001248:   Batch Loss = 0.310202, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.353011995554, Accuracy = 0.97635191679\n",
            "Iter #3440640:  Learning rate = 0.001248:   Batch Loss = 0.292419, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.360374420881, Accuracy = 0.973048150539\n",
            "Iter #3444736:  Learning rate = 0.001248:   Batch Loss = 0.301904, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.368334561586, Accuracy = 0.97235262394\n",
            "Iter #3448832:  Learning rate = 0.001248:   Batch Loss = 0.321477, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.371444284916, Accuracy = 0.972178757191\n",
            "Iter #3452928:  Learning rate = 0.001248:   Batch Loss = 0.310393, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.362741887569, Accuracy = 0.972004890442\n",
            "Iter #3457024:  Learning rate = 0.001248:   Batch Loss = 0.289608, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.36279964447, Accuracy = 0.972874283791\n",
            "Iter #3461120:  Learning rate = 0.001248:   Batch Loss = 0.301963, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.374917060137, Accuracy = 0.968353331089\n",
            "Iter #3465216:  Learning rate = 0.001248:   Batch Loss = 0.306736, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.362670779228, Accuracy = 0.971830964088\n",
            "Iter #3469312:  Learning rate = 0.001248:   Batch Loss = 0.305194, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.353358179331, Accuracy = 0.975482523441\n",
            "Iter #3473408:  Learning rate = 0.001248:   Batch Loss = 0.315830, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.349601984024, Accuracy = 0.976178050041\n",
            "Iter #3477504:  Learning rate = 0.001248:   Batch Loss = 0.312731, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.356185376644, Accuracy = 0.972700417042\n",
            "Iter #3481600:  Learning rate = 0.001248:   Batch Loss = 0.290935, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.355714917183, Accuracy = 0.97496086359\n",
            "Iter #3485696:  Learning rate = 0.001248:   Batch Loss = 0.295103, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.344179451466, Accuracy = 0.977916896343\n",
            "Iter #3489792:  Learning rate = 0.001248:   Batch Loss = 0.302718, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.350766897202, Accuracy = 0.976178050041\n",
            "Iter #3493888:  Learning rate = 0.001248:   Batch Loss = 0.294688, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.352770417929, Accuracy = 0.975830316544\n",
            "Iter #3497984:  Learning rate = 0.001248:   Batch Loss = 0.298675, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.362715631723, Accuracy = 0.971309363842\n",
            "Iter #3502080:  Learning rate = 0.001198:   Batch Loss = 0.297498, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.38175368309, Accuracy = 0.968353331089\n",
            "Iter #3506176:  Learning rate = 0.001198:   Batch Loss = 0.302063, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.357271134853, Accuracy = 0.97235262394\n",
            "Iter #3510272:  Learning rate = 0.001198:   Batch Loss = 0.299165, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.358356297016, Accuracy = 0.972526490688\n",
            "Iter #3514368:  Learning rate = 0.001198:   Batch Loss = 0.303372, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.35239776969, Accuracy = 0.973395943642\n",
            "Iter #3518464:  Learning rate = 0.001198:   Batch Loss = 0.298185, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.342235565186, Accuracy = 0.976699709892\n",
            "Iter #3522560:  Learning rate = 0.001198:   Batch Loss = 0.296650, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.33974096179, Accuracy = 0.977743029594\n",
            "Iter #3526656:  Learning rate = 0.001198:   Batch Loss = 0.300523, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.354876607656, Accuracy = 0.97496086359\n",
            "Iter #3530752:  Learning rate = 0.001198:   Batch Loss = 0.325097, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.361576229334, Accuracy = 0.972004890442\n",
            "Iter #3534848:  Learning rate = 0.001198:   Batch Loss = 0.308005, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.357200324535, Accuracy = 0.973917603493\n",
            "Iter #3538944:  Learning rate = 0.001198:   Batch Loss = 0.289925, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.353255689144, Accuracy = 0.975308656693\n",
            "Iter #3543040:  Learning rate = 0.001198:   Batch Loss = 0.293590, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.34987154603, Accuracy = 0.976178050041\n",
            "Iter #3547136:  Learning rate = 0.001198:   Batch Loss = 0.293404, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.338557332754, Accuracy = 0.977743029594\n",
            "Iter #3551232:  Learning rate = 0.001198:   Batch Loss = 0.288165, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.347544908524, Accuracy = 0.976873576641\n",
            "Iter #3555328:  Learning rate = 0.001198:   Batch Loss = 0.290302, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.345446914434, Accuracy = 0.976525843143\n",
            "Iter #3559424:  Learning rate = 0.001198:   Batch Loss = 0.284905, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.357100009918, Accuracy = 0.973048150539\n",
            "Iter #3563520:  Learning rate = 0.001198:   Batch Loss = 0.295310, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.34673255682, Accuracy = 0.977569103241\n",
            "Iter #3567616:  Learning rate = 0.001198:   Batch Loss = 0.293855, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.376454591751, Accuracy = 0.968527197838\n",
            "Iter #3571712:  Learning rate = 0.001198:   Batch Loss = 0.288785, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.354580521584, Accuracy = 0.975308656693\n",
            "Iter #3575808:  Learning rate = 0.001198:   Batch Loss = 0.302912, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.371543318033, Accuracy = 0.972526490688\n",
            "Iter #3579904:  Learning rate = 0.001198:   Batch Loss = 0.325013, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.357310652733, Accuracy = 0.97026604414\n",
            "Iter #3584000:  Learning rate = 0.001198:   Batch Loss = 0.300709, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.342720270157, Accuracy = 0.976178050041\n",
            "Iter #3588096:  Learning rate = 0.001198:   Batch Loss = 0.283119, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.339141219854, Accuracy = 0.977916896343\n",
            "Iter #3592192:  Learning rate = 0.001198:   Batch Loss = 0.286353, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.343476951122, Accuracy = 0.976699709892\n",
            "Iter #3596288:  Learning rate = 0.001198:   Batch Loss = 0.290083, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.353764057159, Accuracy = 0.975482523441\n",
            "Iter #3600384:  Learning rate = 0.001150:   Batch Loss = 0.307556, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.349449515343, Accuracy = 0.976004183292\n",
            "Iter #3604480:  Learning rate = 0.001150:   Batch Loss = 0.303732, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.340668588877, Accuracy = 0.978960156441\n",
            "Iter #3608576:  Learning rate = 0.001150:   Batch Loss = 0.286026, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.342617422342, Accuracy = 0.976699709892\n",
            "Iter #3612672:  Learning rate = 0.001150:   Batch Loss = 0.294983, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.342657089233, Accuracy = 0.979481816292\n",
            "Iter #3616768:  Learning rate = 0.001150:   Batch Loss = 0.302358, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.348583817482, Accuracy = 0.97565639019\n",
            "Iter #3620864:  Learning rate = 0.001150:   Batch Loss = 0.288579, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.342485278845, Accuracy = 0.979481816292\n",
            "Iter #3624960:  Learning rate = 0.001150:   Batch Loss = 0.316527, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.339244544506, Accuracy = 0.980351269245\n",
            "Iter #3629056:  Learning rate = 0.001150:   Batch Loss = 0.305574, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.3428568542, Accuracy = 0.978612422943\n",
            "Iter #3633152:  Learning rate = 0.001150:   Batch Loss = 0.302222, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.347452223301, Accuracy = 0.976004183292\n",
            "Iter #3637248:  Learning rate = 0.001150:   Batch Loss = 0.312465, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.338519275188, Accuracy = 0.978090763092\n",
            "Iter #3641344:  Learning rate = 0.001150:   Batch Loss = 0.282749, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.343596309423, Accuracy = 0.979481816292\n",
            "Iter #3645440:  Learning rate = 0.001150:   Batch Loss = 0.298762, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.363967955112, Accuracy = 0.972874283791\n",
            "Iter #3649536:  Learning rate = 0.001150:   Batch Loss = 0.283807, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.341599106789, Accuracy = 0.977569103241\n",
            "Iter #3653632:  Learning rate = 0.001150:   Batch Loss = 0.291596, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.376456230879, Accuracy = 0.972178757191\n",
            "Iter #3657728:  Learning rate = 0.001150:   Batch Loss = 0.288324, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.345059365034, Accuracy = 0.979307949543\n",
            "Iter #3661824:  Learning rate = 0.001150:   Batch Loss = 0.310919, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.344891190529, Accuracy = 0.977569103241\n",
            "Iter #3665920:  Learning rate = 0.001150:   Batch Loss = 0.295270, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.330852210522, Accuracy = 0.982090055943\n",
            "Iter #3670016:  Learning rate = 0.001150:   Batch Loss = 0.295379, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.343364655972, Accuracy = 0.978612422943\n",
            "Iter #3674112:  Learning rate = 0.001150:   Batch Loss = 0.307003, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.349512398243, Accuracy = 0.976178050041\n",
            "Iter #3678208:  Learning rate = 0.001150:   Batch Loss = 0.282339, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.344761192799, Accuracy = 0.977221369743\n",
            "Iter #3682304:  Learning rate = 0.001150:   Batch Loss = 0.314086, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.345592737198, Accuracy = 0.97635191679\n",
            "Iter #3686400:  Learning rate = 0.001150:   Batch Loss = 0.282216, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.344840109348, Accuracy = 0.978264629841\n",
            "Iter #3690496:  Learning rate = 0.001150:   Batch Loss = 0.298071, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.358961284161, Accuracy = 0.974439203739\n",
            "Iter #3694592:  Learning rate = 0.001150:   Batch Loss = 0.282472, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.340954482555, Accuracy = 0.978090763092\n",
            "Iter #3698688:  Learning rate = 0.001150:   Batch Loss = 0.308522, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.362065941095, Accuracy = 0.974439203739\n",
            "Iter #3702784:  Learning rate = 0.001104:   Batch Loss = 0.284926, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.352382898331, Accuracy = 0.97426533699\n",
            "Iter #3706880:  Learning rate = 0.001104:   Batch Loss = 0.287956, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.345855683088, Accuracy = 0.977221369743\n",
            "Iter #3710976:  Learning rate = 0.001104:   Batch Loss = 0.300242, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.33875605464, Accuracy = 0.979655683041\n",
            "Iter #3715072:  Learning rate = 0.001104:   Batch Loss = 0.321809, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.359602212906, Accuracy = 0.974439203739\n",
            "Iter #3719168:  Learning rate = 0.001104:   Batch Loss = 0.322578, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.357654690742, Accuracy = 0.973048150539\n",
            "Iter #3723264:  Learning rate = 0.001104:   Batch Loss = 0.305643, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.35657402873, Accuracy = 0.97235262394\n",
            "Iter #3727360:  Learning rate = 0.001104:   Batch Loss = 0.296753, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.348566353321, Accuracy = 0.975482523441\n",
            "Iter #3731456:  Learning rate = 0.001104:   Batch Loss = 0.300676, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.347326308489, Accuracy = 0.97356981039\n",
            "Iter #3735552:  Learning rate = 0.001104:   Batch Loss = 0.292910, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.335545301437, Accuracy = 0.977916896343\n",
            "Iter #3739648:  Learning rate = 0.001104:   Batch Loss = 0.280320, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.342940866947, Accuracy = 0.976873576641\n",
            "Iter #3743744:  Learning rate = 0.001104:   Batch Loss = 0.302070, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.376922190189, Accuracy = 0.969918251038\n",
            "Iter #3747840:  Learning rate = 0.001104:   Batch Loss = 0.318747, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.353541404009, Accuracy = 0.976004183292\n",
            "Iter #3751936:  Learning rate = 0.001104:   Batch Loss = 0.288344, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.349386572838, Accuracy = 0.97496086359\n",
            "Iter #3756032:  Learning rate = 0.001104:   Batch Loss = 0.284786, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.349167227745, Accuracy = 0.977916896343\n",
            "Iter #3760128:  Learning rate = 0.001104:   Batch Loss = 0.274466, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.341327726841, Accuracy = 0.978438556194\n",
            "Iter #3764224:  Learning rate = 0.001104:   Batch Loss = 0.300301, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.358283370733, Accuracy = 0.974439203739\n",
            "Iter #3768320:  Learning rate = 0.001104:   Batch Loss = 0.293527, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.346163868904, Accuracy = 0.977395236492\n",
            "Iter #3772416:  Learning rate = 0.001104:   Batch Loss = 0.290636, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.363717854023, Accuracy = 0.969918251038\n",
            "Iter #3776512:  Learning rate = 0.001104:   Batch Loss = 0.289974, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.34134170413, Accuracy = 0.977395236492\n",
            "Iter #3780608:  Learning rate = 0.001104:   Batch Loss = 0.288969, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.336416363716, Accuracy = 0.979655683041\n",
            "Iter #3784704:  Learning rate = 0.001104:   Batch Loss = 0.300569, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.331998735666, Accuracy = 0.980003476143\n",
            "Iter #3788800:  Learning rate = 0.001104:   Batch Loss = 0.315592, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.360050976276, Accuracy = 0.972004890442\n",
            "Iter #3792896:  Learning rate = 0.001104:   Batch Loss = 0.285087, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.337748944759, Accuracy = 0.977395236492\n",
            "Iter #3796992:  Learning rate = 0.001104:   Batch Loss = 0.317752, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.360080599785, Accuracy = 0.96748393774\n",
            "Iter #3801088:  Learning rate = 0.001060:   Batch Loss = 0.305119, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.353629499674, Accuracy = 0.973048150539\n",
            "Iter #3805184:  Learning rate = 0.001060:   Batch Loss = 0.278570, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.348148167133, Accuracy = 0.97356981039\n",
            "Iter #3809280:  Learning rate = 0.001060:   Batch Loss = 0.286970, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.343379825354, Accuracy = 0.973917603493\n",
            "Iter #3813376:  Learning rate = 0.001060:   Batch Loss = 0.291380, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.358635753393, Accuracy = 0.97426533699\n",
            "Iter #3817472:  Learning rate = 0.001060:   Batch Loss = 0.284204, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.336045861244, Accuracy = 0.979481816292\n",
            "Iter #3821568:  Learning rate = 0.001060:   Batch Loss = 0.289542, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.35368552804, Accuracy = 0.977221369743\n",
            "Iter #3825664:  Learning rate = 0.001060:   Batch Loss = 0.308976, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.353234142065, Accuracy = 0.97565639019\n",
            "Iter #3829760:  Learning rate = 0.001060:   Batch Loss = 0.317226, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.372009903193, Accuracy = 0.96817946434\n",
            "Iter #3833856:  Learning rate = 0.001060:   Batch Loss = 0.288145, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.39694672823, Accuracy = 0.961745798588\n",
            "Iter #3837952:  Learning rate = 0.001060:   Batch Loss = 0.296021, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.366185963154, Accuracy = 0.968701124191\n",
            "Iter #3842048:  Learning rate = 0.001060:   Batch Loss = 0.301519, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.357343733311, Accuracy = 0.972874283791\n",
            "Iter #3846144:  Learning rate = 0.001060:   Batch Loss = 0.300056, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.354538887739, Accuracy = 0.976178050041\n",
            "Iter #3850240:  Learning rate = 0.001060:   Batch Loss = 0.287266, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.347603261471, Accuracy = 0.974091470242\n",
            "Iter #3854336:  Learning rate = 0.001060:   Batch Loss = 0.296138, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.369666695595, Accuracy = 0.964006245136\n",
            "Iter #3858432:  Learning rate = 0.001060:   Batch Loss = 0.294080, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.938295781612, Accuracy = 0.836550176144\n",
            "Iter #3862528:  Learning rate = 0.001060:   Batch Loss = 0.328952, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.382978439331, Accuracy = 0.964701771736\n",
            "Iter #3866624:  Learning rate = 0.001060:   Batch Loss = 0.299310, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.351336926222, Accuracy = 0.972178757191\n",
            "Iter #3870720:  Learning rate = 0.001060:   Batch Loss = 0.319455, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.381695181131, Accuracy = 0.966092824936\n",
            "Iter #3874816:  Learning rate = 0.001060:   Batch Loss = 0.306874, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.374689847231, Accuracy = 0.967136144638\n",
            "Iter #3878912:  Learning rate = 0.001060:   Batch Loss = 0.291913, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.355714946985, Accuracy = 0.970092177391\n",
            "Iter #3883008:  Learning rate = 0.001060:   Batch Loss = 0.281749, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.369875639677, Accuracy = 0.96957051754\n",
            "Iter #3887104:  Learning rate = 0.001060:   Batch Loss = 0.308908, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.348912090063, Accuracy = 0.973395943642\n",
            "Iter #3891200:  Learning rate = 0.001060:   Batch Loss = 0.302514, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.341807156801, Accuracy = 0.97635191679\n",
            "Iter #3895296:  Learning rate = 0.001060:   Batch Loss = 0.291626, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.337646067142, Accuracy = 0.97635191679\n",
            "Iter #3899392:  Learning rate = 0.001060:   Batch Loss = 0.291138, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.341536521912, Accuracy = 0.977569103241\n",
            "Iter #3903488:  Learning rate = 0.001018:   Batch Loss = 0.287280, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.335612356663, Accuracy = 0.979134082794\n",
            "Iter #3907584:  Learning rate = 0.001018:   Batch Loss = 0.275085, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.343319803476, Accuracy = 0.97426533699\n",
            "Iter #3911680:  Learning rate = 0.001018:   Batch Loss = 0.284048, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.341617166996, Accuracy = 0.976699709892\n",
            "Iter #3915776:  Learning rate = 0.001018:   Batch Loss = 0.281031, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.338088542223, Accuracy = 0.977569103241\n",
            "Iter #3919872:  Learning rate = 0.001018:   Batch Loss = 0.281255, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.336165189743, Accuracy = 0.977743029594\n",
            "Iter #3923968:  Learning rate = 0.001018:   Batch Loss = 0.280564, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.333542138338, Accuracy = 0.979307949543\n",
            "Iter #3928064:  Learning rate = 0.001018:   Batch Loss = 0.284307, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.341931849718, Accuracy = 0.977569103241\n",
            "Iter #3932160:  Learning rate = 0.001018:   Batch Loss = 0.270619, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.335187464952, Accuracy = 0.978612422943\n",
            "Iter #3936256:  Learning rate = 0.001018:   Batch Loss = 0.274146, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.332158744335, Accuracy = 0.978438556194\n",
            "Iter #3940352:  Learning rate = 0.001018:   Batch Loss = 0.279004, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.352214992046, Accuracy = 0.974786996841\n",
            "Iter #3944448:  Learning rate = 0.001018:   Batch Loss = 0.280420, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.351715564728, Accuracy = 0.973222076893\n",
            "Iter #3948544:  Learning rate = 0.001018:   Batch Loss = 0.275393, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.341671705246, Accuracy = 0.975482523441\n",
            "Iter #3952640:  Learning rate = 0.001018:   Batch Loss = 0.277060, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.343469083309, Accuracy = 0.976699709892\n",
            "Iter #3956736:  Learning rate = 0.001018:   Batch Loss = 0.272615, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.36157515645, Accuracy = 0.971309363842\n",
            "Iter #3960832:  Learning rate = 0.001018:   Batch Loss = 0.272320, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.346647828817, Accuracy = 0.975308656693\n",
            "Iter #3964928:  Learning rate = 0.001018:   Batch Loss = 0.275729, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.337265253067, Accuracy = 0.978786289692\n",
            "Iter #3969024:  Learning rate = 0.001018:   Batch Loss = 0.286036, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.363343000412, Accuracy = 0.97096157074\n",
            "Iter #3973120:  Learning rate = 0.001018:   Batch Loss = 0.284482, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.333519518375, Accuracy = 0.978786289692\n",
            "Iter #3977216:  Learning rate = 0.001018:   Batch Loss = 0.269320, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.329870700836, Accuracy = 0.978960156441\n",
            "Iter #3981312:  Learning rate = 0.001018:   Batch Loss = 0.270215, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.334778100252, Accuracy = 0.978438556194\n",
            "Iter #3985408:  Learning rate = 0.001018:   Batch Loss = 0.269719, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.340190559626, Accuracy = 0.978264629841\n",
            "Iter #3989504:  Learning rate = 0.001018:   Batch Loss = 0.271136, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.33576181531, Accuracy = 0.978264629841\n",
            "Iter #3993600:  Learning rate = 0.001018:   Batch Loss = 0.280083, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.336062252522, Accuracy = 0.977221369743\n",
            "Iter #3997696:  Learning rate = 0.001018:   Batch Loss = 0.281012, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.362434118986, Accuracy = 0.972178757191\n",
            "Iter #4001792:  Learning rate = 0.000977:   Batch Loss = 0.300040, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.331706941128, Accuracy = 0.978438556194\n",
            "Iter #4005888:  Learning rate = 0.000977:   Batch Loss = 0.291048, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.325714439154, Accuracy = 0.978612422943\n",
            "Iter #4009984:  Learning rate = 0.000977:   Batch Loss = 0.278777, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.334654301405, Accuracy = 0.976178050041\n",
            "Iter #4014080:  Learning rate = 0.000977:   Batch Loss = 0.292413, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.344218432903, Accuracy = 0.972874283791\n",
            "Iter #4018176:  Learning rate = 0.000977:   Batch Loss = 0.267977, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.328417748213, Accuracy = 0.979307949543\n",
            "Iter #4022272:  Learning rate = 0.000977:   Batch Loss = 0.290428, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.331197082996, Accuracy = 0.978090763092\n",
            "Iter #4026368:  Learning rate = 0.000977:   Batch Loss = 0.290213, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.327230423689, Accuracy = 0.97704744339\n",
            "Iter #4030464:  Learning rate = 0.000977:   Batch Loss = 0.295534, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.340846002102, Accuracy = 0.975830316544\n",
            "Iter #4034560:  Learning rate = 0.000977:   Batch Loss = 0.270020, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.325152158737, Accuracy = 0.979307949543\n",
            "Iter #4038656:  Learning rate = 0.000977:   Batch Loss = 0.272817, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.331711411476, Accuracy = 0.979307949543\n",
            "Iter #4042752:  Learning rate = 0.000977:   Batch Loss = 0.291480, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.326402664185, Accuracy = 0.981916189194\n",
            "Iter #4046848:  Learning rate = 0.000977:   Batch Loss = 0.271471, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.332446694374, Accuracy = 0.97635191679\n",
            "Iter #4050944:  Learning rate = 0.000977:   Batch Loss = 0.270218, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.319852471352, Accuracy = 0.981046795845\n",
            "Iter #4055040:  Learning rate = 0.000977:   Batch Loss = 0.272384, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.320252895355, Accuracy = 0.981568396091\n",
            "Iter #4059136:  Learning rate = 0.000977:   Batch Loss = 0.272046, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.324421614408, Accuracy = 0.981046795845\n",
            "Iter #4063232:  Learning rate = 0.000977:   Batch Loss = 0.284235, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.329805314541, Accuracy = 0.977916896343\n",
            "Iter #4067328:  Learning rate = 0.000977:   Batch Loss = 0.282200, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.328429967165, Accuracy = 0.978438556194\n",
            "Iter #4071424:  Learning rate = 0.000977:   Batch Loss = 0.274056, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.324941009283, Accuracy = 0.978786289692\n",
            "Iter #4075520:  Learning rate = 0.000977:   Batch Loss = 0.300407, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.328429669142, Accuracy = 0.976004183292\n",
            "Iter #4079616:  Learning rate = 0.000977:   Batch Loss = 0.284689, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.321132421494, Accuracy = 0.981394529343\n",
            "Iter #4083712:  Learning rate = 0.000977:   Batch Loss = 0.288358, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.33322674036, Accuracy = 0.976525843143\n",
            "Iter #4087808:  Learning rate = 0.000977:   Batch Loss = 0.268298, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.332118451595, Accuracy = 0.978960156441\n",
            "Iter #4091904:  Learning rate = 0.000977:   Batch Loss = 0.275704, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.347702771425, Accuracy = 0.97496086359\n",
            "Iter #4096000:  Learning rate = 0.000977:   Batch Loss = 0.292066, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.339269846678, Accuracy = 0.976873576641\n",
            "Iter #4100096:  Learning rate = 0.000938:   Batch Loss = 0.287544, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.345124423504, Accuracy = 0.976178050041\n",
            "Iter #4104192:  Learning rate = 0.000938:   Batch Loss = 0.269584, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.340493023396, Accuracy = 0.974613130093\n",
            "Iter #4108288:  Learning rate = 0.000938:   Batch Loss = 0.293297, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.350351214409, Accuracy = 0.973743677139\n",
            "Iter #4112384:  Learning rate = 0.000938:   Batch Loss = 0.297447, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.341583043337, Accuracy = 0.97356981039\n",
            "Iter #4116480:  Learning rate = 0.000938:   Batch Loss = 0.280538, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.343553930521, Accuracy = 0.972178757191\n",
            "Iter #4120576:  Learning rate = 0.000938:   Batch Loss = 0.278333, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.339082092047, Accuracy = 0.975482523441\n",
            "Iter #4124672:  Learning rate = 0.000938:   Batch Loss = 0.300941, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.342822551727, Accuracy = 0.973917603493\n",
            "Iter #4128768:  Learning rate = 0.000938:   Batch Loss = 0.298655, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.360230356455, Accuracy = 0.969048857689\n",
            "Iter #4132864:  Learning rate = 0.000938:   Batch Loss = 0.302381, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.379396259785, Accuracy = 0.967657804489\n",
            "Iter #4136960:  Learning rate = 0.000938:   Batch Loss = 0.277478, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.361589252949, Accuracy = 0.968527197838\n",
            "Iter #4141056:  Learning rate = 0.000938:   Batch Loss = 0.287836, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.345920830965, Accuracy = 0.974091470242\n",
            "Iter #4145152:  Learning rate = 0.000938:   Batch Loss = 0.290625, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.328506678343, Accuracy = 0.976004183292\n",
            "Iter #4149248:  Learning rate = 0.000938:   Batch Loss = 0.286373, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.326551198959, Accuracy = 0.979829609394\n",
            "Iter #4153344:  Learning rate = 0.000938:   Batch Loss = 0.270720, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.346585869789, Accuracy = 0.974786996841\n",
            "Iter #4157440:  Learning rate = 0.000938:   Batch Loss = 0.286369, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.3341345191, Accuracy = 0.976873576641\n",
            "Iter #4161536:  Learning rate = 0.000938:   Batch Loss = 0.292486, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.35052934289, Accuracy = 0.972874283791\n",
            "Iter #4165632:  Learning rate = 0.000938:   Batch Loss = 0.284295, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.329778492451, Accuracy = 0.980351269245\n",
            "Iter #4169728:  Learning rate = 0.000938:   Batch Loss = 0.289902, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.333330780268, Accuracy = 0.979481816292\n",
            "Iter #4173824:  Learning rate = 0.000938:   Batch Loss = 0.277071, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.325829297304, Accuracy = 0.979134082794\n",
            "Iter #4177920:  Learning rate = 0.000938:   Batch Loss = 0.272702, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.332635253668, Accuracy = 0.979134082794\n",
            "Iter #4182016:  Learning rate = 0.000938:   Batch Loss = 0.271487, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.333461523056, Accuracy = 0.977221369743\n",
            "Iter #4186112:  Learning rate = 0.000938:   Batch Loss = 0.276829, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.336205422878, Accuracy = 0.97565639019\n",
            "Iter #4190208:  Learning rate = 0.000938:   Batch Loss = 0.263325, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.334661543369, Accuracy = 0.976525843143\n",
            "Iter #4194304:  Learning rate = 0.000938:   Batch Loss = 0.268577, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.335462093353, Accuracy = 0.976525843143\n",
            "Iter #4198400:  Learning rate = 0.000938:   Batch Loss = 0.275955, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.328122735023, Accuracy = 0.978264629841\n",
            "Iter #4202496:  Learning rate = 0.000900:   Batch Loss = 0.267422, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.32613503933, Accuracy = 0.978090763092\n",
            "Iter #4206592:  Learning rate = 0.000900:   Batch Loss = 0.271086, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.32627427578, Accuracy = 0.978612422943\n",
            "Iter #4210688:  Learning rate = 0.000900:   Batch Loss = 0.269732, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.312698066235, Accuracy = 0.981916189194\n",
            "Iter #4214784:  Learning rate = 0.000900:   Batch Loss = 0.262288, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.331751972437, Accuracy = 0.97426533699\n",
            "Iter #4218880:  Learning rate = 0.000900:   Batch Loss = 0.291240, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.328444182873, Accuracy = 0.976004183292\n",
            "Iter #4222976:  Learning rate = 0.000900:   Batch Loss = 0.271432, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.320633202791, Accuracy = 0.980003476143\n",
            "Iter #4227072:  Learning rate = 0.000900:   Batch Loss = 0.265353, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.316885769367, Accuracy = 0.980872869492\n",
            "Iter #4231168:  Learning rate = 0.000900:   Batch Loss = 0.264662, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.318877995014, Accuracy = 0.982437849045\n",
            "Iter #4235264:  Learning rate = 0.000900:   Batch Loss = 0.274857, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.333888858557, Accuracy = 0.978612422943\n",
            "Iter #4239360:  Learning rate = 0.000900:   Batch Loss = 0.264512, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.321048378944, Accuracy = 0.980177342892\n",
            "Iter #4243456:  Learning rate = 0.000900:   Batch Loss = 0.274268, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.338234126568, Accuracy = 0.97635191679\n",
            "Iter #4247552:  Learning rate = 0.000900:   Batch Loss = 0.280392, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.33119109273, Accuracy = 0.978264629841\n",
            "Iter #4251648:  Learning rate = 0.000900:   Batch Loss = 0.267199, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.332782268524, Accuracy = 0.975308656693\n",
            "Iter #4255744:  Learning rate = 0.000900:   Batch Loss = 0.273856, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.330049365759, Accuracy = 0.977221369743\n",
            "Iter #4259840:  Learning rate = 0.000900:   Batch Loss = 0.272272, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.327844649553, Accuracy = 0.978264629841\n",
            "Iter #4263936:  Learning rate = 0.000900:   Batch Loss = 0.266789, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.356329977512, Accuracy = 0.970613777637\n",
            "Iter #4268032:  Learning rate = 0.000900:   Batch Loss = 0.277616, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.317070305347, Accuracy = 0.980872869492\n",
            "Iter #4272128:  Learning rate = 0.000900:   Batch Loss = 0.266683, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.319884836674, Accuracy = 0.979307949543\n",
            "Iter #4276224:  Learning rate = 0.000900:   Batch Loss = 0.294746, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.323769360781, Accuracy = 0.977916896343\n",
            "Iter #4280320:  Learning rate = 0.000900:   Batch Loss = 0.263290, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.336634576321, Accuracy = 0.97356981039\n",
            "Iter #4284416:  Learning rate = 0.000900:   Batch Loss = 0.275384, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.341063916683, Accuracy = 0.976699709892\n",
            "Iter #4288512:  Learning rate = 0.000900:   Batch Loss = 0.282633, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.330004423857, Accuracy = 0.977221369743\n",
            "Iter #4292608:  Learning rate = 0.000900:   Batch Loss = 0.273961, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.328373223543, Accuracy = 0.978438556194\n",
            "Iter #4296704:  Learning rate = 0.000900:   Batch Loss = 0.260365, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.340765327215, Accuracy = 0.975308656693\n",
            "Iter #4300800:  Learning rate = 0.000864:   Batch Loss = 0.282630, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.35670787096, Accuracy = 0.968527197838\n",
            "Iter #4304896:  Learning rate = 0.000864:   Batch Loss = 0.273222, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.326163709164, Accuracy = 0.97635191679\n",
            "Iter #4308992:  Learning rate = 0.000864:   Batch Loss = 0.280458, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.341101020575, Accuracy = 0.975482523441\n",
            "Iter #4313088:  Learning rate = 0.000864:   Batch Loss = 0.269124, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.340524762869, Accuracy = 0.977569103241\n",
            "Iter #4317184:  Learning rate = 0.000864:   Batch Loss = 0.267696, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.330859094858, Accuracy = 0.979134082794\n",
            "Iter #4321280:  Learning rate = 0.000864:   Batch Loss = 0.257788, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.330492347479, Accuracy = 0.978264629841\n",
            "Iter #4325376:  Learning rate = 0.000864:   Batch Loss = 0.268168, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.347786217928, Accuracy = 0.974786996841\n",
            "Iter #4329472:  Learning rate = 0.000864:   Batch Loss = 0.274073, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.329868018627, Accuracy = 0.978090763092\n",
            "Iter #4333568:  Learning rate = 0.000864:   Batch Loss = 0.274041, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.330480992794, Accuracy = 0.976873576641\n",
            "Iter #4337664:  Learning rate = 0.000864:   Batch Loss = 0.261259, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.338419437408, Accuracy = 0.977221369743\n",
            "Iter #4341760:  Learning rate = 0.000864:   Batch Loss = 0.288953, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.336604118347, Accuracy = 0.975308656693\n",
            "Iter #4345856:  Learning rate = 0.000864:   Batch Loss = 0.271617, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.323462516069, Accuracy = 0.980177342892\n",
            "Iter #4349952:  Learning rate = 0.000864:   Batch Loss = 0.266521, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.324850797653, Accuracy = 0.980525135994\n",
            "Iter #4354048:  Learning rate = 0.000864:   Batch Loss = 0.266349, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.335451722145, Accuracy = 0.975308656693\n",
            "Iter #4358144:  Learning rate = 0.000864:   Batch Loss = 0.271664, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.334660470486, Accuracy = 0.976699709892\n",
            "Iter #4362240:  Learning rate = 0.000864:   Batch Loss = 0.270312, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.332685053349, Accuracy = 0.974786996841\n",
            "Iter #4366336:  Learning rate = 0.000864:   Batch Loss = 0.281924, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.323318719864, Accuracy = 0.978612422943\n",
            "Iter #4370432:  Learning rate = 0.000864:   Batch Loss = 0.263886, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.335388481617, Accuracy = 0.976525843143\n",
            "Iter #4374528:  Learning rate = 0.000864:   Batch Loss = 0.281096, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.344293653965, Accuracy = 0.972526490688\n",
            "Iter #4378624:  Learning rate = 0.000864:   Batch Loss = 0.284312, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.332356989384, Accuracy = 0.97565639019\n",
            "Iter #4382720:  Learning rate = 0.000864:   Batch Loss = 0.269601, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.331329792738, Accuracy = 0.978960156441\n",
            "Iter #4386816:  Learning rate = 0.000864:   Batch Loss = 0.271388, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.316173970699, Accuracy = 0.978612422943\n",
            "Iter #4390912:  Learning rate = 0.000864:   Batch Loss = 0.272125, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.314256906509, Accuracy = 0.980003476143\n",
            "Iter #4395008:  Learning rate = 0.000864:   Batch Loss = 0.287588, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.310345977545, Accuracy = 0.982437849045\n",
            "Iter #4399104:  Learning rate = 0.000864:   Batch Loss = 0.271699, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.342471092939, Accuracy = 0.974091470242\n",
            "Iter #4403200:  Learning rate = 0.000830:   Batch Loss = 0.280567, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.321960896254, Accuracy = 0.978786289692\n",
            "Iter #4407296:  Learning rate = 0.000830:   Batch Loss = 0.270149, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.318164825439, Accuracy = 0.978438556194\n",
            "Iter #4411392:  Learning rate = 0.000830:   Batch Loss = 0.266004, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.33544075489, Accuracy = 0.973048150539\n",
            "Iter #4415488:  Learning rate = 0.000830:   Batch Loss = 0.279122, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.329185426235, Accuracy = 0.97356981039\n",
            "Iter #4419584:  Learning rate = 0.000830:   Batch Loss = 0.271319, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.366865724325, Accuracy = 0.96748393774\n",
            "Iter #4423680:  Learning rate = 0.000830:   Batch Loss = 0.268637, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.3203805089, Accuracy = 0.978960156441\n",
            "Iter #4427776:  Learning rate = 0.000830:   Batch Loss = 0.278779, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.328439295292, Accuracy = 0.977221369743\n",
            "Iter #4431872:  Learning rate = 0.000830:   Batch Loss = 0.263842, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.312288761139, Accuracy = 0.980525135994\n",
            "Iter #4435968:  Learning rate = 0.000830:   Batch Loss = 0.267239, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.317783236504, Accuracy = 0.978438556194\n",
            "Iter #4440064:  Learning rate = 0.000830:   Batch Loss = 0.294330, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.366856515408, Accuracy = 0.964006245136\n",
            "Iter #4444160:  Learning rate = 0.000830:   Batch Loss = 0.278035, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.34917268157, Accuracy = 0.97356981039\n",
            "Iter #4448256:  Learning rate = 0.000830:   Batch Loss = 0.284154, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.342015951872, Accuracy = 0.972178757191\n",
            "Iter #4452352:  Learning rate = 0.000830:   Batch Loss = 0.269974, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.332752287388, Accuracy = 0.974786996841\n",
            "Iter #4456448:  Learning rate = 0.000830:   Batch Loss = 0.273909, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.340300500393, Accuracy = 0.97356981039\n",
            "Iter #4460544:  Learning rate = 0.000830:   Batch Loss = 0.286823, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.331490367651, Accuracy = 0.974613130093\n",
            "Iter #4464640:  Learning rate = 0.000830:   Batch Loss = 0.274330, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.337106347084, Accuracy = 0.974439203739\n",
            "Iter #4468736:  Learning rate = 0.000830:   Batch Loss = 0.275189, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.326691061258, Accuracy = 0.976873576641\n",
            "Iter #4472832:  Learning rate = 0.000830:   Batch Loss = 0.270062, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.359547108412, Accuracy = 0.96957051754\n",
            "Iter #4476928:  Learning rate = 0.000830:   Batch Loss = 0.279737, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.360555261374, Accuracy = 0.967657804489\n",
            "Iter #4481024:  Learning rate = 0.000830:   Batch Loss = 0.273008, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.330785214901, Accuracy = 0.975830316544\n",
            "Iter #4485120:  Learning rate = 0.000830:   Batch Loss = 0.279614, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.352710485458, Accuracy = 0.971309363842\n",
            "Iter #4489216:  Learning rate = 0.000830:   Batch Loss = 0.262129, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.326909005642, Accuracy = 0.976699709892\n",
            "Iter #4493312:  Learning rate = 0.000830:   Batch Loss = 0.259452, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.333773910999, Accuracy = 0.976178050041\n",
            "Iter #4497408:  Learning rate = 0.000830:   Batch Loss = 0.261773, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.328043341637, Accuracy = 0.978612422943\n",
            "Iter #4501504:  Learning rate = 0.000796:   Batch Loss = 0.278975, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.336310654879, Accuracy = 0.975482523441\n",
            "Iter #4505600:  Learning rate = 0.000796:   Batch Loss = 0.269135, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.325624227524, Accuracy = 0.977395236492\n",
            "Iter #4509696:  Learning rate = 0.000796:   Batch Loss = 0.269313, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.323474109173, Accuracy = 0.979134082794\n",
            "Iter #4513792:  Learning rate = 0.000796:   Batch Loss = 0.269682, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.342255115509, Accuracy = 0.976004183292\n",
            "Iter #4517888:  Learning rate = 0.000796:   Batch Loss = 0.292030, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.314066559076, Accuracy = 0.980351269245\n",
            "Iter #4521984:  Learning rate = 0.000796:   Batch Loss = 0.291142, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.321484565735, Accuracy = 0.979655683041\n",
            "Iter #4526080:  Learning rate = 0.000796:   Batch Loss = 0.261312, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.319104373455, Accuracy = 0.97704744339\n",
            "Iter #4530176:  Learning rate = 0.000796:   Batch Loss = 0.272224, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.316722273827, Accuracy = 0.977395236492\n",
            "Iter #4534272:  Learning rate = 0.000796:   Batch Loss = 0.266079, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.320437192917, Accuracy = 0.977569103241\n",
            "Iter #4538368:  Learning rate = 0.000796:   Batch Loss = 0.253965, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.314622700214, Accuracy = 0.980003476143\n",
            "Iter #4542464:  Learning rate = 0.000796:   Batch Loss = 0.256428, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.321246832609, Accuracy = 0.980525135994\n",
            "Iter #4546560:  Learning rate = 0.000796:   Batch Loss = 0.260563, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.334862232208, Accuracy = 0.976178050041\n",
            "Iter #4550656:  Learning rate = 0.000796:   Batch Loss = 0.277266, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.338003396988, Accuracy = 0.97356981039\n",
            "Iter #4554752:  Learning rate = 0.000796:   Batch Loss = 0.262472, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.337616562843, Accuracy = 0.974613130093\n",
            "Iter #4558848:  Learning rate = 0.000796:   Batch Loss = 0.262023, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.319745540619, Accuracy = 0.980351269245\n",
            "Iter #4562944:  Learning rate = 0.000796:   Batch Loss = 0.263528, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.327112793922, Accuracy = 0.974439203739\n",
            "Iter #4567040:  Learning rate = 0.000796:   Batch Loss = 0.270261, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.320304691792, Accuracy = 0.979307949543\n",
            "Iter #4571136:  Learning rate = 0.000796:   Batch Loss = 0.259548, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.323077321053, Accuracy = 0.978264629841\n",
            "Iter #4575232:  Learning rate = 0.000796:   Batch Loss = 0.257115, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.326682984829, Accuracy = 0.977569103241\n",
            "Iter #4579328:  Learning rate = 0.000796:   Batch Loss = 0.276819, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.318728506565, Accuracy = 0.979655683041\n",
            "Iter #4583424:  Learning rate = 0.000796:   Batch Loss = 0.255997, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.32160359621, Accuracy = 0.977569103241\n",
            "Iter #4587520:  Learning rate = 0.000796:   Batch Loss = 0.275787, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.330136597157, Accuracy = 0.975482523441\n",
            "Iter #4591616:  Learning rate = 0.000796:   Batch Loss = 0.268908, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.328612595797, Accuracy = 0.977569103241\n",
            "Iter #4595712:  Learning rate = 0.000796:   Batch Loss = 0.259927, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.331967264414, Accuracy = 0.975134730339\n",
            "Iter #4599808:  Learning rate = 0.000796:   Batch Loss = 0.256269, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.326341927052, Accuracy = 0.978612422943\n",
            "Iter #4603904:  Learning rate = 0.000765:   Batch Loss = 0.264455, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.330937981606, Accuracy = 0.97426533699\n",
            "Iter #4608000:  Learning rate = 0.000765:   Batch Loss = 0.264140, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.324738383293, Accuracy = 0.978090763092\n",
            "Iter #4612096:  Learning rate = 0.000765:   Batch Loss = 0.265607, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.344452291727, Accuracy = 0.972700417042\n",
            "Iter #4616192:  Learning rate = 0.000765:   Batch Loss = 0.272156, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.329217195511, Accuracy = 0.974613130093\n",
            "Iter #4620288:  Learning rate = 0.000765:   Batch Loss = 0.273538, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.333487570286, Accuracy = 0.974091470242\n",
            "Iter #4624384:  Learning rate = 0.000765:   Batch Loss = 0.267205, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.313551366329, Accuracy = 0.979481816292\n",
            "Iter #4628480:  Learning rate = 0.000765:   Batch Loss = 0.272266, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.307003676891, Accuracy = 0.978960156441\n",
            "Iter #4632576:  Learning rate = 0.000765:   Batch Loss = 0.263759, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.314897626638, Accuracy = 0.979481816292\n",
            "Iter #4636672:  Learning rate = 0.000765:   Batch Loss = 0.254880, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.318209528923, Accuracy = 0.978786289692\n",
            "Iter #4640768:  Learning rate = 0.000765:   Batch Loss = 0.273640, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.317795276642, Accuracy = 0.978786289692\n",
            "Iter #4644864:  Learning rate = 0.000765:   Batch Loss = 0.253938, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.321532428265, Accuracy = 0.976525843143\n",
            "Iter #4648960:  Learning rate = 0.000765:   Batch Loss = 0.256942, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.318631798029, Accuracy = 0.976699709892\n",
            "Iter #4653056:  Learning rate = 0.000765:   Batch Loss = 0.257816, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.324391245842, Accuracy = 0.976873576641\n",
            "Iter #4657152:  Learning rate = 0.000765:   Batch Loss = 0.273928, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.309321522713, Accuracy = 0.980177342892\n",
            "Iter #4661248:  Learning rate = 0.000765:   Batch Loss = 0.255158, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.316489964724, Accuracy = 0.978612422943\n",
            "Iter #4665344:  Learning rate = 0.000765:   Batch Loss = 0.252085, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.3216316998, Accuracy = 0.978786289692\n",
            "Iter #4669440:  Learning rate = 0.000765:   Batch Loss = 0.255389, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.318562656641, Accuracy = 0.980351269245\n",
            "Iter #4673536:  Learning rate = 0.000765:   Batch Loss = 0.269977, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.332123041153, Accuracy = 0.976525843143\n",
            "Iter #4677632:  Learning rate = 0.000765:   Batch Loss = 0.274685, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.320669233799, Accuracy = 0.977221369743\n",
            "Iter #4681728:  Learning rate = 0.000765:   Batch Loss = 0.254036, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.30137732625, Accuracy = 0.981916189194\n",
            "Iter #4685824:  Learning rate = 0.000765:   Batch Loss = 0.263162, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.336411714554, Accuracy = 0.97426533699\n",
            "Iter #4689920:  Learning rate = 0.000765:   Batch Loss = 0.303925, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.356246173382, Accuracy = 0.965223431587\n",
            "Iter #4694016:  Learning rate = 0.000765:   Batch Loss = 0.264821, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.317064404488, Accuracy = 0.97426533699\n",
            "Iter #4698112:  Learning rate = 0.000765:   Batch Loss = 0.264813, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.339788287878, Accuracy = 0.975308656693\n",
            "Iter #4702208:  Learning rate = 0.000734:   Batch Loss = 0.269069, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.326822221279, Accuracy = 0.97565639019\n",
            "Iter #4706304:  Learning rate = 0.000734:   Batch Loss = 0.268825, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.301922559738, Accuracy = 0.981394529343\n",
            "Iter #4710400:  Learning rate = 0.000734:   Batch Loss = 0.264218, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.330016195774, Accuracy = 0.97426533699\n",
            "Iter #4714496:  Learning rate = 0.000734:   Batch Loss = 0.266930, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.313278794289, Accuracy = 0.978438556194\n",
            "Iter #4718592:  Learning rate = 0.000734:   Batch Loss = 0.262970, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.319101899862, Accuracy = 0.976699709892\n",
            "Iter #4722688:  Learning rate = 0.000734:   Batch Loss = 0.271194, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.316080302, Accuracy = 0.97635191679\n",
            "Iter #4726784:  Learning rate = 0.000734:   Batch Loss = 0.260186, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.309129446745, Accuracy = 0.979481816292\n",
            "Iter #4730880:  Learning rate = 0.000734:   Batch Loss = 0.258377, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.32167583704, Accuracy = 0.978786289692\n",
            "Iter #4734976:  Learning rate = 0.000734:   Batch Loss = 0.263197, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.325609356165, Accuracy = 0.977569103241\n",
            "Iter #4739072:  Learning rate = 0.000734:   Batch Loss = 0.266421, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.315659344196, Accuracy = 0.981916189194\n",
            "Iter #4743168:  Learning rate = 0.000734:   Batch Loss = 0.264055, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.315420657396, Accuracy = 0.97635191679\n",
            "Iter #4747264:  Learning rate = 0.000734:   Batch Loss = 0.257246, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.322059333324, Accuracy = 0.977569103241\n",
            "Iter #4751360:  Learning rate = 0.000734:   Batch Loss = 0.266118, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.320204019547, Accuracy = 0.976699709892\n",
            "Iter #4755456:  Learning rate = 0.000734:   Batch Loss = 0.262135, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.310383200645, Accuracy = 0.979829609394\n",
            "Iter #4759552:  Learning rate = 0.000734:   Batch Loss = 0.269011, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.313501954079, Accuracy = 0.977743029594\n",
            "Iter #4763648:  Learning rate = 0.000734:   Batch Loss = 0.255181, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.322633743286, Accuracy = 0.975830316544\n",
            "Iter #4767744:  Learning rate = 0.000734:   Batch Loss = 0.254542, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.330912292004, Accuracy = 0.97356981039\n",
            "Iter #4771840:  Learning rate = 0.000734:   Batch Loss = 0.263596, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.321827232838, Accuracy = 0.978264629841\n",
            "Iter #4775936:  Learning rate = 0.000734:   Batch Loss = 0.261415, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.322841137648, Accuracy = 0.977743029594\n",
            "Iter #4780032:  Learning rate = 0.000734:   Batch Loss = 0.266643, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.334641635418, Accuracy = 0.970092177391\n",
            "Iter #4784128:  Learning rate = 0.000734:   Batch Loss = 0.261357, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.327181398869, Accuracy = 0.976178050041\n",
            "Iter #4788224:  Learning rate = 0.000734:   Batch Loss = 0.297414, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.308793485165, Accuracy = 0.978960156441\n",
            "Iter #4792320:  Learning rate = 0.000734:   Batch Loss = 0.263869, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.313888907433, Accuracy = 0.979307949543\n",
            "Iter #4796416:  Learning rate = 0.000734:   Batch Loss = 0.255934, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.314571678638, Accuracy = 0.978438556194\n",
            "Iter #4800512:  Learning rate = 0.000705:   Batch Loss = 0.252480, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.329450428486, Accuracy = 0.97426533699\n",
            "Iter #4804608:  Learning rate = 0.000705:   Batch Loss = 0.255120, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.313836693764, Accuracy = 0.981568396091\n",
            "Iter #4808704:  Learning rate = 0.000705:   Batch Loss = 0.271843, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.327273696661, Accuracy = 0.977395236492\n",
            "Iter #4812800:  Learning rate = 0.000705:   Batch Loss = 0.281568, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.311104655266, Accuracy = 0.978960156441\n",
            "Iter #4816896:  Learning rate = 0.000705:   Batch Loss = 0.262057, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.318439364433, Accuracy = 0.980351269245\n",
            "Iter #4820992:  Learning rate = 0.000705:   Batch Loss = 0.289792, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.323853284121, Accuracy = 0.976004183292\n",
            "Iter #4825088:  Learning rate = 0.000705:   Batch Loss = 0.257028, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.321431934834, Accuracy = 0.976004183292\n",
            "Iter #4829184:  Learning rate = 0.000705:   Batch Loss = 0.256446, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.312343120575, Accuracy = 0.981568396091\n",
            "Iter #4833280:  Learning rate = 0.000705:   Batch Loss = 0.251687, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.313781172037, Accuracy = 0.977916896343\n",
            "Iter #4837376:  Learning rate = 0.000705:   Batch Loss = 0.253956, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296815544367, Accuracy = 0.983133375645\n",
            "Iter #4841472:  Learning rate = 0.000705:   Batch Loss = 0.252189, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.306647092104, Accuracy = 0.980177342892\n",
            "Iter #4845568:  Learning rate = 0.000705:   Batch Loss = 0.258519, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.304019868374, Accuracy = 0.981220662594\n",
            "Iter #4849664:  Learning rate = 0.000705:   Batch Loss = 0.255259, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.314834684134, Accuracy = 0.980699002743\n",
            "Iter #4853760:  Learning rate = 0.000705:   Batch Loss = 0.260352, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.33533641696, Accuracy = 0.97235262394\n",
            "Iter #4857856:  Learning rate = 0.000705:   Batch Loss = 0.254439, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.326587200165, Accuracy = 0.975308656693\n",
            "Iter #4861952:  Learning rate = 0.000705:   Batch Loss = 0.264409, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.317291080952, Accuracy = 0.980003476143\n",
            "Iter #4866048:  Learning rate = 0.000705:   Batch Loss = 0.258038, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.321141898632, Accuracy = 0.975830316544\n",
            "Iter #4870144:  Learning rate = 0.000705:   Batch Loss = 0.262233, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.336740225554, Accuracy = 0.97165709734\n",
            "Iter #4874240:  Learning rate = 0.000705:   Batch Loss = 0.267759, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.307895958424, Accuracy = 0.981742322445\n",
            "Iter #4878336:  Learning rate = 0.000705:   Batch Loss = 0.272988, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.317753553391, Accuracy = 0.977916896343\n",
            "Iter #4882432:  Learning rate = 0.000705:   Batch Loss = 0.258817, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.306501150131, Accuracy = 0.979134082794\n",
            "Iter #4886528:  Learning rate = 0.000705:   Batch Loss = 0.251602, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.307797402143, Accuracy = 0.979829609394\n",
            "Iter #4890624:  Learning rate = 0.000705:   Batch Loss = 0.253748, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.330412626266, Accuracy = 0.97496086359\n",
            "Iter #4894720:  Learning rate = 0.000705:   Batch Loss = 0.259599, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.345318078995, Accuracy = 0.972004890442\n",
            "Iter #4898816:  Learning rate = 0.000705:   Batch Loss = 0.270181, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.313044011593, Accuracy = 0.978264629841\n",
            "Iter #4902912:  Learning rate = 0.000676:   Batch Loss = 0.270747, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.325874984264, Accuracy = 0.974091470242\n",
            "Iter #4907008:  Learning rate = 0.000676:   Batch Loss = 0.255597, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.319364786148, Accuracy = 0.977916896343\n",
            "Iter #4911104:  Learning rate = 0.000676:   Batch Loss = 0.251097, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.319491416216, Accuracy = 0.978264629841\n",
            "Iter #4915200:  Learning rate = 0.000676:   Batch Loss = 0.256867, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.318970918655, Accuracy = 0.979134082794\n",
            "Iter #4919296:  Learning rate = 0.000676:   Batch Loss = 0.249986, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.312147319317, Accuracy = 0.980872869492\n",
            "Iter #4923392:  Learning rate = 0.000676:   Batch Loss = 0.253604, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.303167253733, Accuracy = 0.981916189194\n",
            "Iter #4927488:  Learning rate = 0.000676:   Batch Loss = 0.249934, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.316294342279, Accuracy = 0.979829609394\n",
            "Iter #4931584:  Learning rate = 0.000676:   Batch Loss = 0.265055, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.308902591467, Accuracy = 0.979829609394\n",
            "Iter #4935680:  Learning rate = 0.000676:   Batch Loss = 0.257910, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302689045668, Accuracy = 0.981568396091\n",
            "Iter #4939776:  Learning rate = 0.000676:   Batch Loss = 0.251242, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.315549105406, Accuracy = 0.978264629841\n",
            "Iter #4943872:  Learning rate = 0.000676:   Batch Loss = 0.252103, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.307797342539, Accuracy = 0.980699002743\n",
            "Iter #4947968:  Learning rate = 0.000676:   Batch Loss = 0.261983, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.314508795738, Accuracy = 0.979829609394\n",
            "Iter #4952064:  Learning rate = 0.000676:   Batch Loss = 0.262603, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.31740501523, Accuracy = 0.978438556194\n",
            "Iter #4956160:  Learning rate = 0.000676:   Batch Loss = 0.255332, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.317916512489, Accuracy = 0.976525843143\n",
            "Iter #4960256:  Learning rate = 0.000676:   Batch Loss = 0.251947, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.315594911575, Accuracy = 0.979655683041\n",
            "Iter #4964352:  Learning rate = 0.000676:   Batch Loss = 0.251468, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.316597163677, Accuracy = 0.980003476143\n",
            "Iter #4968448:  Learning rate = 0.000676:   Batch Loss = 0.249299, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.316924512386, Accuracy = 0.979655683041\n",
            "Iter #4972544:  Learning rate = 0.000676:   Batch Loss = 0.253798, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.346808493137, Accuracy = 0.972004890442\n",
            "Iter #4976640:  Learning rate = 0.000676:   Batch Loss = 0.247936, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.312687367201, Accuracy = 0.980872869492\n",
            "Iter #4980736:  Learning rate = 0.000676:   Batch Loss = 0.254031, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.32914352417, Accuracy = 0.97426533699\n",
            "Iter #4984832:  Learning rate = 0.000676:   Batch Loss = 0.257705, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.341394364834, Accuracy = 0.970439910889\n",
            "Iter #4988928:  Learning rate = 0.000676:   Batch Loss = 0.246445, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.308799117804, Accuracy = 0.977743029594\n",
            "Iter #4993024:  Learning rate = 0.000676:   Batch Loss = 0.257322, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.326535344124, Accuracy = 0.977221369743\n",
            "Iter #4997120:  Learning rate = 0.000676:   Batch Loss = 0.253760, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.317856699228, Accuracy = 0.975482523441\n",
            "Iter #5001216:  Learning rate = 0.000649:   Batch Loss = 0.245490, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.329486608505, Accuracy = 0.973917603493\n",
            "Iter #5005312:  Learning rate = 0.000649:   Batch Loss = 0.249606, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.312952846289, Accuracy = 0.979655683041\n",
            "Iter #5009408:  Learning rate = 0.000649:   Batch Loss = 0.247587, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.304998517036, Accuracy = 0.981220662594\n",
            "Iter #5013504:  Learning rate = 0.000649:   Batch Loss = 0.260363, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.305082112551, Accuracy = 0.978960156441\n",
            "Iter #5017600:  Learning rate = 0.000649:   Batch Loss = 0.250057, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.301474750042, Accuracy = 0.981046795845\n",
            "Iter #5021696:  Learning rate = 0.000649:   Batch Loss = 0.248215, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.313906490803, Accuracy = 0.980525135994\n",
            "Iter #5025792:  Learning rate = 0.000649:   Batch Loss = 0.245776, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.310325115919, Accuracy = 0.980351269245\n",
            "Iter #5029888:  Learning rate = 0.000649:   Batch Loss = 0.251147, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.308380246162, Accuracy = 0.980872869492\n",
            "Iter #5033984:  Learning rate = 0.000649:   Batch Loss = 0.258669, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.313261598349, Accuracy = 0.97704744339\n",
            "Iter #5038080:  Learning rate = 0.000649:   Batch Loss = 0.244648, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.314365029335, Accuracy = 0.978264629841\n",
            "Iter #5042176:  Learning rate = 0.000649:   Batch Loss = 0.261059, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.339743614197, Accuracy = 0.970092177391\n",
            "Iter #5046272:  Learning rate = 0.000649:   Batch Loss = 0.245313, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302640706301, Accuracy = 0.979481816292\n",
            "Iter #5050368:  Learning rate = 0.000649:   Batch Loss = 0.261141, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.312324374914, Accuracy = 0.976004183292\n",
            "Iter #5054464:  Learning rate = 0.000649:   Batch Loss = 0.277116, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.32258400321, Accuracy = 0.975830316544\n",
            "Iter #5058560:  Learning rate = 0.000649:   Batch Loss = 0.252905, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302679985762, Accuracy = 0.980351269245\n",
            "Iter #5062656:  Learning rate = 0.000649:   Batch Loss = 0.251629, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.312954545021, Accuracy = 0.980699002743\n",
            "Iter #5066752:  Learning rate = 0.000649:   Batch Loss = 0.252868, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.317330569029, Accuracy = 0.980525135994\n",
            "Iter #5070848:  Learning rate = 0.000649:   Batch Loss = 0.260954, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.315281659365, Accuracy = 0.977743029594\n",
            "Iter #5074944:  Learning rate = 0.000649:   Batch Loss = 0.261842, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.330131143332, Accuracy = 0.974439203739\n",
            "Iter #5079040:  Learning rate = 0.000649:   Batch Loss = 0.248401, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.306395471096, Accuracy = 0.981394529343\n",
            "Iter #5083136:  Learning rate = 0.000649:   Batch Loss = 0.265407, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296934366226, Accuracy = 0.982611715794\n",
            "Iter #5087232:  Learning rate = 0.000649:   Batch Loss = 0.247141, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295335382223, Accuracy = 0.982611715794\n",
            "Iter #5091328:  Learning rate = 0.000649:   Batch Loss = 0.260518, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.313216924667, Accuracy = 0.977221369743\n",
            "Iter #5095424:  Learning rate = 0.000649:   Batch Loss = 0.265140, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.309539288282, Accuracy = 0.980351269245\n",
            "Iter #5099520:  Learning rate = 0.000649:   Batch Loss = 0.262356, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.312407016754, Accuracy = 0.976525843143\n",
            "Iter #5103616:  Learning rate = 0.000623:   Batch Loss = 0.245058, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299700975418, Accuracy = 0.981916189194\n",
            "Iter #5107712:  Learning rate = 0.000623:   Batch Loss = 0.268241, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.303214490414, Accuracy = 0.983307242393\n",
            "Iter #5111808:  Learning rate = 0.000623:   Batch Loss = 0.247319, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.324006915092, Accuracy = 0.97565639019\n",
            "Iter #5115904:  Learning rate = 0.000623:   Batch Loss = 0.246698, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.312815606594, Accuracy = 0.978090763092\n",
            "Iter #5120000:  Learning rate = 0.000623:   Batch Loss = 0.252549, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.31133082509, Accuracy = 0.979481816292\n",
            "Iter #5124096:  Learning rate = 0.000623:   Batch Loss = 0.247047, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.31565964222, Accuracy = 0.976873576641\n",
            "Iter #5128192:  Learning rate = 0.000623:   Batch Loss = 0.247842, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.316469490528, Accuracy = 0.977221369743\n",
            "Iter #5132288:  Learning rate = 0.000623:   Batch Loss = 0.251353, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.307504445314, Accuracy = 0.980003476143\n",
            "Iter #5136384:  Learning rate = 0.000623:   Batch Loss = 0.253643, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.309635967016, Accuracy = 0.979307949543\n",
            "Iter #5140480:  Learning rate = 0.000623:   Batch Loss = 0.242973, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.310587257147, Accuracy = 0.979655683041\n",
            "Iter #5144576:  Learning rate = 0.000623:   Batch Loss = 0.241146, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.317957907915, Accuracy = 0.978960156441\n",
            "Iter #5148672:  Learning rate = 0.000623:   Batch Loss = 0.245435, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.307886630297, Accuracy = 0.980003476143\n",
            "Iter #5152768:  Learning rate = 0.000623:   Batch Loss = 0.252234, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.307244300842, Accuracy = 0.980699002743\n",
            "Iter #5156864:  Learning rate = 0.000623:   Batch Loss = 0.241587, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.312283724546, Accuracy = 0.979134082794\n",
            "Iter #5160960:  Learning rate = 0.000623:   Batch Loss = 0.247527, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.312517017126, Accuracy = 0.979829609394\n",
            "Iter #5165056:  Learning rate = 0.000623:   Batch Loss = 0.258643, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.310411155224, Accuracy = 0.979134082794\n",
            "Iter #5169152:  Learning rate = 0.000623:   Batch Loss = 0.267587, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.301722407341, Accuracy = 0.977916896343\n",
            "Iter #5173248:  Learning rate = 0.000623:   Batch Loss = 0.280226, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.306085914373, Accuracy = 0.978786289692\n",
            "Iter #5177344:  Learning rate = 0.000623:   Batch Loss = 0.253029, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.31357806921, Accuracy = 0.978090763092\n",
            "Iter #5181440:  Learning rate = 0.000623:   Batch Loss = 0.255361, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.310694158077, Accuracy = 0.978612422943\n",
            "Iter #5185536:  Learning rate = 0.000623:   Batch Loss = 0.242789, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.310871839523, Accuracy = 0.977916896343\n",
            "Iter #5189632:  Learning rate = 0.000623:   Batch Loss = 0.253570, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.312531322241, Accuracy = 0.979134082794\n",
            "Iter #5193728:  Learning rate = 0.000623:   Batch Loss = 0.251372, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.322815090418, Accuracy = 0.976525843143\n",
            "Iter #5197824:  Learning rate = 0.000623:   Batch Loss = 0.253888, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.345934718847, Accuracy = 0.97235262394\n",
            "Iter #5201920:  Learning rate = 0.000599:   Batch Loss = 0.249532, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.327694714069, Accuracy = 0.974786996841\n",
            "Iter #5206016:  Learning rate = 0.000599:   Batch Loss = 0.247174, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302699565887, Accuracy = 0.980872869492\n",
            "Iter #5210112:  Learning rate = 0.000599:   Batch Loss = 0.243340, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.313575357199, Accuracy = 0.978090763092\n",
            "Iter #5214208:  Learning rate = 0.000599:   Batch Loss = 0.242225, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.310088127851, Accuracy = 0.978786289692\n",
            "Iter #5218304:  Learning rate = 0.000599:   Batch Loss = 0.247382, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.321657001972, Accuracy = 0.976873576641\n",
            "Iter #5222400:  Learning rate = 0.000599:   Batch Loss = 0.246553, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.301851034164, Accuracy = 0.980699002743\n",
            "Iter #5226496:  Learning rate = 0.000599:   Batch Loss = 0.240190, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.306371182203, Accuracy = 0.980003476143\n",
            "Iter #5230592:  Learning rate = 0.000599:   Batch Loss = 0.241654, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.310041606426, Accuracy = 0.980003476143\n",
            "Iter #5234688:  Learning rate = 0.000599:   Batch Loss = 0.240391, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.307765990496, Accuracy = 0.979655683041\n",
            "Iter #5238784:  Learning rate = 0.000599:   Batch Loss = 0.238545, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.318945944309, Accuracy = 0.978090763092\n",
            "Iter #5242880:  Learning rate = 0.000599:   Batch Loss = 0.239587, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.306414633989, Accuracy = 0.981046795845\n",
            "Iter #5246976:  Learning rate = 0.000599:   Batch Loss = 0.243308, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.305792570114, Accuracy = 0.980525135994\n",
            "Iter #5251072:  Learning rate = 0.000599:   Batch Loss = 0.240855, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.31372410059, Accuracy = 0.977916896343\n",
            "Iter #5255168:  Learning rate = 0.000599:   Batch Loss = 0.244239, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.311238110065, Accuracy = 0.978960156441\n",
            "Iter #5259264:  Learning rate = 0.000599:   Batch Loss = 0.248742, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.32388818264, Accuracy = 0.974439203739\n",
            "Iter #5263360:  Learning rate = 0.000599:   Batch Loss = 0.240858, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.309079766273, Accuracy = 0.979307949543\n",
            "Iter #5267456:  Learning rate = 0.000599:   Batch Loss = 0.241703, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.308915913105, Accuracy = 0.979829609394\n",
            "Iter #5271552:  Learning rate = 0.000599:   Batch Loss = 0.245735, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.316432207823, Accuracy = 0.977569103241\n",
            "Iter #5275648:  Learning rate = 0.000599:   Batch Loss = 0.244471, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.311060875654, Accuracy = 0.979134082794\n",
            "Iter #5279744:  Learning rate = 0.000599:   Batch Loss = 0.254629, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.304694414139, Accuracy = 0.980872869492\n",
            "Iter #5283840:  Learning rate = 0.000599:   Batch Loss = 0.245619, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.303961962461, Accuracy = 0.979481816292\n",
            "Iter #5287936:  Learning rate = 0.000599:   Batch Loss = 0.245790, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.325836658478, Accuracy = 0.973395943642\n",
            "Iter #5292032:  Learning rate = 0.000599:   Batch Loss = 0.242541, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.321732163429, Accuracy = 0.97565639019\n",
            "Iter #5296128:  Learning rate = 0.000599:   Batch Loss = 0.248673, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.300556749105, Accuracy = 0.981916189194\n",
            "Iter #5300224:  Learning rate = 0.000575:   Batch Loss = 0.251219, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.310573637486, Accuracy = 0.980003476143\n",
            "Iter #5304320:  Learning rate = 0.000575:   Batch Loss = 0.243837, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.31385204196, Accuracy = 0.978438556194\n",
            "Iter #5308416:  Learning rate = 0.000575:   Batch Loss = 0.249867, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.315618157387, Accuracy = 0.976873576641\n",
            "Iter #5312512:  Learning rate = 0.000575:   Batch Loss = 0.251229, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.313674926758, Accuracy = 0.97704744339\n",
            "Iter #5316608:  Learning rate = 0.000575:   Batch Loss = 0.261820, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.328512638807, Accuracy = 0.973222076893\n",
            "Iter #5320704:  Learning rate = 0.000575:   Batch Loss = 0.237462, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.311679124832, Accuracy = 0.977916896343\n",
            "Iter #5324800:  Learning rate = 0.000575:   Batch Loss = 0.249663, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.310599148273, Accuracy = 0.978090763092\n",
            "Iter #5328896:  Learning rate = 0.000575:   Batch Loss = 0.255699, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.320260643959, Accuracy = 0.97496086359\n",
            "Iter #5332992:  Learning rate = 0.000575:   Batch Loss = 0.253007, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.298273384571, Accuracy = 0.979829609394\n",
            "Iter #5337088:  Learning rate = 0.000575:   Batch Loss = 0.248929, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.297658085823, Accuracy = 0.980699002743\n",
            "Iter #5341184:  Learning rate = 0.000575:   Batch Loss = 0.245410, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.313920617104, Accuracy = 0.977221369743\n",
            "Iter #5345280:  Learning rate = 0.000575:   Batch Loss = 0.251378, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.310947030783, Accuracy = 0.977569103241\n",
            "Iter #5349376:  Learning rate = 0.000575:   Batch Loss = 0.244690, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.32980749011, Accuracy = 0.975134730339\n",
            "Iter #5353472:  Learning rate = 0.000575:   Batch Loss = 0.239530, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.30575504899, Accuracy = 0.980351269245\n",
            "Iter #5357568:  Learning rate = 0.000575:   Batch Loss = 0.236777, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.321895033121, Accuracy = 0.976873576641\n",
            "Iter #5361664:  Learning rate = 0.000575:   Batch Loss = 0.256927, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.316892325878, Accuracy = 0.976699709892\n",
            "Iter #5365760:  Learning rate = 0.000575:   Batch Loss = 0.250038, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.311109006405, Accuracy = 0.978264629841\n",
            "Iter #5369856:  Learning rate = 0.000575:   Batch Loss = 0.249785, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.327431678772, Accuracy = 0.975482523441\n",
            "Iter #5373952:  Learning rate = 0.000575:   Batch Loss = 0.241924, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.307203054428, Accuracy = 0.977569103241\n",
            "Iter #5378048:  Learning rate = 0.000575:   Batch Loss = 0.244434, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.30359351635, Accuracy = 0.978960156441\n",
            "Iter #5382144:  Learning rate = 0.000575:   Batch Loss = 0.250585, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.305082112551, Accuracy = 0.979481816292\n",
            "Iter #5386240:  Learning rate = 0.000575:   Batch Loss = 0.243471, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.300635159016, Accuracy = 0.981394529343\n",
            "Iter #5390336:  Learning rate = 0.000575:   Batch Loss = 0.244158, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.313066124916, Accuracy = 0.977916896343\n",
            "Iter #5394432:  Learning rate = 0.000575:   Batch Loss = 0.244291, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.307534277439, Accuracy = 0.979134082794\n",
            "Iter #5398528:  Learning rate = 0.000575:   Batch Loss = 0.242610, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.301142036915, Accuracy = 0.981394529343\n",
            "Iter #5402624:  Learning rate = 0.000552:   Batch Loss = 0.235813, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.300631940365, Accuracy = 0.979829609394\n",
            "Iter #5406720:  Learning rate = 0.000552:   Batch Loss = 0.238908, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.300855249166, Accuracy = 0.981046795845\n",
            "Iter #5410816:  Learning rate = 0.000552:   Batch Loss = 0.247029, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.300016969442, Accuracy = 0.981916189194\n",
            "Iter #5414912:  Learning rate = 0.000552:   Batch Loss = 0.244581, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.320366710424, Accuracy = 0.977743029594\n",
            "Iter #5419008:  Learning rate = 0.000552:   Batch Loss = 0.251326, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.308445185423, Accuracy = 0.975830316544\n",
            "Iter #5423104:  Learning rate = 0.000552:   Batch Loss = 0.242260, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.312491089106, Accuracy = 0.97704744339\n",
            "Iter #5427200:  Learning rate = 0.000552:   Batch Loss = 0.238212, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.294610828161, Accuracy = 0.982437849045\n",
            "Iter #5431296:  Learning rate = 0.000552:   Batch Loss = 0.239943, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.298310935497, Accuracy = 0.980525135994\n",
            "Iter #5435392:  Learning rate = 0.000552:   Batch Loss = 0.245482, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.303332149982, Accuracy = 0.980177342892\n",
            "Iter #5439488:  Learning rate = 0.000552:   Batch Loss = 0.243497, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.305791288614, Accuracy = 0.978786289692\n",
            "Iter #5443584:  Learning rate = 0.000552:   Batch Loss = 0.238648, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.303995341063, Accuracy = 0.979655683041\n",
            "Iter #5447680:  Learning rate = 0.000552:   Batch Loss = 0.272283, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.297620892525, Accuracy = 0.981568396091\n",
            "Iter #5451776:  Learning rate = 0.000552:   Batch Loss = 0.251150, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.370676279068, Accuracy = 0.964701771736\n",
            "Iter #5455872:  Learning rate = 0.000552:   Batch Loss = 0.249622, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.306768059731, Accuracy = 0.976873576641\n",
            "Iter #5459968:  Learning rate = 0.000552:   Batch Loss = 0.245499, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.309250324965, Accuracy = 0.978264629841\n",
            "Iter #5464064:  Learning rate = 0.000552:   Batch Loss = 0.244303, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295379221439, Accuracy = 0.981220662594\n",
            "Iter #5468160:  Learning rate = 0.000552:   Batch Loss = 0.243559, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.297752916813, Accuracy = 0.981742322445\n",
            "Iter #5472256:  Learning rate = 0.000552:   Batch Loss = 0.248432, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.303631424904, Accuracy = 0.979655683041\n",
            "Iter #5476352:  Learning rate = 0.000552:   Batch Loss = 0.242582, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.303488880396, Accuracy = 0.980003476143\n",
            "Iter #5480448:  Learning rate = 0.000552:   Batch Loss = 0.255798, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.301843225956, Accuracy = 0.979307949543\n",
            "Iter #5484544:  Learning rate = 0.000552:   Batch Loss = 0.237942, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296172142029, Accuracy = 0.980177342892\n",
            "Iter #5488640:  Learning rate = 0.000552:   Batch Loss = 0.244181, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.298766791821, Accuracy = 0.979481816292\n",
            "Iter #5492736:  Learning rate = 0.000552:   Batch Loss = 0.236221, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.300251185894, Accuracy = 0.977916896343\n",
            "Iter #5496832:  Learning rate = 0.000552:   Batch Loss = 0.238130, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302738517523, Accuracy = 0.980699002743\n",
            "Iter #5500928:  Learning rate = 0.000530:   Batch Loss = 0.235065, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.308305233717, Accuracy = 0.977395236492\n",
            "Iter #5505024:  Learning rate = 0.000530:   Batch Loss = 0.242238, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.305464327335, Accuracy = 0.976699709892\n",
            "Iter #5509120:  Learning rate = 0.000530:   Batch Loss = 0.241611, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.304749578238, Accuracy = 0.977916896343\n",
            "Iter #5513216:  Learning rate = 0.000530:   Batch Loss = 0.245464, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.311203360558, Accuracy = 0.975830316544\n",
            "Iter #5517312:  Learning rate = 0.000530:   Batch Loss = 0.238177, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302120149136, Accuracy = 0.979134082794\n",
            "Iter #5521408:  Learning rate = 0.000530:   Batch Loss = 0.236489, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.312083154917, Accuracy = 0.977395236492\n",
            "Iter #5525504:  Learning rate = 0.000530:   Batch Loss = 0.233536, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.305463671684, Accuracy = 0.980003476143\n",
            "Iter #5529600:  Learning rate = 0.000530:   Batch Loss = 0.233459, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.300859242678, Accuracy = 0.979829609394\n",
            "Iter #5533696:  Learning rate = 0.000530:   Batch Loss = 0.247383, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.303143620491, Accuracy = 0.976178050041\n",
            "Iter #5537792:  Learning rate = 0.000530:   Batch Loss = 0.253920, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.308130323887, Accuracy = 0.978264629841\n",
            "Iter #5541888:  Learning rate = 0.000530:   Batch Loss = 0.236573, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.368082433939, Accuracy = 0.962615191936\n",
            "Iter #5545984:  Learning rate = 0.000530:   Batch Loss = 0.274104, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.308685302734, Accuracy = 0.977916896343\n",
            "Iter #5550080:  Learning rate = 0.000530:   Batch Loss = 0.247212, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.308884114027, Accuracy = 0.976699709892\n",
            "Iter #5554176:  Learning rate = 0.000530:   Batch Loss = 0.255028, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302196115255, Accuracy = 0.978612422943\n",
            "Iter #5558272:  Learning rate = 0.000530:   Batch Loss = 0.242587, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299560815096, Accuracy = 0.979307949543\n",
            "Iter #5562368:  Learning rate = 0.000530:   Batch Loss = 0.241338, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291617929935, Accuracy = 0.979829609394\n",
            "Iter #5566464:  Learning rate = 0.000530:   Batch Loss = 0.236420, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.286659479141, Accuracy = 0.982090055943\n",
            "Iter #5570560:  Learning rate = 0.000530:   Batch Loss = 0.237314, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.292971938848, Accuracy = 0.981220662594\n",
            "Iter #5574656:  Learning rate = 0.000530:   Batch Loss = 0.241845, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.308276295662, Accuracy = 0.976699709892\n",
            "Iter #5578752:  Learning rate = 0.000530:   Batch Loss = 0.239754, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.308164149523, Accuracy = 0.976873576641\n",
            "Iter #5582848:  Learning rate = 0.000530:   Batch Loss = 0.252934, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.312005400658, Accuracy = 0.97704744339\n",
            "Iter #5586944:  Learning rate = 0.000530:   Batch Loss = 0.250131, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.328010678291, Accuracy = 0.974439203739\n",
            "Iter #5591040:  Learning rate = 0.000530:   Batch Loss = 0.244168, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.300263822079, Accuracy = 0.978438556194\n",
            "Iter #5595136:  Learning rate = 0.000530:   Batch Loss = 0.248362, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.303288519382, Accuracy = 0.977221369743\n",
            "Iter #5599232:  Learning rate = 0.000530:   Batch Loss = 0.252932, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.301815778017, Accuracy = 0.978264629841\n",
            "Iter #5603328:  Learning rate = 0.000508:   Batch Loss = 0.260891, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.317884206772, Accuracy = 0.972700417042\n",
            "Iter #5607424:  Learning rate = 0.000508:   Batch Loss = 0.264801, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.297845005989, Accuracy = 0.978090763092\n",
            "Iter #5611520:  Learning rate = 0.000508:   Batch Loss = 0.266088, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.316996753216, Accuracy = 0.976004183292\n",
            "Iter #5615616:  Learning rate = 0.000508:   Batch Loss = 0.242501, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.3170568645, Accuracy = 0.972874283791\n",
            "Iter #5619712:  Learning rate = 0.000508:   Batch Loss = 0.239309, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.304608225822, Accuracy = 0.976525843143\n",
            "Iter #5623808:  Learning rate = 0.000508:   Batch Loss = 0.243146, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.289875596762, Accuracy = 0.979829609394\n",
            "Iter #5627904:  Learning rate = 0.000508:   Batch Loss = 0.241106, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291234791279, Accuracy = 0.979134082794\n",
            "Iter #5632000:  Learning rate = 0.000508:   Batch Loss = 0.241668, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.301302492619, Accuracy = 0.978264629841\n",
            "Iter #5636096:  Learning rate = 0.000508:   Batch Loss = 0.248191, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.301132082939, Accuracy = 0.978612422943\n",
            "Iter #5640192:  Learning rate = 0.000508:   Batch Loss = 0.234348, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.316722661257, Accuracy = 0.974613130093\n",
            "Iter #5644288:  Learning rate = 0.000508:   Batch Loss = 0.239946, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.309757918119, Accuracy = 0.97635191679\n",
            "Iter #5648384:  Learning rate = 0.000508:   Batch Loss = 0.247808, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.297668516636, Accuracy = 0.980177342892\n",
            "Iter #5652480:  Learning rate = 0.000508:   Batch Loss = 0.234384, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.304889559746, Accuracy = 0.979481816292\n",
            "Iter #5656576:  Learning rate = 0.000508:   Batch Loss = 0.238152, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.337403446436, Accuracy = 0.967831671238\n",
            "Iter #5660672:  Learning rate = 0.000508:   Batch Loss = 0.246407, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.310980975628, Accuracy = 0.975308656693\n",
            "Iter #5664768:  Learning rate = 0.000508:   Batch Loss = 0.240564, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.300136983395, Accuracy = 0.978438556194\n",
            "Iter #5668864:  Learning rate = 0.000508:   Batch Loss = 0.257828, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.316257953644, Accuracy = 0.975134730339\n",
            "Iter #5672960:  Learning rate = 0.000508:   Batch Loss = 0.239156, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.323791235685, Accuracy = 0.973917603493\n",
            "Iter #5677056:  Learning rate = 0.000508:   Batch Loss = 0.237020, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.309555232525, Accuracy = 0.97635191679\n",
            "Iter #5681152:  Learning rate = 0.000508:   Batch Loss = 0.237424, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.309512555599, Accuracy = 0.97704744339\n",
            "Iter #5685248:  Learning rate = 0.000508:   Batch Loss = 0.237778, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.318080872297, Accuracy = 0.977221369743\n",
            "Iter #5689344:  Learning rate = 0.000508:   Batch Loss = 0.240186, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.305089950562, Accuracy = 0.980003476143\n",
            "Iter #5693440:  Learning rate = 0.000508:   Batch Loss = 0.235379, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.30209338665, Accuracy = 0.978960156441\n",
            "Iter #5697536:  Learning rate = 0.000508:   Batch Loss = 0.236234, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.31365275383, Accuracy = 0.976699709892\n",
            "Iter #5701632:  Learning rate = 0.000488:   Batch Loss = 0.233044, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.292268246412, Accuracy = 0.981394529343\n",
            "Iter #5705728:  Learning rate = 0.000488:   Batch Loss = 0.245717, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.298469662666, Accuracy = 0.980003476143\n",
            "Iter #5709824:  Learning rate = 0.000488:   Batch Loss = 0.241477, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299149513245, Accuracy = 0.981742322445\n",
            "Iter #5713920:  Learning rate = 0.000488:   Batch Loss = 0.240387, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.305173277855, Accuracy = 0.979134082794\n",
            "Iter #5718016:  Learning rate = 0.000488:   Batch Loss = 0.234568, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295392423868, Accuracy = 0.979655683041\n",
            "Iter #5722112:  Learning rate = 0.000488:   Batch Loss = 0.235292, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.305484414101, Accuracy = 0.977395236492\n",
            "Iter #5726208:  Learning rate = 0.000488:   Batch Loss = 0.235663, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.292175889015, Accuracy = 0.980177342892\n",
            "Iter #5730304:  Learning rate = 0.000488:   Batch Loss = 0.246066, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.297272980213, Accuracy = 0.979655683041\n",
            "Iter #5734400:  Learning rate = 0.000488:   Batch Loss = 0.243469, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.308530420065, Accuracy = 0.976525843143\n",
            "Iter #5738496:  Learning rate = 0.000488:   Batch Loss = 0.239271, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.309332549572, Accuracy = 0.976004183292\n",
            "Iter #5742592:  Learning rate = 0.000488:   Batch Loss = 0.247755, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.320777177811, Accuracy = 0.97565639019\n",
            "Iter #5746688:  Learning rate = 0.000488:   Batch Loss = 0.243451, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.292531996965, Accuracy = 0.979829609394\n",
            "Iter #5750784:  Learning rate = 0.000488:   Batch Loss = 0.249055, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.307584792376, Accuracy = 0.978612422943\n",
            "Iter #5754880:  Learning rate = 0.000488:   Batch Loss = 0.244809, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.301681160927, Accuracy = 0.978786289692\n",
            "Iter #5758976:  Learning rate = 0.000488:   Batch Loss = 0.238181, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.317013442516, Accuracy = 0.97356981039\n",
            "Iter #5763072:  Learning rate = 0.000488:   Batch Loss = 0.254505, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.293621391058, Accuracy = 0.978612422943\n",
            "Iter #5767168:  Learning rate = 0.000488:   Batch Loss = 0.245367, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.29345446825, Accuracy = 0.981742322445\n",
            "Iter #5771264:  Learning rate = 0.000488:   Batch Loss = 0.243815, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299631178379, Accuracy = 0.979481816292\n",
            "Iter #5775360:  Learning rate = 0.000488:   Batch Loss = 0.240307, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299891084433, Accuracy = 0.978960156441\n",
            "Iter #5779456:  Learning rate = 0.000488:   Batch Loss = 0.251915, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299968570471, Accuracy = 0.977916896343\n",
            "Iter #5783552:  Learning rate = 0.000488:   Batch Loss = 0.236042, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.303340137005, Accuracy = 0.978090763092\n",
            "Iter #5787648:  Learning rate = 0.000488:   Batch Loss = 0.236889, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.303260147572, Accuracy = 0.979307949543\n",
            "Iter #5791744:  Learning rate = 0.000488:   Batch Loss = 0.238643, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302317947149, Accuracy = 0.977743029594\n",
            "Iter #5795840:  Learning rate = 0.000488:   Batch Loss = 0.237947, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296626478434, Accuracy = 0.980872869492\n",
            "Iter #5799936:  Learning rate = 0.000488:   Batch Loss = 0.233595, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.297559022903, Accuracy = 0.980525135994\n",
            "Iter #5804032:  Learning rate = 0.000468:   Batch Loss = 0.235696, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.297880798578, Accuracy = 0.980699002743\n",
            "Iter #5808128:  Learning rate = 0.000468:   Batch Loss = 0.239465, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.294590175152, Accuracy = 0.980872869492\n",
            "Iter #5812224:  Learning rate = 0.000468:   Batch Loss = 0.234287, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.306939065456, Accuracy = 0.977221369743\n",
            "Iter #5816320:  Learning rate = 0.000468:   Batch Loss = 0.239464, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.300677955151, Accuracy = 0.978612422943\n",
            "Iter #5820416:  Learning rate = 0.000468:   Batch Loss = 0.238014, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296158522367, Accuracy = 0.980872869492\n",
            "Iter #5824512:  Learning rate = 0.000468:   Batch Loss = 0.233519, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.311514079571, Accuracy = 0.977916896343\n",
            "Iter #5828608:  Learning rate = 0.000468:   Batch Loss = 0.231221, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.300760298967, Accuracy = 0.978612422943\n",
            "Iter #5832704:  Learning rate = 0.000468:   Batch Loss = 0.234729, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.292806714773, Accuracy = 0.979829609394\n",
            "Iter #5836800:  Learning rate = 0.000468:   Batch Loss = 0.239046, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.294231176376, Accuracy = 0.980351269245\n",
            "Iter #5840896:  Learning rate = 0.000468:   Batch Loss = 0.235257, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.293515235186, Accuracy = 0.981568396091\n",
            "Iter #5844992:  Learning rate = 0.000468:   Batch Loss = 0.230765, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.297271728516, Accuracy = 0.980003476143\n",
            "Iter #5849088:  Learning rate = 0.000468:   Batch Loss = 0.238177, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.292431354523, Accuracy = 0.981742322445\n",
            "Iter #5853184:  Learning rate = 0.000468:   Batch Loss = 0.236826, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296917170286, Accuracy = 0.980525135994\n",
            "Iter #5857280:  Learning rate = 0.000468:   Batch Loss = 0.232728, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299575686455, Accuracy = 0.978960156441\n",
            "Iter #5861376:  Learning rate = 0.000468:   Batch Loss = 0.255671, Accuracy = 0.986328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296753466129, Accuracy = 0.979655683041\n",
            "Iter #5865472:  Learning rate = 0.000468:   Batch Loss = 0.236442, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.300363242626, Accuracy = 0.979655683041\n",
            "Iter #5869568:  Learning rate = 0.000468:   Batch Loss = 0.231896, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.292314052582, Accuracy = 0.980699002743\n",
            "Iter #5873664:  Learning rate = 0.000468:   Batch Loss = 0.245877, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.298665106297, Accuracy = 0.980525135994\n",
            "Iter #5877760:  Learning rate = 0.000468:   Batch Loss = 0.309995, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.363362014294, Accuracy = 0.960180819035\n",
            "Iter #5881856:  Learning rate = 0.000468:   Batch Loss = 0.291632, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.338912636042, Accuracy = 0.965571224689\n",
            "Iter #5885952:  Learning rate = 0.000468:   Batch Loss = 0.310333, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.354311943054, Accuracy = 0.960180819035\n",
            "Iter #5890048:  Learning rate = 0.000468:   Batch Loss = 0.244421, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.315043091774, Accuracy = 0.973917603493\n",
            "Iter #5894144:  Learning rate = 0.000468:   Batch Loss = 0.280926, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.365934610367, Accuracy = 0.959485292435\n",
            "Iter #5898240:  Learning rate = 0.000468:   Batch Loss = 0.254949, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.30962562561, Accuracy = 0.972004890442\n",
            "Iter #5902336:  Learning rate = 0.000450:   Batch Loss = 0.252694, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.323154330254, Accuracy = 0.969744384289\n",
            "Iter #5906432:  Learning rate = 0.000450:   Batch Loss = 0.239006, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.292502462864, Accuracy = 0.979655683041\n",
            "Iter #5910528:  Learning rate = 0.000450:   Batch Loss = 0.247416, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.2986766994, Accuracy = 0.978438556194\n",
            "Iter #5914624:  Learning rate = 0.000450:   Batch Loss = 0.239182, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.31344589591, Accuracy = 0.976004183292\n",
            "Iter #5918720:  Learning rate = 0.000450:   Batch Loss = 0.241921, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.287436455488, Accuracy = 0.979829609394\n",
            "Iter #5922816:  Learning rate = 0.000450:   Batch Loss = 0.251701, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.293186128139, Accuracy = 0.97635191679\n",
            "Iter #5926912:  Learning rate = 0.000450:   Batch Loss = 0.236620, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.286023885012, Accuracy = 0.980351269245\n",
            "Iter #5931008:  Learning rate = 0.000450:   Batch Loss = 0.242373, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.284198760986, Accuracy = 0.981046795845\n",
            "Iter #5935104:  Learning rate = 0.000450:   Batch Loss = 0.234878, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.286278069019, Accuracy = 0.980699002743\n",
            "Iter #5939200:  Learning rate = 0.000450:   Batch Loss = 0.231457, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.292622059584, Accuracy = 0.978438556194\n",
            "Iter #5943296:  Learning rate = 0.000450:   Batch Loss = 0.241382, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.298176050186, Accuracy = 0.976873576641\n",
            "Iter #5947392:  Learning rate = 0.000450:   Batch Loss = 0.242932, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.293006718159, Accuracy = 0.977569103241\n",
            "Iter #5951488:  Learning rate = 0.000450:   Batch Loss = 0.237464, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.288165509701, Accuracy = 0.981220662594\n",
            "Iter #5955584:  Learning rate = 0.000450:   Batch Loss = 0.238604, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.286638855934, Accuracy = 0.982437849045\n",
            "Iter #5959680:  Learning rate = 0.000450:   Batch Loss = 0.234138, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.286874979734, Accuracy = 0.982090055943\n",
            "Iter #5963776:  Learning rate = 0.000450:   Batch Loss = 0.232038, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291227668524, Accuracy = 0.979307949543\n",
            "Iter #5967872:  Learning rate = 0.000450:   Batch Loss = 0.250594, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296028614044, Accuracy = 0.978264629841\n",
            "Iter #5971968:  Learning rate = 0.000450:   Batch Loss = 0.232503, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.290066242218, Accuracy = 0.980351269245\n",
            "Iter #5976064:  Learning rate = 0.000450:   Batch Loss = 0.230197, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.297282069921, Accuracy = 0.980525135994\n",
            "Iter #5980160:  Learning rate = 0.000450:   Batch Loss = 0.239965, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.292658388615, Accuracy = 0.981046795845\n",
            "Iter #5984256:  Learning rate = 0.000450:   Batch Loss = 0.231260, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.287583261728, Accuracy = 0.980872869492\n",
            "Iter #5988352:  Learning rate = 0.000450:   Batch Loss = 0.230102, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.284780293703, Accuracy = 0.981220662594\n",
            "Iter #5992448:  Learning rate = 0.000450:   Batch Loss = 0.244726, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.30036970973, Accuracy = 0.978960156441\n",
            "Iter #5996544:  Learning rate = 0.000450:   Batch Loss = 0.237144, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.288860201836, Accuracy = 0.980351269245\n",
            "Iter #6000640:  Learning rate = 0.000432:   Batch Loss = 0.230501, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.285858780146, Accuracy = 0.982437849045\n",
            "Iter #6004736:  Learning rate = 0.000432:   Batch Loss = 0.234526, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.284161061049, Accuracy = 0.981916189194\n",
            "Iter #6008832:  Learning rate = 0.000432:   Batch Loss = 0.232572, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296402394772, Accuracy = 0.977743029594\n",
            "Iter #6012928:  Learning rate = 0.000432:   Batch Loss = 0.232448, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.310837328434, Accuracy = 0.977221369743\n",
            "Iter #6017024:  Learning rate = 0.000432:   Batch Loss = 0.233715, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.28969463706, Accuracy = 0.982785582542\n",
            "Iter #6021120:  Learning rate = 0.000432:   Batch Loss = 0.235514, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.293843984604, Accuracy = 0.981568396091\n",
            "Iter #6025216:  Learning rate = 0.000432:   Batch Loss = 0.232966, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.298111945391, Accuracy = 0.980525135994\n",
            "Iter #6029312:  Learning rate = 0.000432:   Batch Loss = 0.231016, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.290500223637, Accuracy = 0.980525135994\n",
            "Iter #6033408:  Learning rate = 0.000432:   Batch Loss = 0.239313, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295767366886, Accuracy = 0.980351269245\n",
            "Iter #6037504:  Learning rate = 0.000432:   Batch Loss = 0.234143, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295223116875, Accuracy = 0.979307949543\n",
            "Iter #6041600:  Learning rate = 0.000432:   Batch Loss = 0.231780, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.287033498287, Accuracy = 0.982437849045\n",
            "Iter #6045696:  Learning rate = 0.000432:   Batch Loss = 0.230814, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.287108153105, Accuracy = 0.981394529343\n",
            "Iter #6049792:  Learning rate = 0.000432:   Batch Loss = 0.239271, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291609674692, Accuracy = 0.982437849045\n",
            "Iter #6053888:  Learning rate = 0.000432:   Batch Loss = 0.229466, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296475708485, Accuracy = 0.981220662594\n",
            "Iter #6057984:  Learning rate = 0.000432:   Batch Loss = 0.232748, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.294844418764, Accuracy = 0.980351269245\n",
            "Iter #6062080:  Learning rate = 0.000432:   Batch Loss = 0.234348, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.3069858253, Accuracy = 0.977395236492\n",
            "Iter #6066176:  Learning rate = 0.000432:   Batch Loss = 0.232626, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.290777117014, Accuracy = 0.981394529343\n",
            "Iter #6070272:  Learning rate = 0.000432:   Batch Loss = 0.227513, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.290699183941, Accuracy = 0.980872869492\n",
            "Iter #6074368:  Learning rate = 0.000432:   Batch Loss = 0.232605, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299302071333, Accuracy = 0.978786289692\n",
            "Iter #6078464:  Learning rate = 0.000432:   Batch Loss = 0.232797, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.292941331863, Accuracy = 0.980351269245\n",
            "Iter #6082560:  Learning rate = 0.000432:   Batch Loss = 0.236739, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.287158578634, Accuracy = 0.981046795845\n",
            "Iter #6086656:  Learning rate = 0.000432:   Batch Loss = 0.231119, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291062057018, Accuracy = 0.982437849045\n",
            "Iter #6090752:  Learning rate = 0.000432:   Batch Loss = 0.236792, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.305767983198, Accuracy = 0.977221369743\n",
            "Iter #6094848:  Learning rate = 0.000432:   Batch Loss = 0.228780, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295466810465, Accuracy = 0.979655683041\n",
            "Iter #6098944:  Learning rate = 0.000432:   Batch Loss = 0.230439, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.293639957905, Accuracy = 0.981046795845\n",
            "Iter #6103040:  Learning rate = 0.000414:   Batch Loss = 0.233867, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296230733395, Accuracy = 0.980351269245\n",
            "Iter #6107136:  Learning rate = 0.000414:   Batch Loss = 0.232481, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295902818441, Accuracy = 0.980003476143\n",
            "Iter #6111232:  Learning rate = 0.000414:   Batch Loss = 0.234094, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.293545812368, Accuracy = 0.980525135994\n",
            "Iter #6115328:  Learning rate = 0.000414:   Batch Loss = 0.239644, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.292734235525, Accuracy = 0.980003476143\n",
            "Iter #6119424:  Learning rate = 0.000414:   Batch Loss = 0.239293, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296141624451, Accuracy = 0.979134082794\n",
            "Iter #6123520:  Learning rate = 0.000414:   Batch Loss = 0.231429, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302934348583, Accuracy = 0.977916896343\n",
            "Iter #6127616:  Learning rate = 0.000414:   Batch Loss = 0.230129, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.300836741924, Accuracy = 0.979829609394\n",
            "Iter #6131712:  Learning rate = 0.000414:   Batch Loss = 0.231613, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.294643521309, Accuracy = 0.980699002743\n",
            "Iter #6135808:  Learning rate = 0.000414:   Batch Loss = 0.228549, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.29156678915, Accuracy = 0.981742322445\n",
            "Iter #6139904:  Learning rate = 0.000414:   Batch Loss = 0.235019, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.294535160065, Accuracy = 0.981046795845\n",
            "Iter #6144000:  Learning rate = 0.000414:   Batch Loss = 0.231610, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296331703663, Accuracy = 0.980525135994\n",
            "Iter #6148096:  Learning rate = 0.000414:   Batch Loss = 0.231878, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.301903158426, Accuracy = 0.977916896343\n",
            "Iter #6152192:  Learning rate = 0.000414:   Batch Loss = 0.247268, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302787363529, Accuracy = 0.978264629841\n",
            "Iter #6156288:  Learning rate = 0.000414:   Batch Loss = 0.227560, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295439213514, Accuracy = 0.979134082794\n",
            "Iter #6160384:  Learning rate = 0.000414:   Batch Loss = 0.236496, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295707046986, Accuracy = 0.980872869492\n",
            "Iter #6164480:  Learning rate = 0.000414:   Batch Loss = 0.226830, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.305188417435, Accuracy = 0.977916896343\n",
            "Iter #6168576:  Learning rate = 0.000414:   Batch Loss = 0.229928, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.294688969851, Accuracy = 0.980525135994\n",
            "Iter #6172672:  Learning rate = 0.000414:   Batch Loss = 0.233967, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.298549830914, Accuracy = 0.979655683041\n",
            "Iter #6176768:  Learning rate = 0.000414:   Batch Loss = 0.230010, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.30046710372, Accuracy = 0.980872869492\n",
            "Iter #6180864:  Learning rate = 0.000414:   Batch Loss = 0.230008, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.29884827137, Accuracy = 0.979134082794\n",
            "Iter #6184960:  Learning rate = 0.000414:   Batch Loss = 0.232082, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295352071524, Accuracy = 0.979829609394\n",
            "Iter #6189056:  Learning rate = 0.000414:   Batch Loss = 0.230998, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.305349200964, Accuracy = 0.976525843143\n",
            "Iter #6193152:  Learning rate = 0.000414:   Batch Loss = 0.238880, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.30829384923, Accuracy = 0.976525843143\n",
            "Iter #6197248:  Learning rate = 0.000414:   Batch Loss = 0.239079, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.29582887888, Accuracy = 0.979655683041\n",
            "Iter #6201344:  Learning rate = 0.000398:   Batch Loss = 0.243322, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.345617681742, Accuracy = 0.967136144638\n",
            "Iter #6205440:  Learning rate = 0.000398:   Batch Loss = 0.230013, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296224892139, Accuracy = 0.979307949543\n",
            "Iter #6209536:  Learning rate = 0.000398:   Batch Loss = 0.237663, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291956305504, Accuracy = 0.980872869492\n",
            "Iter #6213632:  Learning rate = 0.000398:   Batch Loss = 0.229285, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.29300558567, Accuracy = 0.980003476143\n",
            "Iter #6217728:  Learning rate = 0.000398:   Batch Loss = 0.239758, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.289394617081, Accuracy = 0.981220662594\n",
            "Iter #6221824:  Learning rate = 0.000398:   Batch Loss = 0.232861, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.29088935256, Accuracy = 0.980177342892\n",
            "Iter #6225920:  Learning rate = 0.000398:   Batch Loss = 0.228263, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291067183018, Accuracy = 0.982611715794\n",
            "Iter #6230016:  Learning rate = 0.000398:   Batch Loss = 0.248683, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.285270273685, Accuracy = 0.982263982296\n",
            "Iter #6234112:  Learning rate = 0.000398:   Batch Loss = 0.229489, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295540720224, Accuracy = 0.978960156441\n",
            "Iter #6238208:  Learning rate = 0.000398:   Batch Loss = 0.232677, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302754670382, Accuracy = 0.978960156441\n",
            "Iter #6242304:  Learning rate = 0.000398:   Batch Loss = 0.240207, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.306876569986, Accuracy = 0.97704744339\n",
            "Iter #6246400:  Learning rate = 0.000398:   Batch Loss = 0.233795, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.290733486414, Accuracy = 0.980177342892\n",
            "Iter #6250496:  Learning rate = 0.000398:   Batch Loss = 0.239985, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.29805919528, Accuracy = 0.976699709892\n",
            "Iter #6254592:  Learning rate = 0.000398:   Batch Loss = 0.236893, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.288616687059, Accuracy = 0.980699002743\n",
            "Iter #6258688:  Learning rate = 0.000398:   Batch Loss = 0.231823, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291417658329, Accuracy = 0.980177342892\n",
            "Iter #6262784:  Learning rate = 0.000398:   Batch Loss = 0.227825, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291514337063, Accuracy = 0.980351269245\n",
            "Iter #6266880:  Learning rate = 0.000398:   Batch Loss = 0.226565, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.285530865192, Accuracy = 0.982959508896\n",
            "Iter #6270976:  Learning rate = 0.000398:   Batch Loss = 0.231196, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295724660158, Accuracy = 0.978786289692\n",
            "Iter #6275072:  Learning rate = 0.000398:   Batch Loss = 0.226387, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.289630949497, Accuracy = 0.980699002743\n",
            "Iter #6279168:  Learning rate = 0.000398:   Batch Loss = 0.233306, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.292456030846, Accuracy = 0.980699002743\n",
            "Iter #6283264:  Learning rate = 0.000398:   Batch Loss = 0.232887, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296553194523, Accuracy = 0.980177342892\n",
            "Iter #6287360:  Learning rate = 0.000398:   Batch Loss = 0.225455, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299773037434, Accuracy = 0.977916896343\n",
            "Iter #6291456:  Learning rate = 0.000398:   Batch Loss = 0.230127, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.290731430054, Accuracy = 0.980351269245\n",
            "Iter #6295552:  Learning rate = 0.000398:   Batch Loss = 0.225960, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.288802206516, Accuracy = 0.980351269245\n",
            "Iter #6299648:  Learning rate = 0.000398:   Batch Loss = 0.228011, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.297377824783, Accuracy = 0.978786289692\n",
            "Iter #6303744:  Learning rate = 0.000382:   Batch Loss = 0.225099, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296057879925, Accuracy = 0.978612422943\n",
            "Iter #6307840:  Learning rate = 0.000382:   Batch Loss = 0.230982, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.290671914816, Accuracy = 0.978786289692\n",
            "Iter #6311936:  Learning rate = 0.000382:   Batch Loss = 0.234965, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.294047772884, Accuracy = 0.980177342892\n",
            "Iter #6316032:  Learning rate = 0.000382:   Batch Loss = 0.233179, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299717754126, Accuracy = 0.978438556194\n",
            "Iter #6320128:  Learning rate = 0.000382:   Batch Loss = 0.225871, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.293276101351, Accuracy = 0.979829609394\n",
            "Iter #6324224:  Learning rate = 0.000382:   Batch Loss = 0.226011, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295497953892, Accuracy = 0.979829609394\n",
            "Iter #6328320:  Learning rate = 0.000382:   Batch Loss = 0.237978, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295164912939, Accuracy = 0.978786289692\n",
            "Iter #6332416:  Learning rate = 0.000382:   Batch Loss = 0.230310, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.297394931316, Accuracy = 0.978438556194\n",
            "Iter #6336512:  Learning rate = 0.000382:   Batch Loss = 0.232349, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291463792324, Accuracy = 0.981916189194\n",
            "Iter #6340608:  Learning rate = 0.000382:   Batch Loss = 0.227210, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295720040798, Accuracy = 0.980177342892\n",
            "Iter #6344704:  Learning rate = 0.000382:   Batch Loss = 0.231366, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.294589757919, Accuracy = 0.979134082794\n",
            "Iter #6348800:  Learning rate = 0.000382:   Batch Loss = 0.225562, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.289779186249, Accuracy = 0.981220662594\n",
            "Iter #6352896:  Learning rate = 0.000382:   Batch Loss = 0.228592, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.292159408331, Accuracy = 0.981220662594\n",
            "Iter #6356992:  Learning rate = 0.000382:   Batch Loss = 0.235163, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.301760822535, Accuracy = 0.977916896343\n",
            "Iter #6361088:  Learning rate = 0.000382:   Batch Loss = 0.243038, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.297966361046, Accuracy = 0.979307949543\n",
            "Iter #6365184:  Learning rate = 0.000382:   Batch Loss = 0.228033, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291740357876, Accuracy = 0.979829609394\n",
            "Iter #6369280:  Learning rate = 0.000382:   Batch Loss = 0.232384, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.289316952229, Accuracy = 0.980003476143\n",
            "Iter #6373376:  Learning rate = 0.000382:   Batch Loss = 0.226806, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.289412081242, Accuracy = 0.981220662594\n",
            "Iter #6377472:  Learning rate = 0.000382:   Batch Loss = 0.228916, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299678981304, Accuracy = 0.978612422943\n",
            "Iter #6381568:  Learning rate = 0.000382:   Batch Loss = 0.228833, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302126526833, Accuracy = 0.977916896343\n",
            "Iter #6385664:  Learning rate = 0.000382:   Batch Loss = 0.230415, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.293709039688, Accuracy = 0.978960156441\n",
            "Iter #6389760:  Learning rate = 0.000382:   Batch Loss = 0.225606, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.292014092207, Accuracy = 0.978960156441\n",
            "Iter #6393856:  Learning rate = 0.000382:   Batch Loss = 0.244397, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.287595689297, Accuracy = 0.981046795845\n",
            "Iter #6397952:  Learning rate = 0.000382:   Batch Loss = 0.232342, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.297706216574, Accuracy = 0.977743029594\n",
            "Iter #6402048:  Learning rate = 0.000367:   Batch Loss = 0.232110, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.301616191864, Accuracy = 0.978438556194\n",
            "Iter #6406144:  Learning rate = 0.000367:   Batch Loss = 0.232013, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296066850424, Accuracy = 0.980003476143\n",
            "Iter #6410240:  Learning rate = 0.000367:   Batch Loss = 0.234598, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.294391423464, Accuracy = 0.979307949543\n",
            "Iter #6414336:  Learning rate = 0.000367:   Batch Loss = 0.233610, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.286968439817, Accuracy = 0.979655683041\n",
            "Iter #6418432:  Learning rate = 0.000367:   Batch Loss = 0.225283, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.289983838797, Accuracy = 0.977569103241\n",
            "Iter #6422528:  Learning rate = 0.000367:   Batch Loss = 0.230514, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291529685259, Accuracy = 0.979307949543\n",
            "Iter #6426624:  Learning rate = 0.000367:   Batch Loss = 0.229923, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.2896874547, Accuracy = 0.978612422943\n",
            "Iter #6430720:  Learning rate = 0.000367:   Batch Loss = 0.229164, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.301891744137, Accuracy = 0.97635191679\n",
            "Iter #6434816:  Learning rate = 0.000367:   Batch Loss = 0.232025, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.310588002205, Accuracy = 0.974613130093\n",
            "Iter #6438912:  Learning rate = 0.000367:   Batch Loss = 0.233446, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.30050611496, Accuracy = 0.976873576641\n",
            "Iter #6443008:  Learning rate = 0.000367:   Batch Loss = 0.230964, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.313856452703, Accuracy = 0.97426533699\n",
            "Iter #6447104:  Learning rate = 0.000367:   Batch Loss = 0.227434, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.300793945789, Accuracy = 0.976699709892\n",
            "Iter #6451200:  Learning rate = 0.000367:   Batch Loss = 0.228877, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295277893543, Accuracy = 0.97704744339\n",
            "Iter #6455296:  Learning rate = 0.000367:   Batch Loss = 0.227701, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.309512555599, Accuracy = 0.976004183292\n",
            "Iter #6459392:  Learning rate = 0.000367:   Batch Loss = 0.229571, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.305303186178, Accuracy = 0.976699709892\n",
            "Iter #6463488:  Learning rate = 0.000367:   Batch Loss = 0.229105, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299639493227, Accuracy = 0.977221369743\n",
            "Iter #6467584:  Learning rate = 0.000367:   Batch Loss = 0.227627, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295375555754, Accuracy = 0.978438556194\n",
            "Iter #6471680:  Learning rate = 0.000367:   Batch Loss = 0.228868, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296792596579, Accuracy = 0.977916896343\n",
            "Iter #6475776:  Learning rate = 0.000367:   Batch Loss = 0.234182, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.29321295023, Accuracy = 0.978612422943\n",
            "Iter #6479872:  Learning rate = 0.000367:   Batch Loss = 0.230249, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299929797649, Accuracy = 0.976873576641\n",
            "Iter #6483968:  Learning rate = 0.000367:   Batch Loss = 0.229415, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.293725848198, Accuracy = 0.979655683041\n",
            "Iter #6488064:  Learning rate = 0.000367:   Batch Loss = 0.237037, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.29515221715, Accuracy = 0.977569103241\n",
            "Iter #6492160:  Learning rate = 0.000367:   Batch Loss = 0.225861, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291212320328, Accuracy = 0.977569103241\n",
            "Iter #6496256:  Learning rate = 0.000367:   Batch Loss = 0.227407, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291795253754, Accuracy = 0.980351269245\n",
            "Iter #6500352:  Learning rate = 0.000352:   Batch Loss = 0.224605, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299975395203, Accuracy = 0.979134082794\n",
            "Iter #6504448:  Learning rate = 0.000352:   Batch Loss = 0.244735, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295492231846, Accuracy = 0.977221369743\n",
            "Iter #6508544:  Learning rate = 0.000352:   Batch Loss = 0.227608, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291793107986, Accuracy = 0.980872869492\n",
            "Iter #6512640:  Learning rate = 0.000352:   Batch Loss = 0.237610, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.30305609107, Accuracy = 0.977221369743\n",
            "Iter #6516736:  Learning rate = 0.000352:   Batch Loss = 0.229434, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302344501019, Accuracy = 0.977916896343\n",
            "Iter #6520832:  Learning rate = 0.000352:   Batch Loss = 0.232647, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291064739227, Accuracy = 0.977916896343\n",
            "Iter #6524928:  Learning rate = 0.000352:   Batch Loss = 0.234834, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295652031898, Accuracy = 0.978612422943\n",
            "Iter #6529024:  Learning rate = 0.000352:   Batch Loss = 0.229426, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296367943287, Accuracy = 0.978438556194\n",
            "Iter #6533120:  Learning rate = 0.000352:   Batch Loss = 0.237681, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.290846943855, Accuracy = 0.978786289692\n",
            "Iter #6537216:  Learning rate = 0.000352:   Batch Loss = 0.247371, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.294010192156, Accuracy = 0.977569103241\n",
            "Iter #6541312:  Learning rate = 0.000352:   Batch Loss = 0.228056, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.289150983095, Accuracy = 0.978960156441\n",
            "Iter #6545408:  Learning rate = 0.000352:   Batch Loss = 0.225187, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302741289139, Accuracy = 0.977221369743\n",
            "Iter #6549504:  Learning rate = 0.000352:   Batch Loss = 0.233428, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.294862359762, Accuracy = 0.978438556194\n",
            "Iter #6553600:  Learning rate = 0.000352:   Batch Loss = 0.226897, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.289495646954, Accuracy = 0.982090055943\n",
            "Iter #6557696:  Learning rate = 0.000352:   Batch Loss = 0.229320, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.298930585384, Accuracy = 0.977395236492\n",
            "Iter #6561792:  Learning rate = 0.000352:   Batch Loss = 0.235192, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.285185098648, Accuracy = 0.980003476143\n",
            "Iter #6565888:  Learning rate = 0.000352:   Batch Loss = 0.235606, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302821576595, Accuracy = 0.977916896343\n",
            "Iter #6569984:  Learning rate = 0.000352:   Batch Loss = 0.236445, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291465073824, Accuracy = 0.979134082794\n",
            "Iter #6574080:  Learning rate = 0.000352:   Batch Loss = 0.229331, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.276964336634, Accuracy = 0.981220662594\n",
            "Iter #6578176:  Learning rate = 0.000352:   Batch Loss = 0.230156, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295330375433, Accuracy = 0.978090763092\n",
            "Iter #6582272:  Learning rate = 0.000352:   Batch Loss = 0.229761, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.287700027227, Accuracy = 0.980351269245\n",
            "Iter #6586368:  Learning rate = 0.000352:   Batch Loss = 0.225051, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.288016706705, Accuracy = 0.981220662594\n",
            "Iter #6590464:  Learning rate = 0.000352:   Batch Loss = 0.224429, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.286171287298, Accuracy = 0.981742322445\n",
            "Iter #6594560:  Learning rate = 0.000352:   Batch Loss = 0.224321, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.285406291485, Accuracy = 0.981742322445\n",
            "Iter #6598656:  Learning rate = 0.000352:   Batch Loss = 0.225131, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.285032093525, Accuracy = 0.981394529343\n",
            "Iter #6602752:  Learning rate = 0.000338:   Batch Loss = 0.225869, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.288971573114, Accuracy = 0.981568396091\n",
            "Iter #6606848:  Learning rate = 0.000338:   Batch Loss = 0.222705, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.294314205647, Accuracy = 0.978786289692\n",
            "Iter #6610944:  Learning rate = 0.000338:   Batch Loss = 0.223285, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.307070285082, Accuracy = 0.975830316544\n",
            "Iter #6615040:  Learning rate = 0.000338:   Batch Loss = 0.227516, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.29732427001, Accuracy = 0.979829609394\n",
            "Iter #6619136:  Learning rate = 0.000338:   Batch Loss = 0.236382, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.30763065815, Accuracy = 0.975134730339\n",
            "Iter #6623232:  Learning rate = 0.000338:   Batch Loss = 0.228387, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.292718827724, Accuracy = 0.979655683041\n",
            "Iter #6627328:  Learning rate = 0.000338:   Batch Loss = 0.237145, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295219480991, Accuracy = 0.978438556194\n",
            "Iter #6631424:  Learning rate = 0.000338:   Batch Loss = 0.233322, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.311763614416, Accuracy = 0.975134730339\n",
            "Iter #6635520:  Learning rate = 0.000338:   Batch Loss = 0.235482, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.288459897041, Accuracy = 0.979134082794\n",
            "Iter #6639616:  Learning rate = 0.000338:   Batch Loss = 0.231920, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.293834000826, Accuracy = 0.978090763092\n",
            "Iter #6643712:  Learning rate = 0.000338:   Batch Loss = 0.234434, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299482673407, Accuracy = 0.97704744339\n",
            "Iter #6647808:  Learning rate = 0.000338:   Batch Loss = 0.227364, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.28828138113, Accuracy = 0.979829609394\n",
            "Iter #6651904:  Learning rate = 0.000338:   Batch Loss = 0.231485, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.295201331377, Accuracy = 0.977395236492\n",
            "Iter #6656000:  Learning rate = 0.000338:   Batch Loss = 0.239116, Accuracy = 0.994140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.298575997353, Accuracy = 0.975830316544\n",
            "Iter #6660096:  Learning rate = 0.000338:   Batch Loss = 0.224020, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.30099272728, Accuracy = 0.977743029594\n",
            "Iter #6664192:  Learning rate = 0.000338:   Batch Loss = 0.223706, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.292295485735, Accuracy = 0.978960156441\n",
            "Iter #6668288:  Learning rate = 0.000338:   Batch Loss = 0.225281, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.288759171963, Accuracy = 0.980003476143\n",
            "Iter #6672384:  Learning rate = 0.000338:   Batch Loss = 0.234117, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.298720389605, Accuracy = 0.977221369743\n",
            "Iter #6676480:  Learning rate = 0.000338:   Batch Loss = 0.224945, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.290180563927, Accuracy = 0.979655683041\n",
            "Iter #6680576:  Learning rate = 0.000338:   Batch Loss = 0.232403, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.288126885891, Accuracy = 0.979829609394\n",
            "Iter #6684672:  Learning rate = 0.000338:   Batch Loss = 0.223433, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.285412967205, Accuracy = 0.981220662594\n",
            "Iter #6688768:  Learning rate = 0.000338:   Batch Loss = 0.232037, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.288714587688, Accuracy = 0.980177342892\n",
            "Iter #6692864:  Learning rate = 0.000338:   Batch Loss = 0.226885, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296644568443, Accuracy = 0.977221369743\n",
            "Iter #6696960:  Learning rate = 0.000338:   Batch Loss = 0.236897, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.301530480385, Accuracy = 0.974439203739\n",
            "Iter #6701056:  Learning rate = 0.000324:   Batch Loss = 0.222548, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.291584908962, Accuracy = 0.978960156441\n",
            "Iter #6705152:  Learning rate = 0.000324:   Batch Loss = 0.228385, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299365848303, Accuracy = 0.977395236492\n",
            "Iter #6709248:  Learning rate = 0.000324:   Batch Loss = 0.234863, Accuracy = 0.9921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.29726138711, Accuracy = 0.977916896343\n",
            "Iter #6713344:  Learning rate = 0.000324:   Batch Loss = 0.228660, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302071839571, Accuracy = 0.976525843143\n",
            "Iter #6717440:  Learning rate = 0.000324:   Batch Loss = 0.226327, Accuracy = 0.99609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.294881910086, Accuracy = 0.978786289692\n",
            "Iter #6721536:  Learning rate = 0.000324:   Batch Loss = 0.228231, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.297087728977, Accuracy = 0.978612422943\n",
            "Iter #6725632:  Learning rate = 0.000324:   Batch Loss = 0.225768, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296729177237, Accuracy = 0.977743029594\n",
            "Iter #6729728:  Learning rate = 0.000324:   Batch Loss = 0.225698, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.290737450123, Accuracy = 0.978786289692\n",
            "Iter #6733824:  Learning rate = 0.000324:   Batch Loss = 0.222188, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.290744006634, Accuracy = 0.980872869492\n",
            "Iter #6737920:  Learning rate = 0.000324:   Batch Loss = 0.224314, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.298285067081, Accuracy = 0.980177342892\n",
            "Iter #6742016:  Learning rate = 0.000324:   Batch Loss = 0.224262, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299440681934, Accuracy = 0.980872869492\n",
            "Iter #6746112:  Learning rate = 0.000324:   Batch Loss = 0.227403, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.305980026722, Accuracy = 0.977916896343\n",
            "Iter #6750208:  Learning rate = 0.000324:   Batch Loss = 0.233031, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302622795105, Accuracy = 0.976525843143\n",
            "Iter #6754304:  Learning rate = 0.000324:   Batch Loss = 0.222333, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296842157841, Accuracy = 0.977916896343\n",
            "Iter #6758400:  Learning rate = 0.000324:   Batch Loss = 0.221884, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.297930687666, Accuracy = 0.976699709892\n",
            "Iter #6762496:  Learning rate = 0.000324:   Batch Loss = 0.224919, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.299882292747, Accuracy = 0.979307949543\n",
            "Iter #6766592:  Learning rate = 0.000324:   Batch Loss = 0.220103, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.293358117342, Accuracy = 0.979134082794\n",
            "Iter #6770688:  Learning rate = 0.000324:   Batch Loss = 0.221114, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.296476274729, Accuracy = 0.978612422943\n",
            "Iter #6774784:  Learning rate = 0.000324:   Batch Loss = 0.225933, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.2959010005, Accuracy = 0.977395236492\n",
            "Iter #6778880:  Learning rate = 0.000324:   Batch Loss = 0.245349, Accuracy = 0.990234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.297092914581, Accuracy = 0.977569103241\n",
            "Iter #6782976:  Learning rate = 0.000324:   Batch Loss = 0.223804, Accuracy = 1.0\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.29052054882, Accuracy = 0.977395236492\n",
            "Iter #6787072:  Learning rate = 0.000324:   Batch Loss = 0.224903, Accuracy = 0.998046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.302266955376, Accuracy = 0.97565639019\n",
            "Optimization Finished!\n",
            "FINAL RESULT: Batch Loss = 0.302266955376, Accuracy = 0.97565639019\n",
            "TOTAL TIME:  938.761539936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aaBMsM0PPqw",
        "colab_type": "text"
      },
      "source": [
        "## Results:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PJhE4fTrPPqx",
        "colab_type": "code",
        "outputId": "3ce73091-0d0b-4f1c-ee27-d6debfac4471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1804
        }
      },
      "source": [
        "# (Inline plots: )\n",
        "%matplotlib inline\n",
        "\n",
        "font = {\n",
        "    'family' : 'Bitstream Vera Sans',\n",
        "    'weight' : 'bold',\n",
        "    'size'   : 18\n",
        "}\n",
        "matplotlib.rc('font', **font)\n",
        "\n",
        "width = 12\n",
        "height = 12\n",
        "plt.figure(figsize=(width, height))\n",
        "\n",
        "indep_train_axis = np.array(range(batch_size, (len(train_losses)+1)*batch_size, batch_size))\n",
        "#plt.plot(indep_train_axis, np.array(train_losses),     \"b--\", label=\"Train losses\")\n",
        "plt.plot(indep_train_axis, np.array(train_accuracies), \"g--\", label=\"Train accuracies\")\n",
        "\n",
        "indep_test_axis = np.append(\n",
        "    np.array(range(batch_size, len(test_losses)*display_iter, display_iter)[:-1]),\n",
        "    [training_iters]\n",
        ")\n",
        "#plt.plot(indep_test_axis, np.array(test_losses), \"b-\", linewidth=2.0, label=\"Test losses\")\n",
        "plt.plot(indep_test_axis, np.array(test_accuracies), \"b-\", linewidth=2.0, label=\"Test accuracies\")\n",
        "print len(test_accuracies)\n",
        "print len(train_accuracies)\n",
        "\n",
        "plt.title(\"Training session's Accuracy over Iterations\")\n",
        "plt.legend(loc='lower right', shadow=True)\n",
        "plt.ylabel('Training Accuracy')\n",
        "plt.xlabel('Training Iteration')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Results\n",
        "\n",
        "predictions = one_hot_predictions.argmax(1)\n",
        "\n",
        "print(\"Testing Accuracy: {}%\".format(100*accuracy))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n",
        "print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n",
        "print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"Created using test set of {} datapoints, normalised to % of each class in the test dataset\".format(len(y_test)))\n",
        "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
        "\n",
        "\n",
        "#print(confusion_matrix)\n",
        "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
        "\n",
        "\n",
        "# Plot Results: \n",
        "width = 12\n",
        "height = 12\n",
        "plt.figure(figsize=(width, height))\n",
        "plt.imshow(\n",
        "    normalised_confusion_matrix, \n",
        "    interpolation='nearest', \n",
        "    cmap=plt.cm.Blues\n",
        ")\n",
        "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(n_classes)\n",
        "plt.xticks(tick_marks, LABELS, rotation=90)\n",
        "plt.yticks(tick_marks, LABELS)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1659\n",
            "13256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:1331: UserWarning: findfont: Font family [u'Bitstream Vera Sans'] not found. Falling back to DejaVu Sans\n",
            "  (prop.get_family(), self.defaultFamily[fontext]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwkAAALfCAYAAAAqi44KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x864 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy: 97.565639019%\n",
            "\n",
            "Precision: 97.5692825893%\n",
            "Recall: 97.5656407581%\n",
            "f1_score: 97.5431525698%\n",
            "\n",
            "Confusion Matrix:\n",
            "Created using test set of 5751 datapoints, normalised to % of each class in the test dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0AAAAM8CAYAAAB+g8D4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x864 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8pZ5E1RPPq4",
        "colab_type": "code",
        "outputId": "103ead95-ec05-4073-8420-0cff859e3b02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "\n",
        "X_val_path = DATASET_PATH + \"X_val.txt\"\n",
        "X_val = load_X(X_val_path)\n",
        "print X_val\n",
        "\n",
        "preds = sess.run(\n",
        "   [pred],\n",
        "   feed_dict={\n",
        "       x: X_val\n",
        "  }\n",
        ")\n",
        "print \"ok\\n\"\n",
        "print preds"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[307.589 162.976 319.364 ...   0.    329.752 161.651]\n",
            "  [307.567 162.979 319.362 ...   0.    328.527 161.655]\n",
            "  [306.298 162.951 319.351 ...   0.    328.495 161.681]\n",
            "  ...\n",
            "  [293.291 122.534 307.676 ... 128.953 315.438 119.884]\n",
            "  [289.392 140.743 307.615 ...   0.    315.393 139.408]\n",
            "  [295.848 161.658 307.628 ... 160.331 314.112 160.264]]]\n",
            "ok\n",
            "\n",
            "[array([[ 5.167862  , -1.5922406 , -0.6538908 , -0.7053172 , -1.3355899 ,\n",
            "        -0.80608106]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO3I35VSPPq9",
        "colab_type": "code",
        "outputId": "0b3d49b2-a5b8-46f5-ddff-13981addc288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 59
        }
      },
      "source": [
        "#sess.close()\n",
        "print test_accuracies"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.16031994, 0.25699878, 0.3710659, 0.29055816, 0.43209878, 0.4277517, 0.4133194, 0.4823509, 0.50599897, 0.44757435, 0.44009736, 0.5033907, 0.5645975, 0.62076163, 0.6329334, 0.61919665, 0.42931664, 0.50130415, 0.543036, 0.61484957, 0.5894627, 0.5812902, 0.42027473, 0.6016345, 0.6510172, 0.49991307, 0.6362372, 0.5941575, 0.621631, 0.6531038, 0.6692749, 0.7188315, 0.55538166, 0.5936359, 0.62041384, 0.64649624, 0.68196833, 0.3395931, 0.5680751, 0.50026083, 0.5531212, 0.654321, 0.69170576, 0.6225004, 0.5739871, 0.6005912, 0.6797079, 0.6769258, 0.69240135, 0.71848375, 0.7454356, 0.7221353, 0.76073724, 0.7979482, 0.78403753, 0.80629456, 0.7880369, 0.80403405, 0.8403756, 0.8078595, 0.75795513, 0.8009042, 0.8170753, 0.82837766, 0.7817771, 0.8379412, 0.8473309, 0.86506695, 0.827856, 0.8735872, 0.73934966, 0.8066423, 0.82837766, 0.84089726, 0.8379412, 0.78699356, 0.8328986, 0.8742827, 0.8716745, 0.8621109, 0.83237696, 0.8814119, 0.86228484, 0.8668058, 0.87323946, 0.8692401, 0.88436794, 0.88089025, 0.9007129, 0.68701094, 0.727004, 0.80212134, 0.8748044, 0.8756738, 0.89427924, 0.8490697, 0.86819685, 0.8948009, 0.8767171, 0.8715006, 0.8935837, 0.9041906, 0.90245175, 0.9048861, 0.90053904, 0.90923315, 0.9022779, 0.90140843, 0.9094071, 0.9153191, 0.8994957, 0.91584074, 0.9250565, 0.90123457, 0.8687185, 0.83081204, 0.87532604, 0.9007129, 0.91949224, 0.9156668, 0.9196662, 0.9290558, 0.9274909, 0.9290558, 0.9101026, 0.90210396, 0.93079466, 0.9262737, 0.92575204, 0.9259259, 0.9353156, 0.8814119, 0.8742827, 0.89619195, 0.9071466, 0.9137541, 0.9120153, 0.9161885, 0.9274909, 0.9212311, 0.9346201, 0.91427577, 0.9318379, 0.9353156, 0.9330551, 0.89340985, 0.9231438, 0.8935837, 0.9250565, 0.91427577, 0.92766476, 0.899148, 0.9259259, 0.92453486, 0.92975134, 0.93479395, 0.9187967, 0.9200139, 0.9189706, 0.93879324, 0.9342723, 0.9314902, 0.91757953, 0.9224483, 0.936185, 0.9313163, 0.9210572, 0.9370544, 0.9306208, 0.93479395, 0.90453833, 0.9149713, 0.8928882, 0.9354895, 0.93514174, 0.94261867, 0.93914104, 0.94505304, 0.9473135, 0.95218223, 0.95009565, 0.93983656, 0.9386194, 0.9380977, 0.9156668, 0.9414015, 0.93670666, 0.9221005, 0.9311424, 0.9455747, 0.9252304, 0.9415754, 0.9380977, 0.9471396, 0.95409495, 0.93879324, 0.93201184, 0.9240132, 0.9401843, 0.9464441, 0.9424448, 0.87376106, 0.9165363, 0.9353156, 0.93392456, 0.9240132, 0.91271085, 0.9283603, 0.9405321, 0.9380977, 0.93479395, 0.9412276, 0.9405321, 0.94783515, 0.94905233, 0.93635887, 0.9405321, 0.9462702, 0.9563554, 0.9405321, 0.9514867, 0.93879324, 0.9431403, 0.94783515, 0.9264476, 0.9415754, 0.93079466, 0.9396627, 0.93775, 0.94348806, 0.94470525, 0.94279253, 0.9499217, 0.94383585, 0.9502695, 0.94383585, 0.9539211, 0.93792385, 0.9424448, 0.9473135, 0.9546166, 0.9464441, 0.9476613, 0.94783515, 0.9431403, 0.9504434, 0.94105375, 0.9492262, 0.9502695, 0.9476613, 0.94383585, 0.95565987, 0.9561815, 0.9596592, 0.95844203, 0.9527039, 0.95287776, 0.90523386, 0.91636235, 0.9094071, 0.93879324, 0.9370544, 0.93583727, 0.93792385, 0.9507912, 0.9565293, 0.9353156, 0.9137541, 0.9445314, 0.9570509, 0.94574857, 0.9382716, 0.9433142, 0.95513827, 0.949574, 0.9542688, 0.9535733, 0.96035475, 0.96000695, 0.95722485, 0.95809424, 0.96226746, 0.9558338, 0.95600766, 0.94348806, 0.9506173, 0.955486, 0.9561815, 0.95183444, 0.95218223, 0.95565987, 0.949574, 0.96000695, 0.95913756, 0.94696575, 0.94661796, 0.95600766, 0.9448792, 0.9506173, 0.9586159, 0.9563554, 0.9542688, 0.9502695, 0.95513827, 0.955486, 0.9598331, 0.94192314, 0.95565987, 0.9542688, 0.9601808, 0.9579204, 0.8904538, 0.9187967, 0.92853415, 0.9384455, 0.9494001, 0.9586159, 0.93201184, 0.9408798, 0.9567032, 0.9431403, 0.95218223, 0.96226746, 0.9633107, 0.9523561, 0.95218223, 0.95809424, 0.9594853, 0.9605286, 0.9544427, 0.9382716, 0.93444616, 0.9334029, 0.9483568, 0.9615719, 0.96209353, 0.96226746, 0.9535733, 0.9626152, 0.9492262, 0.9414015, 0.9533994, 0.9593114, 0.9633107, 0.95913756, 0.96435404, 0.9669623, 0.96278906, 0.9579204, 0.95531213, 0.9542688, 0.9636585, 0.9579204, 0.9641802, 0.94661796, 0.9523561, 0.9607025, 0.95878977, 0.9594853, 0.9596592, 0.95896363, 0.9607025, 0.9594853, 0.9666145, 0.9579204, 0.9601808, 0.9615719, 0.97183096, 0.9624413, 0.96278906, 0.9130586, 0.9400104, 0.9384455, 0.9156668, 0.9278386, 0.9374022, 0.95531213, 0.95878977, 0.9558338, 0.94105375, 0.9507912, 0.9607025, 0.9660928, 0.9523561, 0.93879324, 0.9563554, 0.95287776, 0.9605286, 0.9318379, 0.9405321, 0.9610503, 0.9567032, 0.9647018, 0.9653973, 0.96991825, 0.9617458, 0.94070596, 0.96313685, 0.9647018, 0.95878977, 0.9634846, 0.9648757, 0.9647018, 0.9561815, 0.9598331, 0.95496434, 0.95200837, 0.962963, 0.9565293, 0.9443575, 0.96226746, 0.95218223, 0.95913756, 0.9504434, 0.96000695, 0.96191967, 0.9594853, 0.9586159, 0.9502695, 0.95844203, 0.962963, 0.9626152, 0.962963, 0.9664406, 0.9617458, 0.9685272, 0.96800554, 0.96522343, 0.96435404, 0.96504956, 0.96522343, 0.94383585, 0.95531213, 0.9544427, 0.9535733, 0.9570509, 0.95878977, 0.9593114, 0.96591896, 0.9605286, 0.9641802, 0.95600766, 0.9565293, 0.9626152, 0.9586159, 0.96400625, 0.9452269, 0.96748394, 0.88941056, 0.9547905, 0.9292297, 0.87376106, 0.9000174, 0.94296646, 0.9455747, 0.9533994, 0.9565293, 0.95600766, 0.95687705, 0.95722485, 0.9586159, 0.9558338, 0.96226746, 0.9507912, 0.94470525, 0.95565987, 0.9615719, 0.9634846, 0.9655712, 0.96000695, 0.968875, 0.9669623, 0.9725265, 0.9641802, 0.9700922, 0.9678317, 0.96748394, 0.96939665, 0.97217876, 0.9669623, 0.962963, 0.9657451, 0.9598331, 0.9695705, 0.9687011, 0.96226746, 0.9586159, 0.97130936, 0.9723526, 0.9756564, 0.9687011, 0.9615719, 0.9706138, 0.96435404, 0.9655712, 0.96748394, 0.9709616, 0.96817946, 0.9700922, 0.9626152, 0.96087635, 0.9660928, 0.9707877, 0.96226746, 0.95913756, 0.96591896, 0.9706138, 0.9709616, 0.97113544, 0.9700922, 0.97113544, 0.9687011, 0.968875, 0.97130936, 0.97183096, 0.96939665, 0.96504956, 0.9664406, 0.9547905, 0.9704399, 0.9687011, 0.9685272, 0.9720049, 0.9735698, 0.97496086, 0.9610503, 0.9641802, 0.9570509, 0.9645279, 0.96939665, 0.96504956, 0.9669623, 0.9727004, 0.9666145, 0.9638324, 0.9546166, 0.9687011, 0.9687011, 0.97339594, 0.9735698, 0.9707877, 0.9695705, 0.9666145, 0.96800554, 0.97339594, 0.9676578, 0.9374022, 0.9487046, 0.9546166, 0.9610503, 0.968875, 0.9692227, 0.9638324, 0.95913756, 0.96209353, 0.9452269, 0.95374715, 0.9645279, 0.97183096, 0.9667884, 0.96835333, 0.96939665, 0.9678317, 0.9707877, 0.9754825, 0.9716571, 0.9634846, 0.9707877, 0.9687011, 0.96835333, 0.9700922, 0.9704399, 0.97304815, 0.9728743, 0.96748394, 0.9723526, 0.9634846, 0.9594853, 0.96313685, 0.9546166, 0.96278906, 0.9660928, 0.9573987, 0.9607025, 0.96313685, 0.96748394, 0.96731, 0.97461313, 0.9692227, 0.9692227, 0.9573987, 0.97339594, 0.97148323, 0.97183096, 0.9739176, 0.96817946, 0.9647018, 0.96835333, 0.9655712, 0.97130936, 0.9638324, 0.97530866, 0.96504956, 0.97339594, 0.9685272, 0.9648757, 0.96278906, 0.97217876, 0.9697444, 0.968875, 0.97113544, 0.9709616, 0.97217876, 0.9758303, 0.9700922, 0.97130936, 0.96991825, 0.97304815, 0.9700922, 0.97304815, 0.97496086, 0.96400625, 0.9692227, 0.9655712, 0.9707877, 0.97304815, 0.96748394, 0.9706138, 0.9598331, 0.9723526, 0.96435404, 0.9633107, 0.9334029, 0.9415754, 0.96191967, 0.95878977, 0.9598331, 0.9707877, 0.96731, 0.96035475, 0.96122414, 0.9700922, 0.9704399, 0.96991825, 0.9666145, 0.9706138, 0.9657451, 0.97148323, 0.9723526, 0.97026604, 0.962963, 0.9735698, 0.9727004, 0.9716571, 0.96191967, 0.968875, 0.96904886, 0.9739176, 0.9739176, 0.9727004, 0.96991825, 0.96939665, 0.9720049, 0.9756564, 0.9728743, 0.9697444, 0.9485307, 0.9641802, 0.968875, 0.9735698, 0.9707877, 0.9678317, 0.9707877, 0.97304815, 0.9756564, 0.9744392, 0.9756564, 0.9667884, 0.97461313, 0.96504956, 0.9725265, 0.9720049, 0.9754825, 0.9700922, 0.9667884, 0.9695705, 0.97130936, 0.97130936, 0.9634846, 0.96817946, 0.97461313, 0.97113544, 0.9723526, 0.97426534, 0.9768736, 0.96904886, 0.95496434, 0.96226746, 0.9707877, 0.974787, 0.9607025, 0.9664406, 0.9756564, 0.97722137, 0.948009, 0.9504434, 0.9507912, 0.9607025, 0.97217876, 0.97148323, 0.93948877, 0.9669623, 0.9707877, 0.97148323, 0.9709616, 0.9709616, 0.9728743, 0.9740915, 0.9737437, 0.9758303, 0.9758303, 0.9695705, 0.9692227, 0.9725265, 0.97617805, 0.9725265, 0.9760042, 0.9779169, 0.97496086, 0.97722137, 0.977743, 0.9739176, 0.9692227, 0.97304815, 0.97530866, 0.97217876, 0.9728743, 0.97513473, 0.9791341, 0.97617805, 0.9735698, 0.9737437, 0.9692227, 0.9732221, 0.9768736, 0.96800554, 0.9720049, 0.9723526, 0.97426534, 0.97339594, 0.9760042, 0.97704744, 0.974787, 0.9744392, 0.9725265, 0.98017734, 0.97722137, 0.97722137, 0.9768736, 0.9740915, 0.974787, 0.9766997, 0.9700922, 0.97739524, 0.97652584, 0.97339594, 0.9754825, 0.97530866, 0.974787, 0.9758303, 0.9763519, 0.9737437, 0.9669623, 0.9697444, 0.9664406, 0.97304815, 0.97496086, 0.97113544, 0.9704399, 0.974787, 0.9700922, 0.9756564, 0.9645279, 0.9706138, 0.97513473, 0.9754825, 0.97130936, 0.97426534, 0.97704744, 0.97461313, 0.9697444, 0.97113544, 0.9645279, 0.9728743, 0.9728743, 0.977743, 0.9737437, 0.97722137, 0.9786124, 0.9766997, 0.97652584, 0.97652584, 0.9744392, 0.9763519, 0.9758303, 0.97217876, 0.96400625, 0.9704399, 0.96591896, 0.96904886, 0.97217876, 0.9763519, 0.9766997, 0.9728743, 0.9735698, 0.9744392, 0.9763519, 0.9775691, 0.9667884, 0.9700922, 0.97652584, 0.97026604, 0.9728743, 0.9754825, 0.9744392, 0.9737437, 0.9760042, 0.9744392, 0.9735698, 0.96817946, 0.9720049, 0.97461313, 0.9766997, 0.9760042, 0.95183444, 0.96835333, 0.97652584, 0.96991825, 0.9739176, 0.9763519, 0.97304815, 0.9723526, 0.97217876, 0.9720049, 0.9728743, 0.96835333, 0.97183096, 0.9754825, 0.97617805, 0.9727004, 0.97496086, 0.9779169, 0.97617805, 0.9758303, 0.97130936, 0.96835333, 0.9723526, 0.9725265, 0.97339594, 0.9766997, 0.977743, 0.97496086, 0.9720049, 0.9739176, 0.97530866, 0.97617805, 0.977743, 0.9768736, 0.97652584, 0.97304815, 0.9775691, 0.9685272, 0.97530866, 0.9725265, 0.97026604, 0.97617805, 0.9779169, 0.9766997, 0.9754825, 0.9760042, 0.97896016, 0.9766997, 0.9794818, 0.9756564, 0.9794818, 0.98035127, 0.9786124, 0.9760042, 0.97809076, 0.9794818, 0.9728743, 0.9775691, 0.97217876, 0.97930795, 0.9775691, 0.98209006, 0.9786124, 0.97617805, 0.97722137, 0.9763519, 0.97826463, 0.9744392, 0.97809076, 0.9744392, 0.97426534, 0.97722137, 0.9796557, 0.9744392, 0.97304815, 0.9723526, 0.9754825, 0.9735698, 0.9779169, 0.9768736, 0.96991825, 0.9760042, 0.97496086, 0.9779169, 0.97843856, 0.9744392, 0.97739524, 0.96991825, 0.97739524, 0.9796557, 0.9800035, 0.9720049, 0.97739524, 0.96748394, 0.97304815, 0.9735698, 0.9739176, 0.97426534, 0.9794818, 0.97722137, 0.9756564, 0.96817946, 0.9617458, 0.9687011, 0.9728743, 0.97617805, 0.9740915, 0.96400625, 0.8365502, 0.9647018, 0.97217876, 0.9660928, 0.96713614, 0.9700922, 0.9695705, 0.97339594, 0.9763519, 0.9763519, 0.9775691, 0.9791341, 0.97426534, 0.9766997, 0.9775691, 0.977743, 0.97930795, 0.9775691, 0.9786124, 0.97843856, 0.974787, 0.9732221, 0.9754825, 0.9766997, 0.97130936, 0.97530866, 0.9787863, 0.9709616, 0.9787863, 0.97896016, 0.97843856, 0.97826463, 0.97826463, 0.97722137, 0.97217876, 0.97843856, 0.9786124, 0.97617805, 0.9728743, 0.97930795, 0.97809076, 0.97704744, 0.9758303, 0.97930795, 0.97930795, 0.9819162, 0.9763519, 0.9810468, 0.9815684, 0.9810468, 0.9779169, 0.97843856, 0.9787863, 0.9760042, 0.9813945, 0.97652584, 0.97896016, 0.97496086, 0.9768736, 0.97617805, 0.97461313, 0.9737437, 0.9735698, 0.97217876, 0.9754825, 0.9739176, 0.96904886, 0.9676578, 0.9685272, 0.9740915, 0.9760042, 0.9798296, 0.974787, 0.9768736, 0.9728743, 0.98035127, 0.9794818, 0.9791341, 0.9791341, 0.97722137, 0.9756564, 0.97652584, 0.97652584, 0.97826463, 0.97809076, 0.9786124, 0.9819162, 0.97426534, 0.9760042, 0.9800035, 0.98087287, 0.98243785, 0.9786124, 0.98017734, 0.9763519, 0.97826463, 0.97530866, 0.97722137, 0.97826463, 0.9706138, 0.98087287, 0.97930795, 0.9779169, 0.9735698, 0.9766997, 0.97722137, 0.97843856, 0.97530866, 0.9685272, 0.9763519, 0.9754825, 0.9775691, 0.9791341, 0.97826463, 0.974787, 0.97809076, 0.9768736, 0.97722137, 0.97530866, 0.98017734, 0.98052514, 0.97530866, 0.9766997, 0.974787, 0.9786124, 0.97652584, 0.9725265, 0.9756564, 0.97896016, 0.9786124, 0.9800035, 0.98243785, 0.9740915, 0.9787863, 0.97843856, 0.97304815, 0.9735698, 0.96748394, 0.97896016, 0.97722137, 0.98052514, 0.97843856, 0.96400625, 0.9735698, 0.97217876, 0.974787, 0.9735698, 0.97461313, 0.9744392, 0.9768736, 0.9695705, 0.9676578, 0.9758303, 0.97130936, 0.9766997, 0.97617805, 0.9786124, 0.9754825, 0.97739524, 0.9791341, 0.9760042, 0.98035127, 0.9796557, 0.97704744, 0.97739524, 0.9775691, 0.9800035, 0.98052514, 0.97617805, 0.9735698, 0.97461313, 0.98035127, 0.9744392, 0.97930795, 0.97826463, 0.9775691, 0.9796557, 0.9775691, 0.9754825, 0.9775691, 0.97513473, 0.9786124, 0.97426534, 0.97809076, 0.9727004, 0.97461313, 0.9740915, 0.9794818, 0.97896016, 0.9794818, 0.9787863, 0.9787863, 0.97652584, 0.9766997, 0.9768736, 0.98017734, 0.9786124, 0.9787863, 0.98035127, 0.97652584, 0.97722137, 0.9819162, 0.97426534, 0.96522343, 0.97426534, 0.97530866, 0.9756564, 0.9813945, 0.97426534, 0.97843856, 0.9766997, 0.9763519, 0.9794818, 0.9787863, 0.9775691, 0.9819162, 0.9763519, 0.9775691, 0.9766997, 0.9798296, 0.977743, 0.9758303, 0.9735698, 0.97826463, 0.977743, 0.9700922, 0.97617805, 0.97896016, 0.97930795, 0.97843856, 0.97426534, 0.9815684, 0.97739524, 0.97896016, 0.98035127, 0.9760042, 0.9760042, 0.9815684, 0.9779169, 0.9831334, 0.98017734, 0.98122066, 0.980699, 0.9723526, 0.97530866, 0.9800035, 0.9758303, 0.9716571, 0.9817423, 0.9779169, 0.9791341, 0.9798296, 0.97496086, 0.9720049, 0.97826463, 0.9740915, 0.9779169, 0.97826463, 0.9791341, 0.98087287, 0.9819162, 0.9798296, 0.9798296, 0.9815684, 0.97826463, 0.980699, 0.9798296, 0.97843856, 0.97652584, 0.9796557, 0.9800035, 0.9796557, 0.9720049, 0.98087287, 0.97426534, 0.9704399, 0.977743, 0.97722137, 0.9754825, 0.9739176, 0.9796557, 0.98122066, 0.97896016, 0.9810468, 0.98052514, 0.98035127, 0.98087287, 0.97704744, 0.97826463, 0.9700922, 0.9794818, 0.9760042, 0.9758303, 0.98035127, 0.980699, 0.98052514, 0.977743, 0.9744392, 0.9813945, 0.9826117, 0.9826117, 0.97722137, 0.98035127, 0.97652584, 0.9819162, 0.98330724, 0.9756564, 0.97809076, 0.9794818, 0.9768736, 0.97722137, 0.9800035, 0.97930795, 0.9796557, 0.97896016, 0.9800035, 0.980699, 0.9791341, 0.9798296, 0.9791341, 0.9779169, 0.9787863, 0.97809076, 0.9786124, 0.9779169, 0.9791341, 0.97652584, 0.9723526, 0.974787, 0.98087287, 0.97809076, 0.9787863, 0.9768736, 0.980699, 0.9800035, 0.9800035, 0.9796557, 0.97809076, 0.9810468, 0.98052514, 0.9779169, 0.97896016, 0.9744392, 0.97930795, 0.9798296, 0.9775691, 0.9791341, 0.98087287, 0.9794818, 0.97339594, 0.9756564, 0.9819162, 0.9800035, 0.97843856, 0.9768736, 0.97704744, 0.9732221, 0.9779169, 0.97809076, 0.97496086, 0.9798296, 0.980699, 0.97722137, 0.9775691, 0.97513473, 0.98035127, 0.9768736, 0.9766997, 0.97826463, 0.9754825, 0.9775691, 0.97896016, 0.9794818, 0.9813945, 0.9779169, 0.9791341, 0.9813945, 0.9798296, 0.9810468, 0.9819162, 0.977743, 0.9758303, 0.97704744, 0.98243785, 0.98052514, 0.98017734, 0.9787863, 0.9796557, 0.9815684, 0.9647018, 0.9768736, 0.97826463, 0.98122066, 0.9817423, 0.9796557, 0.9800035, 0.97930795, 0.98017734, 0.9794818, 0.9779169, 0.980699, 0.97739524, 0.9766997, 0.9779169, 0.9758303, 0.9791341, 0.97739524, 0.9800035, 0.9798296, 0.97617805, 0.97826463, 0.9626152, 0.9779169, 0.9766997, 0.9786124, 0.97930795, 0.9798296, 0.98209006, 0.98122066, 0.9766997, 0.9768736, 0.97704744, 0.9744392, 0.97843856, 0.97722137, 0.97826463, 0.9727004, 0.97809076, 0.9760042, 0.9728743, 0.97652584, 0.9798296, 0.9791341, 0.97826463, 0.9786124, 0.97461313, 0.9763519, 0.98017734, 0.9794818, 0.9678317, 0.97530866, 0.97843856, 0.97513473, 0.9739176, 0.9763519, 0.97704744, 0.97722137, 0.9800035, 0.97896016, 0.9766997, 0.9813945, 0.9800035, 0.9817423, 0.9791341, 0.9796557, 0.97739524, 0.98017734, 0.9796557, 0.97652584, 0.9760042, 0.9756564, 0.9798296, 0.9786124, 0.9787863, 0.9735698, 0.9786124, 0.9817423, 0.9794818, 0.97896016, 0.9779169, 0.97809076, 0.97930795, 0.977743, 0.98087287, 0.98052514, 0.980699, 0.98087287, 0.97722137, 0.9786124, 0.98087287, 0.9779169, 0.9786124, 0.9798296, 0.98035127, 0.9815684, 0.9800035, 0.9817423, 0.98052514, 0.97896016, 0.9796557, 0.9796557, 0.980699, 0.98052514, 0.9601808, 0.9655712, 0.9601808, 0.9739176, 0.9594853, 0.9720049, 0.9697444, 0.9796557, 0.97843856, 0.9760042, 0.9798296, 0.9763519, 0.98035127, 0.9810468, 0.980699, 0.97843856, 0.9768736, 0.9775691, 0.98122066, 0.98243785, 0.98209006, 0.97930795, 0.97826463, 0.98035127, 0.98052514, 0.9810468, 0.98087287, 0.98122066, 0.97896016, 0.98035127, 0.98243785, 0.9819162, 0.977743, 0.97722137, 0.9827856, 0.9815684, 0.98052514, 0.98052514, 0.98035127, 0.97930795, 0.98243785, 0.9813945, 0.98243785, 0.98122066, 0.98035127, 0.97739524, 0.9813945, 0.98087287, 0.9787863, 0.98035127, 0.9810468, 0.98243785, 0.97722137, 0.9796557, 0.9810468, 0.98035127, 0.9800035, 0.98052514, 0.9800035, 0.9791341, 0.9779169, 0.9798296, 0.980699, 0.9817423, 0.9810468, 0.98052514, 0.9779169, 0.97826463, 0.9791341, 0.98087287, 0.9779169, 0.98052514, 0.9796557, 0.98087287, 0.9791341, 0.9798296, 0.97652584, 0.97652584, 0.9796557, 0.96713614, 0.97930795, 0.98087287, 0.9800035, 0.98122066, 0.98017734, 0.9826117, 0.982264, 0.97896016, 0.97896016, 0.97704744, 0.98017734, 0.9766997, 0.980699, 0.98017734, 0.98035127, 0.9829595, 0.9787863, 0.980699, 0.980699, 0.98017734, 0.9779169, 0.98035127, 0.98035127, 0.9787863, 0.9786124, 0.9787863, 0.98017734, 0.97843856, 0.9798296, 0.9798296, 0.9787863, 0.97843856, 0.9819162, 0.98017734, 0.9791341, 0.98122066, 0.98122066, 0.9779169, 0.97930795, 0.9798296, 0.9800035, 0.98122066, 0.9786124, 0.9779169, 0.97896016, 0.97896016, 0.9810468, 0.977743, 0.97843856, 0.9800035, 0.97930795, 0.9796557, 0.9775691, 0.97930795, 0.9786124, 0.9763519, 0.97461313, 0.9768736, 0.97426534, 0.9766997, 0.97704744, 0.9760042, 0.9766997, 0.97722137, 0.97843856, 0.9779169, 0.9786124, 0.9768736, 0.9796557, 0.9775691, 0.9775691, 0.98035127, 0.9791341, 0.97722137, 0.98087287, 0.97722137, 0.9779169, 0.9779169, 0.9786124, 0.97843856, 0.9787863, 0.9775691, 0.97896016, 0.97722137, 0.97843856, 0.98209006, 0.97739524, 0.9800035, 0.9779169, 0.9791341, 0.98122066, 0.97809076, 0.98035127, 0.98122066, 0.9817423, 0.9817423, 0.9813945, 0.9815684, 0.9787863, 0.9758303, 0.9798296, 0.97513473, 0.9796557, 0.97843856, 0.97513473, 0.9791341, 0.97809076, 0.97704744, 0.9798296, 0.97739524, 0.9758303, 0.977743, 0.97896016, 0.9800035, 0.97722137, 0.9796557, 0.9798296, 0.98122066, 0.98017734, 0.97722137, 0.9744392, 0.97896016, 0.97739524, 0.9779169, 0.97652584, 0.9787863, 0.9786124, 0.977743, 0.9787863, 0.98087287, 0.98017734, 0.98087287, 0.9779169, 0.97652584, 0.9779169, 0.9766997, 0.97930795, 0.9791341, 0.9786124, 0.97739524, 0.9775691, 0.97739524, 0.9756564, 0.9756564]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AXbWCH_bftp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saver = tf.train.Saver()\n",
        "# !mkdir model\n",
        "save_path = saver.save(sess, 'model3/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOjRD6QzxAaF",
        "colab_type": "code",
        "outputId": "f51bf561-072d-4eee-ad2d-03dff85008a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "def freeze_graph(model_dir, output_node_names):\n",
        "    \"\"\"Extract the sub graph defined by the output nodes and convert \n",
        "    all its variables into constant \n",
        "    Args:\n",
        "        model_dir: the root folder containing the checkpoint state file\n",
        "        output_node_names: a string, containing all the output node's names, \n",
        "                            comma separated\n",
        "    \"\"\"\n",
        "    if not tf.gfile.Exists(model_dir):\n",
        "        raise AssertionError(\n",
        "            \"Export directory doesn't exists. Please specify an export \"\n",
        "            \"directory: %s\" % model_dir)\n",
        "\n",
        "    if not output_node_names:\n",
        "        print(\"You need to supply the name of a node to --output_node_names.\")\n",
        "        return -1\n",
        "\n",
        "    # We retrieve our checkpoint fullpath\n",
        "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
        "    input_checkpoint = checkpoint.model_checkpoint_path\n",
        "    \n",
        "    # We precise the file fullname of our freezed graph\n",
        "    absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\n",
        "#     absolute_model_dir = 'model/'\n",
        "    output_graph = absolute_model_dir + \"/frozen_model.pb\"\n",
        "\n",
        "    # We clear devices to allow TensorFlow to control on which device it will load operations\n",
        "    clear_devices = True\n",
        "\n",
        "    # We start a session using a temporary fresh Graph\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        # We import the meta graph in the current default Graph\n",
        "        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
        "\n",
        "        # We restore the weights\n",
        "        saver.restore(sess, input_checkpoint)\n",
        "\n",
        "        # We use a built-in TF helper to export variables to constants\n",
        "        output_graph_def = tf.compat.v1.graph_util.convert_variables_to_constants(\n",
        "            sess, # The session is used to retrieve the weights\n",
        "            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \n",
        "            output_node_names.split(\",\") # The output node names are used to select the usefull nodes\n",
        "        ) \n",
        "\n",
        "        # Finally we serialize and dump the output graph to the filesystem\n",
        "        with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
        "            f.write(output_graph_def.SerializeToString())\n",
        "        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n",
        "\n",
        "    return\n",
        "  \n",
        "freeze_graph('model2', 'output')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from model2/model\n",
            "WARNING:tensorflow:From <ipython-input-18-f741260948e8>:42: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.compat.v1.graph_util.extract_sub_graph\n",
            "INFO:tensorflow:Froze 8 variables.\n",
            "INFO:tensorflow:Converted 8 variables to const ops.\n",
            "1149 ops in the final graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVA3TQWSb87Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install pydrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUB7sjPRicWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pydrive.auth import GoogleAuth\n",
        "# from pydrive.drive import GoogleDrive\n",
        "# from google.colab import auth\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# # Authenticate and create the PyDrive client.\n",
        "# # This only needs to be done once in a notebook.\n",
        "# auth.authenticate_user()\n",
        "# gauth = GoogleAuth()\n",
        "# gauth.credentials = GoogleCredentials.get_application_default()\n",
        "# drive = GoogleDrive(gauth)\n",
        "\n",
        "# uploaded = drive.CreateFile({'title': 'poseLSTM2.data-00000-of-00001'})\n",
        "# uploaded.SetContentFile('model/poseLSTM2.data-00000-of-00001')\n",
        "# uploaded.Upload()\n",
        "# print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "\n",
        "# uploaded = drive.CreateFile({'title': 'poseLSTM2.index'})\n",
        "# uploaded.SetContentFile('model/poseLSTM2.index')\n",
        "# uploaded.Upload()\n",
        "# print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "\n",
        "# uploaded = drive.CreateFile({'title': 'poseLSTM2.meta'})\n",
        "# uploaded.SetContentFile('model/poseLSTM2.meta')\n",
        "# uploaded.Upload()\n",
        "# print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "\n",
        "# uploaded = drive.CreateFile({'title': 'checkpoint'})\n",
        "# uploaded.SetContentFile('model/checkpoint')\n",
        "# uploaded.Upload()\n",
        "# print('Uploaded file with ID {}'.format(uploaded.get('id')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdMeTU1v3vbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.saved_model.simple_save(sess,\n",
        "#                           export_dir='newpb3',\n",
        "#                           inputs={'Input': x},\n",
        "#                           outputs={'output': y})\n",
        "\n",
        "# #saved\n",
        "# uploaded = drive.CreateFile({'title': 'poseLSTM2.pb'})\n",
        "# uploaded.SetContentFile('newpb3/saved_model.pb')\n",
        "# uploaded.Upload()\n",
        "# print('Uploaded file with ID {}'.format(uploaded.get('id')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35ABMFpQDawh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tensorflow.python.saved_model\n",
        "# from tensorflow.python.saved_model import tag_constants\n",
        "# from tensorflow.python.saved_model.signature_def_utils_impl import predict_signature_def\n",
        "\n",
        "# builder = tf.saved_model.builder.SavedModelBuilder('./SavedModel/')\n",
        "# signature = predict_signature_def(inputs={'input': x},\n",
        "#                                   outputs={'output': y})\n",
        "\n",
        "# builder.add_meta_graph_and_variables(sess,\n",
        "#                                    [tf.saved_model.tag_constants.SERVING],\n",
        "#                                    signature_def_map={'predict':signature},\n",
        "#                                     strip_default_attrs=True)\n",
        "# builder.save()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxm6HoqHQn7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with tf.Session(graph=tf.Graph()) as sess:\n",
        "#     tf.saved_model.loader.load(sess, [\"serve\"], './SavedModel/')\n",
        "#     graph = tf.get_default_graph()\n",
        "#     print(graph.get_operations())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6bM9AjkVbY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.train.write_graph(sess.graph.as_graph_def(), '.', 'model2.pbtxt', as_text=True)\n",
        "\n",
        "# from tensorflow.python.tools import freeze_graph\n",
        "\n",
        "# freeze_graph.freeze_graph('model2.pbtxt', \"\", False, \n",
        "#                           'model/checkpoint', \"output\",\n",
        "#                            \"save/restore_all\", \"save/Const:0\",\n",
        "#                            'Model.pb', True, \"\"  \n",
        "#                          )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xcj96IOXm654",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
        "#         sess,\n",
        "#         sess.graph_def,\n",
        "#         ['output'])\n",
        "# with open('output_graph2.pb', 'wb') as f:\n",
        "#   f.write(frozen_graph_def.SerializeToString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_ZZTOoa3mv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saver = tf.train.Saver()\n",
        "\n",
        "# Later, launch the model, use the saver to restore variables from disk, and\n",
        "# do some work with the model.\n",
        "with tf.Session() as sess:\n",
        "  # Restore variables from disk.\n",
        "  saver.restore(sess, \"model.ckpt\")\n",
        "  print(\"Model restored.\")\n",
        "  print(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KRBnB467-xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "# files.download('model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agyQaCZdfw9Y",
        "colab_type": "code",
        "outputId": "add17d6a-2c5d-4c07-ca2c-3d7f8be05274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1754
        }
      },
      "source": [
        "data = np.load('data.npy')\n",
        "with tf.Session() as sess:\n",
        "  y_out = sess.run(\n",
        "    [pred],\n",
        "    feed_dict={x: [data]}\n",
        "  )\n",
        "  print y_out\n",
        "  print np.argmax(y_out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5c84bdb0036b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   y_out = sess.run(\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   )\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mprint\u001b[0m \u001b[0my_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Variable_4\n\t [[node Variable_4/read (defined at <ipython-input-8-f213a2d7b153>:11) ]]\n\t [[node output (defined at <ipython-input-7-8dee4e0301f6>:21) ]]\n\nCaused by op u'Variable_4/read', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-f213a2d7b153>\", line 11, in <module>\n    'out': tf.Variable(tf.random_normal([n_classes]))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 213, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 176, in _variable_v1_call\n    aggregation=aggregation)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 155, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2495, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 217, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 1395, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 1557, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 81, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3890, in identity\n    \"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable_4\n\t [[node Variable_4/read (defined at <ipython-input-8-f213a2d7b153>:11) ]]\n\t [[node output (defined at <ipython-input-7-8dee4e0301f6>:21) ]]\n"
          ]
        }
      ]
    }
  ]
}