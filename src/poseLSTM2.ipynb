{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "name": "poseLSTM2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "002b1168-c1d5-4b85-ee09-89dd453ced8d",
        "colab_type": "code",
        "id": "A2NPWvSqPlaR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!ls \"/content/drive/My Drive\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n",
            " BFS.cpp\t\t\t     'poseLSTM2 (2).index'\n",
            " BI2\t\t\t\t     'poseLSTM2 (2).meta'\n",
            " checkpoint\t\t\t      poseLSTM2.data-00000-of-00001\n",
            "'checkpoint (1)'\t\t      poseLSTM2.index\n",
            "'checkpoint (2)'\t\t      poseLSTM2.meta\n",
            "'Colab Notebooks'\t\t      poseLSTM2.pb\n",
            " data3\t\t\t\t      poseLSTM.data-00000-of-00001\n",
            " eyes\t\t\t\t      poseLSTM_model\n",
            "'Getting started.pdf'\t\t      RNN-HAR-2D-Pose-database.zip\n",
            "'poseLSTM2 (1).data-00000-of-00001'   train_supervised_pose.csv\n",
            "'poseLSTM2 (1).index'\t\t      train_unsupervised_pose.tsv\n",
            "'poseLSTM2 (1).meta'\t\t      validation_supervised_pose.csv\n",
            "'poseLSTM2 (1).pb'\t\t      validation_unsupervised_pose.tsv\n",
            "'poseLSTM2 (2).data-00000-of-00001'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "e0d657ab-c5e5-4a97-9c05-7c9aa254c404",
        "colab_type": "code",
        "id": "0unkoS2kR3FD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!unzip \"/content/drive/My Drive/RNN-HAR-2D-Pose-database.zip\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/RNN-HAR-2D-Pose-database.zip\n",
            "   creating: RNN-HAR-2D-Pose-database/\n",
            "  inflating: RNN-HAR-2D-Pose-database/README.md  \n",
            "  inflating: RNN-HAR-2D-Pose-database/X_val2.txt  \n",
            "  inflating: RNN-HAR-2D-Pose-database/X_val.txt  \n",
            "  inflating: RNN-HAR-2D-Pose-database/Y_train.txt  \n",
            "  inflating: RNN-HAR-2D-Pose-database/Y_test.txt  \n",
            "  inflating: RNN-HAR-2D-Pose-database/X_train.txt  \n",
            "  inflating: RNN-HAR-2D-Pose-database/X_test.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XRjgZ8IHPPpt",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
        "from sklearn import metrics\n",
        "import random\n",
        "from random import randint\n",
        "import time\n",
        "import os\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0ghEY_0oPPp1"
      },
      "source": [
        "## Preparing dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q1I1wTMxPPp3",
        "colab": {}
      },
      "source": [
        "# Useful Constants\n",
        "\n",
        "# Output classes to learn how to classify\n",
        "LABELS = [\n",
        "    \"JUMPING\",\n",
        "    \"JUMPING_JACKS\",\n",
        "    \"BOXING\",\n",
        "    \"WAVING_2HANDS\",\n",
        "    \"WAVING_1HAND\",\n",
        "    \"CLAPPING_HANDS\"\n",
        "\n",
        "]\n",
        "DATASET_PATH = \"RNN-HAR-2D-Pose-database/\"\n",
        "\n",
        "X_train_path = DATASET_PATH + \"X_train.txt\"\n",
        "X_test_path = DATASET_PATH + \"X_test.txt\"\n",
        "\n",
        "y_train_path = DATASET_PATH + \"Y_train.txt\"\n",
        "y_test_path = DATASET_PATH + \"Y_test.txt\"\n",
        "\n",
        "n_steps = 32 # 32 timesteps per series"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "piJXHD9iPPp-",
        "colab": {}
      },
      "source": [
        "\n",
        "# Load the networks inputs\n",
        "\n",
        "def load_X(X_path):\n",
        "    file = open(X_path, 'r')\n",
        "    X_ = np.array(\n",
        "        [elem for elem in [\n",
        "            row.split(',') for row in file\n",
        "        ]],\n",
        "        dtype=np.float32\n",
        "    )\n",
        "    file.close()\n",
        "    blocks = int(len(X_) / n_steps)\n",
        "\n",
        "    X_ = np.array(np.split(X_,blocks))\n",
        "\n",
        "    return X_\n",
        "\n",
        "# Load the networks outputs\n",
        "\n",
        "def load_y(y_path):\n",
        "    file = open(y_path, 'r')\n",
        "    y_ = np.array(\n",
        "        [elem for elem in [\n",
        "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "        ]],\n",
        "        dtype=np.int32\n",
        "    )\n",
        "    file.close()\n",
        "\n",
        "    # for 0-based indexing\n",
        "    return y_ - 1\n",
        "\n",
        "X_train = load_X(X_train_path)\n",
        "X_test = load_X(X_test_path)\n",
        "#print X_test\n",
        "\n",
        "y_train = load_y(y_train_path)\n",
        "y_test = load_y(y_test_path)\n",
        "# proof that it actually works for the skeptical: replace labelled classes with random classes to train on\n",
        "#for i in range(len(y_train)):\n",
        "#    y_train[i] = randint(0, 5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8tqk7CrbPPqD"
      },
      "source": [
        "## Set Parameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "08b9ca08-1d4e-4a5b-b47b-ec3776913d70",
        "colab_type": "code",
        "id": "N28TvyJPPPqF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Input Data\n",
        "\n",
        "training_data_count = len(X_train)  # 4519 training series (with 50% overlap between each serie)\n",
        "test_data_count = len(X_test)  # 1197 test series\n",
        "n_input = len(X_train[0][0])  # num input parameters per timestep\n",
        "\n",
        "n_hidden = 34 # Hidden layer num of features\n",
        "n_classes = 6\n",
        "\n",
        "#updated for learning-rate decay\n",
        "# calculated as: decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
        "decaying_learning_rate = True\n",
        "learning_rate = 0.0025 #used if decaying_learning_rate set to False\n",
        "init_learning_rate = 0.005\n",
        "decay_rate = 0.96 #the base of the exponential in the decay\n",
        "decay_steps = 100000 #used in decay every 60000 steps with a base of 0.96\n",
        "\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "lambda_loss_amount = 0.0015\n",
        "\n",
        "training_iters = training_data_count *300  # Loop 300 times on the dataset, ie 300 epochs\n",
        "batch_size = 512\n",
        "display_iter = batch_size*8  # To show test set accuracy during training\n",
        "\n",
        "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
        "print(X_train.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
        "print(\"\\nThe dataset has not been preprocessed, is not normalised etc\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "(X shape, y shape, every X's mean, every X's standard deviation)\n",
            "(22625, 32, 36) (5751, 1) 251.01117 126.12204\n",
            "\n",
            "The dataset has not been preprocessed, is not normalised etc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D-K-KEVWPPqQ"
      },
      "source": [
        "## Utility functions for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xpz7E9pzPPqS",
        "colab": {}
      },
      "source": [
        "def LSTM_RNN(_X, _weights, _biases):\n",
        "    # model architecture based on \"guillaume-chevalier\" and \"aymericdamien\" under the MIT license.\n",
        "\n",
        "    _X = tf.transpose(a=_X, perm=[1, 0, 2])  # permute n_steps and batch_size\n",
        "    _X = tf.reshape(_X, [-1, n_input])\n",
        "    # Rectifies Linear Unit activation function used\n",
        "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
        "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
        "    _X = tf.split(_X, n_steps, 0)\n",
        "\n",
        "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
        "    lstm_cell_1 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
        "    lstm_cell_2 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
        "    lstm_cells = tf.compat.v1.nn.rnn_cell.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
        "    outputs, states = tf.compat.v1.nn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
        "\n",
        "    # A single output is produced, in style of \"many to one\" classifier, refer to http://karpathy.github.io/2015/05/21/rnn-effectiveness/ for details\n",
        "    lstm_last_output = outputs[-1]\n",
        "\n",
        "    # Linear activation\n",
        "    return tf.add(tf.matmul(lstm_last_output, _weights['out']), _biases['out'], name='output')\n",
        "\n",
        "\n",
        "def extract_batch_size(_train, _labels, _unsampled, batch_size):\n",
        "    # Fetch a \"batch_size\" amount of data and labels from \"(X|y)_train\" data.\n",
        "    # Elements of each batch are chosen randomly, without replacement, from X_train with corresponding label from Y_train\n",
        "    # unsampled_indices keeps track of sampled data ensuring non-replacement. Resets when remaining datapoints < batch_size\n",
        "\n",
        "    shape = list(_train.shape)\n",
        "    shape[0] = batch_size\n",
        "    batch_s = np.empty(shape)\n",
        "    batch_labels = np.empty((batch_size,1))\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Loop index\n",
        "        # index = random sample from _unsampled (indices)\n",
        "        index = random.choice(_unsampled)\n",
        "        batch_s[i] = _train[index]\n",
        "        batch_labels[i] = _labels[index]\n",
        "        _unsampled.remove(index)\n",
        "\n",
        "\n",
        "    return batch_s, batch_labels, _unsampled\n",
        "\n",
        "\n",
        "def one_hot(y_):\n",
        "    # One hot encoding of the network outputs\n",
        "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
        "\n",
        "    y_ = y_.reshape(len(y_))\n",
        "    n_values = int(np.max(y_)) + 1\n",
        "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M93XcSfwPPqY"
      },
      "source": [
        "## Build the network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "31e239ca-ecc6-4557-fb6e-38aae24dcd2f",
        "colab_type": "code",
        "id": "D05nnl_OPPqa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "\n",
        "# Graph input/output\n",
        "x = tf.compat.v1.placeholder(tf.float32, [None, n_steps, n_input], name='input')\n",
        "y = tf.compat.v1.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "# Graph weights\n",
        "weights = {\n",
        "    'hidden': tf.Variable(tf.random.normal([n_input, n_hidden])), # Hidden layer weights\n",
        "    'out': tf.Variable(tf.random.normal([n_hidden, n_classes], mean=1.0))\n",
        "}\n",
        "biases = {\n",
        "    'hidden': tf.Variable(tf.random.normal([n_hidden])),\n",
        "    'out': tf.Variable(tf.random.normal([n_classes]))\n",
        "}\n",
        "\n",
        "pred = LSTM_RNN(x, weights, biases)\n",
        "\n",
        "# Loss, optimizer and evaluation\n",
        "l2 = lambda_loss_amount * sum(\n",
        "    tf.nn.l2_loss(tf_var) for tf_var in tf.compat.v1.trainable_variables()\n",
        ") # L2 loss prevents this overkill neural network to overfit the data\n",
        "cost = tf.reduce_mean(input_tensor=tf.nn.softmax_cross_entropy_with_logits(labels=tf.stop_gradient(y), logits=pred)) + l2 # Softmax loss\n",
        "if decaying_learning_rate:\n",
        "    learning_rate = tf.compat.v1.train.exponential_decay(init_learning_rate, global_step*batch_size, decay_steps, decay_rate, staircase=True)\n",
        "\n",
        "\n",
        "#decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps) #exponentially decayed learning rate\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step=global_step) # Adam Optimizer\n",
        "\n",
        "correct_pred = tf.equal(tf.argmax(input=pred,axis=1), tf.argmax(input=y,axis=1), name='prediction')\n",
        "accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_pred, tf.float32), name='accuracy')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-d91a1d900b64>:12: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-7-d91a1d900b64>:14: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-7-d91a1d900b64>:15: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:740: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:744: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jfBvVBk8PPqg"
      },
      "source": [
        "## Train the network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab_type": "code",
        "id": "1rJE_CEePPqi",
        "outputId": "a2e5fc86-cf58-43b8-d975-0cd81b8e103f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_losses = []\n",
        "test_accuracies = []\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "sess = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "# Perform Training steps with \"batch_size\" amount of data at each loop.\n",
        "# Elements of each batch are chosen randomly, without replacement, from X_train,\n",
        "# restarting when remaining datapoints < batch_size\n",
        "step = 1\n",
        "time_start = time.time()\n",
        "unsampled_indices = list(range(0,len(X_train)))\n",
        "\n",
        "while step * batch_size <= training_iters:\n",
        "    #print (sess.run(learning_rate)) #decaying learning rate\n",
        "    #print (sess.run(global_step)) # global number of iterations\n",
        "    if len(unsampled_indices) < batch_size:\n",
        "        unsampled_indices = list(range(0,len(X_train)))\n",
        "    batch_xs, raw_labels, unsampled_indicies = extract_batch_size(X_train, y_train, unsampled_indices, batch_size)\n",
        "    batch_ys = one_hot(raw_labels)\n",
        "    # check that encoded output is same length as num_classes, if not, pad it\n",
        "    if len(batch_ys[0]) < n_classes:\n",
        "        temp_ys = np.zeros((batch_size, n_classes))\n",
        "        temp_ys[:batch_ys.shape[0],:batch_ys.shape[1]] = batch_ys\n",
        "        batch_ys = temp_ys\n",
        "\n",
        "\n",
        "\n",
        "    # Fit training using batch data\n",
        "    _, loss, acc = sess.run(\n",
        "        [optimizer, cost, accuracy],\n",
        "        feed_dict={\n",
        "            x: batch_xs,\n",
        "            y: batch_ys\n",
        "        }\n",
        "    )\n",
        "    train_losses.append(loss)\n",
        "    train_accuracies.append(acc)\n",
        "\n",
        "    # Evaluate network only at some steps for faster training:\n",
        "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
        "\n",
        "        # To not spam console, show training accuracy/loss in this \"if\"\n",
        "        print(\"Iter #\" + str(step*batch_size) + \\\n",
        "              \":  Learning rate = \" + \"{:.6f}\".format(sess.run(learning_rate)) + \\\n",
        "              \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
        "              \", Accuracy = {}\".format(acc))\n",
        "\n",
        "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
        "        loss, acc = sess.run(\n",
        "            [cost, accuracy],\n",
        "            feed_dict={\n",
        "                x: X_test,\n",
        "                y: one_hot(y_test)\n",
        "            }\n",
        "        )\n",
        "        test_losses.append(loss)\n",
        "        test_accuracies.append(acc)\n",
        "        print(\"PERFORMANCE ON TEST SET:             \" + \\\n",
        "              \"Batch Loss = {}\".format(loss) + \\\n",
        "              \", Accuracy = {}\".format(acc))\n",
        "\n",
        "    step += 1\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "\n",
        "# Accuracy for test data\n",
        "\n",
        "one_hot_predictions, accuracy, final_loss = sess.run(\n",
        "    [pred, accuracy, cost],\n",
        "    feed_dict={\n",
        "        x: X_test,\n",
        "        y: one_hot(y_test)\n",
        "    }\n",
        ")\n",
        "\n",
        "test_losses.append(final_loss)\n",
        "test_accuracies.append(accuracy)\n",
        "\n",
        "print(\"FINAL RESULT: \" + \\\n",
        "      \"Batch Loss = {}\".format(final_loss) + \\\n",
        "      \", Accuracy = {}\".format(accuracy))\n",
        "time_stop = time.time()\n",
        "print(\"TOTAL TIME:  {}\".format(time_stop - time_start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1\n",
            "\n",
            "Iter #512:  Learning rate = 0.005000:   Batch Loss = 4.087337, Accuracy = 0.138671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 3.3486485481262207, Accuracy = 0.22187446057796478\n",
            "Iter #4096:  Learning rate = 0.005000:   Batch Loss = 3.025705, Accuracy = 0.201171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.9977312088012695, Accuracy = 0.28169015049934387\n",
            "Iter #8192:  Learning rate = 0.005000:   Batch Loss = 2.884714, Accuracy = 0.267578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.8623485565185547, Accuracy = 0.3248130679130554\n",
            "Iter #12288:  Learning rate = 0.005000:   Batch Loss = 2.853406, Accuracy = 0.279296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.860707998275757, Accuracy = 0.24587027728557587\n",
            "Iter #16384:  Learning rate = 0.005000:   Batch Loss = 2.737589, Accuracy = 0.353515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.7492291927337646, Accuracy = 0.34533125162124634\n",
            "Iter #20480:  Learning rate = 0.005000:   Batch Loss = 2.664719, Accuracy = 0.32421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.7350635528564453, Accuracy = 0.32811686396598816\n",
            "Iter #24576:  Learning rate = 0.005000:   Batch Loss = 2.676649, Accuracy = 0.34765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.5393056869506836, Accuracy = 0.36115458607673645\n",
            "Iter #28672:  Learning rate = 0.005000:   Batch Loss = 2.431275, Accuracy = 0.435546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.4082682132720947, Accuracy = 0.4550513029098511\n",
            "Iter #32768:  Learning rate = 0.005000:   Batch Loss = 2.343899, Accuracy = 0.40625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.2932229042053223, Accuracy = 0.44322726130485535\n",
            "Iter #36864:  Learning rate = 0.005000:   Batch Loss = 2.460073, Accuracy = 0.337890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.3563623428344727, Accuracy = 0.3333333432674408\n",
            "Iter #40960:  Learning rate = 0.005000:   Batch Loss = 2.287176, Accuracy = 0.4140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.3300938606262207, Accuracy = 0.41679707169532776\n",
            "Iter #45056:  Learning rate = 0.005000:   Batch Loss = 2.252211, Accuracy = 0.44140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.197267532348633, Accuracy = 0.4468787908554077\n",
            "Iter #49152:  Learning rate = 0.005000:   Batch Loss = 2.115582, Accuracy = 0.5078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.167170763015747, Accuracy = 0.4712224006652832\n",
            "Iter #53248:  Learning rate = 0.005000:   Batch Loss = 2.538435, Accuracy = 0.32421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.318446397781372, Accuracy = 0.3910624384880066\n",
            "Iter #57344:  Learning rate = 0.005000:   Batch Loss = 2.204161, Accuracy = 0.392578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.2683746814727783, Accuracy = 0.4086245894432068\n",
            "Iter #61440:  Learning rate = 0.005000:   Batch Loss = 2.185011, Accuracy = 0.435546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.0774428844451904, Accuracy = 0.46757087111473083\n",
            "Iter #65536:  Learning rate = 0.005000:   Batch Loss = 2.105891, Accuracy = 0.46875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.1628928184509277, Accuracy = 0.4557468295097351\n",
            "Iter #69632:  Learning rate = 0.005000:   Batch Loss = 2.110766, Accuracy = 0.421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.021183490753174, Accuracy = 0.4574856460094452\n",
            "Iter #73728:  Learning rate = 0.005000:   Batch Loss = 2.040905, Accuracy = 0.443359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.0086307525634766, Accuracy = 0.47487393021583557\n",
            "Iter #77824:  Learning rate = 0.005000:   Batch Loss = 2.130439, Accuracy = 0.41015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.0157008171081543, Accuracy = 0.4771344065666199\n",
            "Iter #81920:  Learning rate = 0.005000:   Batch Loss = 2.007554, Accuracy = 0.484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.97868013381958, Accuracy = 0.4981742203235626\n",
            "Iter #86016:  Learning rate = 0.005000:   Batch Loss = 2.002778, Accuracy = 0.447265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.9450385570526123, Accuracy = 0.4974786937236786\n",
            "Iter #90112:  Learning rate = 0.005000:   Batch Loss = 1.907061, Accuracy = 0.529296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.9249062538146973, Accuracy = 0.5106937885284424\n",
            "Iter #94208:  Learning rate = 0.005000:   Batch Loss = 1.898950, Accuracy = 0.509765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.012575626373291, Accuracy = 0.45661625266075134\n",
            "Iter #98304:  Learning rate = 0.005000:   Batch Loss = 1.950870, Accuracy = 0.466796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.019836902618408, Accuracy = 0.45644235610961914\n",
            "Iter #102400:  Learning rate = 0.004800:   Batch Loss = 2.004881, Accuracy = 0.408203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.9905388355255127, Accuracy = 0.4381846487522125\n",
            "Iter #106496:  Learning rate = 0.004800:   Batch Loss = 1.936002, Accuracy = 0.48046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.8795994520187378, Accuracy = 0.5089549422264099\n",
            "Iter #110592:  Learning rate = 0.004800:   Batch Loss = 1.906651, Accuracy = 0.486328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.902909278869629, Accuracy = 0.4793948829174042\n",
            "Iter #114688:  Learning rate = 0.004800:   Batch Loss = 1.991962, Accuracy = 0.435546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.923850178718567, Accuracy = 0.439923495054245\n",
            "Iter #118784:  Learning rate = 0.004800:   Batch Loss = 1.923127, Accuracy = 0.4921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.897620439529419, Accuracy = 0.5070422291755676\n",
            "Iter #122880:  Learning rate = 0.004800:   Batch Loss = 1.787507, Accuracy = 0.5078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.8307929039001465, Accuracy = 0.5136498212814331\n",
            "Iter #126976:  Learning rate = 0.004800:   Batch Loss = 1.773869, Accuracy = 0.498046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.8469743728637695, Accuracy = 0.515910267829895\n",
            "Iter #131072:  Learning rate = 0.004800:   Batch Loss = 1.764282, Accuracy = 0.525390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.792398452758789, Accuracy = 0.5303425788879395\n",
            "Iter #135168:  Learning rate = 0.004800:   Batch Loss = 1.802895, Accuracy = 0.5234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.8584357500076294, Accuracy = 0.4780038297176361\n",
            "Iter #139264:  Learning rate = 0.004800:   Batch Loss = 1.744475, Accuracy = 0.50390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.8156156539916992, Accuracy = 0.515910267829895\n",
            "Iter #143360:  Learning rate = 0.004800:   Batch Loss = 1.763392, Accuracy = 0.51171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.781409502029419, Accuracy = 0.5383411645889282\n",
            "Iter #147456:  Learning rate = 0.004800:   Batch Loss = 1.893897, Accuracy = 0.5\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.7587835788726807, Accuracy = 0.49017563462257385\n",
            "Iter #151552:  Learning rate = 0.004800:   Batch Loss = 1.781367, Accuracy = 0.4921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.7007339000701904, Accuracy = 0.5313858389854431\n",
            "Iter #155648:  Learning rate = 0.004800:   Batch Loss = 1.850412, Accuracy = 0.455078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.9695314168930054, Accuracy = 0.4684402644634247\n",
            "Iter #159744:  Learning rate = 0.004800:   Batch Loss = 1.865530, Accuracy = 0.412109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.7663795948028564, Accuracy = 0.5129542946815491\n",
            "Iter #163840:  Learning rate = 0.004800:   Batch Loss = 1.835835, Accuracy = 0.494140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.7000634670257568, Accuracy = 0.5317336320877075\n",
            "Iter #167936:  Learning rate = 0.004800:   Batch Loss = 1.755217, Accuracy = 0.5078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.7884764671325684, Accuracy = 0.5016518831253052\n",
            "Iter #172032:  Learning rate = 0.004800:   Batch Loss = 1.744549, Accuracy = 0.51171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6976637840270996, Accuracy = 0.5291253924369812\n",
            "Iter #176128:  Learning rate = 0.004800:   Batch Loss = 1.760127, Accuracy = 0.525390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6772983074188232, Accuracy = 0.5235611200332642\n",
            "Iter #180224:  Learning rate = 0.004800:   Batch Loss = 1.631000, Accuracy = 0.509765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6625704765319824, Accuracy = 0.5256476998329163\n",
            "Iter #184320:  Learning rate = 0.004800:   Batch Loss = 1.692606, Accuracy = 0.552734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6506425142288208, Accuracy = 0.5167796611785889\n",
            "Iter #188416:  Learning rate = 0.004800:   Batch Loss = 1.656358, Accuracy = 0.498046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6757926940917969, Accuracy = 0.5324291586875916\n",
            "Iter #192512:  Learning rate = 0.004800:   Batch Loss = 1.619091, Accuracy = 0.529296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6102949380874634, Accuracy = 0.5430359840393066\n",
            "Iter #196608:  Learning rate = 0.004800:   Batch Loss = 1.629448, Accuracy = 0.529296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6249971389770508, Accuracy = 0.5287775993347168\n",
            "Iter #200704:  Learning rate = 0.004608:   Batch Loss = 1.710476, Accuracy = 0.470703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6334412097930908, Accuracy = 0.516605794429779\n",
            "Iter #204800:  Learning rate = 0.004608:   Batch Loss = 1.654951, Accuracy = 0.490234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6207695007324219, Accuracy = 0.47035297751426697\n",
            "Iter #208896:  Learning rate = 0.004608:   Batch Loss = 1.604361, Accuracy = 0.5\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6068956851959229, Accuracy = 0.5359067916870117\n",
            "Iter #212992:  Learning rate = 0.004608:   Batch Loss = 1.608028, Accuracy = 0.501953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6080081462860107, Accuracy = 0.5359067916870117\n",
            "Iter #217088:  Learning rate = 0.004608:   Batch Loss = 1.748825, Accuracy = 0.390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.7428717613220215, Accuracy = 0.3795861601829529\n",
            "Iter #221184:  Learning rate = 0.004608:   Batch Loss = 1.643065, Accuracy = 0.525390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.622934341430664, Accuracy = 0.5256476998329163\n",
            "Iter #225280:  Learning rate = 0.004608:   Batch Loss = 1.633654, Accuracy = 0.46875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6112419366836548, Accuracy = 0.5167796611785889\n",
            "Iter #229376:  Learning rate = 0.004608:   Batch Loss = 1.764430, Accuracy = 0.484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.35509991645813, Accuracy = 0.46339768171310425\n",
            "Iter #233472:  Learning rate = 0.004608:   Batch Loss = 1.752808, Accuracy = 0.48828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.7465381622314453, Accuracy = 0.48965397477149963\n",
            "Iter #237568:  Learning rate = 0.004608:   Batch Loss = 1.725105, Accuracy = 0.529296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.757779598236084, Accuracy = 0.4993914067745209\n",
            "Iter #241664:  Learning rate = 0.004608:   Batch Loss = 1.719930, Accuracy = 0.486328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.693210244178772, Accuracy = 0.5023474097251892\n",
            "Iter #245760:  Learning rate = 0.004608:   Batch Loss = 1.653510, Accuracy = 0.5234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6921743154525757, Accuracy = 0.5089549422264099\n",
            "Iter #249856:  Learning rate = 0.004608:   Batch Loss = 1.652513, Accuracy = 0.5\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6251602172851562, Accuracy = 0.5315597057342529\n",
            "Iter #253952:  Learning rate = 0.004608:   Batch Loss = 1.690616, Accuracy = 0.478515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6683533191680908, Accuracy = 0.5068683624267578\n",
            "Iter #258048:  Learning rate = 0.004608:   Batch Loss = 1.616831, Accuracy = 0.498046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6061246395111084, Accuracy = 0.48826292157173157\n",
            "Iter #262144:  Learning rate = 0.004608:   Batch Loss = 1.577560, Accuracy = 0.521484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.573204755783081, Accuracy = 0.538515031337738\n",
            "Iter #266240:  Learning rate = 0.004608:   Batch Loss = 1.572206, Accuracy = 0.51171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6111574172973633, Accuracy = 0.528603732585907\n",
            "Iter #270336:  Learning rate = 0.004608:   Batch Loss = 1.557904, Accuracy = 0.529296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.5484633445739746, Accuracy = 0.5327768921852112\n",
            "Iter #274432:  Learning rate = 0.004608:   Batch Loss = 1.564620, Accuracy = 0.486328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.53975510597229, Accuracy = 0.5503390431404114\n",
            "Iter #278528:  Learning rate = 0.004608:   Batch Loss = 1.531615, Accuracy = 0.55078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.5326430797576904, Accuracy = 0.5418187975883484\n",
            "Iter #282624:  Learning rate = 0.004608:   Batch Loss = 1.630852, Accuracy = 0.494140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.582273006439209, Accuracy = 0.5160841345787048\n",
            "Iter #286720:  Learning rate = 0.004608:   Batch Loss = 1.583341, Accuracy = 0.5078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.5732474327087402, Accuracy = 0.5412971377372742\n",
            "Iter #290816:  Learning rate = 0.004608:   Batch Loss = 1.556943, Accuracy = 0.48828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.532949686050415, Accuracy = 0.5414710640907288\n",
            "Iter #294912:  Learning rate = 0.004608:   Batch Loss = 1.547394, Accuracy = 0.53125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.5377144813537598, Accuracy = 0.5329508185386658\n",
            "Iter #299008:  Learning rate = 0.004608:   Batch Loss = 1.538982, Accuracy = 0.521484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.5647516250610352, Accuracy = 0.5199095606803894\n",
            "Iter #303104:  Learning rate = 0.004424:   Batch Loss = 1.572342, Accuracy = 0.509765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.5112981796264648, Accuracy = 0.5388628244400024\n",
            "Iter #307200:  Learning rate = 0.004424:   Batch Loss = 1.569454, Accuracy = 0.4921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4973825216293335, Accuracy = 0.5593809485435486\n",
            "Iter #311296:  Learning rate = 0.004424:   Batch Loss = 1.503638, Accuracy = 0.533203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.5153391361236572, Accuracy = 0.5381672978401184\n",
            "Iter #315392:  Learning rate = 0.004424:   Batch Loss = 1.544372, Accuracy = 0.5234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.5037736892700195, Accuracy = 0.5395583510398865\n",
            "Iter #319488:  Learning rate = 0.004424:   Batch Loss = 1.523933, Accuracy = 0.515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4982293844223022, Accuracy = 0.5452964901924133\n",
            "Iter #323584:  Learning rate = 0.004424:   Batch Loss = 1.487659, Accuracy = 0.53125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.483463168144226, Accuracy = 0.5432098507881165\n",
            "Iter #327680:  Learning rate = 0.004424:   Batch Loss = 1.493635, Accuracy = 0.525390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4865429401397705, Accuracy = 0.5534689426422119\n",
            "Iter #331776:  Learning rate = 0.004424:   Batch Loss = 1.525452, Accuracy = 0.546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.512965440750122, Accuracy = 0.5270387530326843\n",
            "Iter #335872:  Learning rate = 0.004424:   Batch Loss = 1.462135, Accuracy = 0.546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.497992992401123, Accuracy = 0.5400800108909607\n",
            "Iter #339968:  Learning rate = 0.004424:   Batch Loss = 1.467739, Accuracy = 0.521484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4750990867614746, Accuracy = 0.5496435165405273\n",
            "Iter #344064:  Learning rate = 0.004424:   Batch Loss = 1.451100, Accuracy = 0.5546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4833635091781616, Accuracy = 0.5406016111373901\n",
            "Iter #348160:  Learning rate = 0.004424:   Batch Loss = 1.560004, Accuracy = 0.48828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.5915848016738892, Accuracy = 0.5343418717384338\n",
            "Iter #352256:  Learning rate = 0.004424:   Batch Loss = 1.458746, Accuracy = 0.55859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4719040393829346, Accuracy = 0.5437315106391907\n",
            "Iter #356352:  Learning rate = 0.004424:   Batch Loss = 1.489068, Accuracy = 0.5546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.490874171257019, Accuracy = 0.5294731259346008\n",
            "Iter #360448:  Learning rate = 0.004424:   Batch Loss = 1.509246, Accuracy = 0.51953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4603116512298584, Accuracy = 0.55120849609375\n",
            "Iter #364544:  Learning rate = 0.004424:   Batch Loss = 1.468253, Accuracy = 0.548828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.468274712562561, Accuracy = 0.5409494042396545\n",
            "Iter #368640:  Learning rate = 0.004424:   Batch Loss = 1.403906, Accuracy = 0.60546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.442604422569275, Accuracy = 0.5576421618461609\n",
            "Iter #372736:  Learning rate = 0.004424:   Batch Loss = 1.489137, Accuracy = 0.556640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4760277271270752, Accuracy = 0.5496435165405273\n",
            "Iter #376832:  Learning rate = 0.004424:   Batch Loss = 1.496679, Accuracy = 0.537109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.5115971565246582, Accuracy = 0.5447748303413391\n",
            "Iter #380928:  Learning rate = 0.004424:   Batch Loss = 1.464856, Accuracy = 0.5703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4656965732574463, Accuracy = 0.5306903123855591\n",
            "Iter #385024:  Learning rate = 0.004424:   Batch Loss = 1.503297, Accuracy = 0.50390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4759299755096436, Accuracy = 0.5310381054878235\n",
            "Iter #389120:  Learning rate = 0.004424:   Batch Loss = 1.494044, Accuracy = 0.490234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.5068695545196533, Accuracy = 0.5364284515380859\n",
            "Iter #393216:  Learning rate = 0.004424:   Batch Loss = 1.511967, Accuracy = 0.52734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.437659740447998, Accuracy = 0.5546861290931702\n",
            "Iter #397312:  Learning rate = 0.004424:   Batch Loss = 1.479812, Accuracy = 0.529296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4533748626708984, Accuracy = 0.5360806584358215\n",
            "Iter #401408:  Learning rate = 0.004247:   Batch Loss = 1.445121, Accuracy = 0.564453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.462369680404663, Accuracy = 0.5489479899406433\n",
            "Iter #405504:  Learning rate = 0.004247:   Batch Loss = 1.455685, Accuracy = 0.541015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4289274215698242, Accuracy = 0.5461658835411072\n",
            "Iter #409600:  Learning rate = 0.004247:   Batch Loss = 1.432281, Accuracy = 0.583984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4420719146728516, Accuracy = 0.5508607029914856\n",
            "Iter #413696:  Learning rate = 0.004247:   Batch Loss = 1.453504, Accuracy = 0.552734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4177403450012207, Accuracy = 0.5685967803001404\n",
            "Iter #417792:  Learning rate = 0.004247:   Batch Loss = 1.393872, Accuracy = 0.5546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4250339269638062, Accuracy = 0.5446009635925293\n",
            "Iter #421888:  Learning rate = 0.004247:   Batch Loss = 1.436399, Accuracy = 0.5625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4001482725143433, Accuracy = 0.5600765347480774\n",
            "Iter #425984:  Learning rate = 0.004247:   Batch Loss = 1.391122, Accuracy = 0.59375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4428060054779053, Accuracy = 0.5559033155441284\n",
            "Iter #430080:  Learning rate = 0.004247:   Batch Loss = 1.463137, Accuracy = 0.50390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.8950586318969727, Accuracy = 0.44600939750671387\n",
            "Iter #434176:  Learning rate = 0.004247:   Batch Loss = 1.450970, Accuracy = 0.505859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4723106622695923, Accuracy = 0.5376456379890442\n",
            "Iter #438272:  Learning rate = 0.004247:   Batch Loss = 1.492243, Accuracy = 0.482421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.5013649463653564, Accuracy = 0.5336463451385498\n",
            "Iter #442368:  Learning rate = 0.004247:   Batch Loss = 1.431548, Accuracy = 0.513671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4499280452728271, Accuracy = 0.5339940786361694\n",
            "Iter #446464:  Learning rate = 0.004247:   Batch Loss = 1.387270, Accuracy = 0.552734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.451134443283081, Accuracy = 0.5348635315895081\n",
            "Iter #450560:  Learning rate = 0.004247:   Batch Loss = 1.400290, Accuracy = 0.552734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4289121627807617, Accuracy = 0.5402538776397705\n",
            "Iter #454656:  Learning rate = 0.004247:   Batch Loss = 1.473342, Accuracy = 0.486328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4861929416656494, Accuracy = 0.5051295161247253\n",
            "Iter #458752:  Learning rate = 0.004247:   Batch Loss = 1.432986, Accuracy = 0.537109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4410542249679565, Accuracy = 0.5353851318359375\n",
            "Iter #462848:  Learning rate = 0.004247:   Batch Loss = 1.431747, Accuracy = 0.58203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4135184288024902, Accuracy = 0.5579898953437805\n",
            "Iter #466944:  Learning rate = 0.004247:   Batch Loss = 1.421124, Accuracy = 0.5625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.5919687747955322, Accuracy = 0.5141714215278625\n",
            "Iter #471040:  Learning rate = 0.004247:   Batch Loss = 1.399697, Accuracy = 0.578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.46770179271698, Accuracy = 0.5515562295913696\n",
            "Iter #475136:  Learning rate = 0.004247:   Batch Loss = 1.436870, Accuracy = 0.541015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3974932432174683, Accuracy = 0.542688250541687\n",
            "Iter #479232:  Learning rate = 0.004247:   Batch Loss = 1.391922, Accuracy = 0.521484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4235306978225708, Accuracy = 0.5383411645889282\n",
            "Iter #483328:  Learning rate = 0.004247:   Batch Loss = 1.362595, Accuracy = 0.515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3876293897628784, Accuracy = 0.5571205019950867\n",
            "Iter #487424:  Learning rate = 0.004247:   Batch Loss = 1.370899, Accuracy = 0.546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.395888090133667, Accuracy = 0.5525995492935181\n",
            "Iter #491520:  Learning rate = 0.004247:   Batch Loss = 1.404714, Accuracy = 0.53515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.383385181427002, Accuracy = 0.5567727088928223\n",
            "Iter #495616:  Learning rate = 0.004247:   Batch Loss = 1.400074, Accuracy = 0.51953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3785064220428467, Accuracy = 0.5491219162940979\n",
            "Iter #499712:  Learning rate = 0.004247:   Batch Loss = 1.378607, Accuracy = 0.548828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3684663772583008, Accuracy = 0.5559033155441284\n",
            "Iter #503808:  Learning rate = 0.004077:   Batch Loss = 1.370577, Accuracy = 0.560546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3786380290985107, Accuracy = 0.5503390431404114\n",
            "Iter #507904:  Learning rate = 0.004077:   Batch Loss = 1.372880, Accuracy = 0.53515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.380324125289917, Accuracy = 0.5569466352462769\n",
            "Iter #512000:  Learning rate = 0.004077:   Batch Loss = 1.401585, Accuracy = 0.51953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3762664794921875, Accuracy = 0.5486002564430237\n",
            "Iter #516096:  Learning rate = 0.004077:   Batch Loss = 1.350775, Accuracy = 0.5390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3546030521392822, Accuracy = 0.5705094933509827\n",
            "Iter #520192:  Learning rate = 0.004077:   Batch Loss = 1.367164, Accuracy = 0.55078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3750078678131104, Accuracy = 0.5211267471313477\n",
            "Iter #524288:  Learning rate = 0.004077:   Batch Loss = 1.381300, Accuracy = 0.52734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3453365564346313, Accuracy = 0.5781603455543518\n",
            "Iter #528384:  Learning rate = 0.004077:   Batch Loss = 1.384131, Accuracy = 0.541015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3651007413864136, Accuracy = 0.5437315106391907\n",
            "Iter #532480:  Learning rate = 0.004077:   Batch Loss = 1.356381, Accuracy = 0.564453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3626937866210938, Accuracy = 0.5649452209472656\n",
            "Iter #536576:  Learning rate = 0.004077:   Batch Loss = 1.390670, Accuracy = 0.548828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4056819677352905, Accuracy = 0.5430359840393066\n",
            "Iter #540672:  Learning rate = 0.004077:   Batch Loss = 1.422591, Accuracy = 0.544921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3663874864578247, Accuracy = 0.5449486970901489\n",
            "Iter #544768:  Learning rate = 0.004077:   Batch Loss = 1.366834, Accuracy = 0.521484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4112063646316528, Accuracy = 0.5357329249382019\n",
            "Iter #548864:  Learning rate = 0.004077:   Batch Loss = 1.395144, Accuracy = 0.498046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3912639617919922, Accuracy = 0.5524256825447083\n",
            "Iter #552960:  Learning rate = 0.004077:   Batch Loss = 1.429050, Accuracy = 0.53125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.38936185836792, Accuracy = 0.546339750289917\n",
            "Iter #557056:  Learning rate = 0.004077:   Batch Loss = 1.401053, Accuracy = 0.529296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.448517084121704, Accuracy = 0.5628586411476135\n",
            "Iter #561152:  Learning rate = 0.004077:   Batch Loss = 1.400644, Accuracy = 0.599609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3842298984527588, Accuracy = 0.5612936615943909\n",
            "Iter #565248:  Learning rate = 0.004077:   Batch Loss = 1.409848, Accuracy = 0.56640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3812999725341797, Accuracy = 0.5578160285949707\n",
            "Iter #569344:  Learning rate = 0.004077:   Batch Loss = 1.353464, Accuracy = 0.5859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3824506998062134, Accuracy = 0.5628586411476135\n",
            "Iter #573440:  Learning rate = 0.004077:   Batch Loss = 1.340809, Accuracy = 0.59765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3788164854049683, Accuracy = 0.5532950758934021\n",
            "Iter #577536:  Learning rate = 0.004077:   Batch Loss = 1.380126, Accuracy = 0.57421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3474441766738892, Accuracy = 0.5614675879478455\n",
            "Iter #581632:  Learning rate = 0.004077:   Batch Loss = 1.342178, Accuracy = 0.58984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.376760482788086, Accuracy = 0.5595548748970032\n",
            "Iter #585728:  Learning rate = 0.004077:   Batch Loss = 1.388716, Accuracy = 0.5625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3353023529052734, Accuracy = 0.5811163187026978\n",
            "Iter #589824:  Learning rate = 0.004077:   Batch Loss = 1.340194, Accuracy = 0.599609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3937193155288696, Accuracy = 0.5525995492935181\n",
            "Iter #593920:  Learning rate = 0.004077:   Batch Loss = 1.410242, Accuracy = 0.5546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3258644342422485, Accuracy = 0.5821596384048462\n",
            "Iter #598016:  Learning rate = 0.004077:   Batch Loss = 1.494456, Accuracy = 0.548828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4682714939117432, Accuracy = 0.5459920167922974\n",
            "Iter #602112:  Learning rate = 0.003914:   Batch Loss = 1.346023, Accuracy = 0.60546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3831822872161865, Accuracy = 0.5532950758934021\n",
            "Iter #606208:  Learning rate = 0.003914:   Batch Loss = 1.322922, Accuracy = 0.58203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.35719633102417, Accuracy = 0.5639019012451172\n",
            "Iter #610304:  Learning rate = 0.003914:   Batch Loss = 1.333024, Accuracy = 0.560546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3422825336456299, Accuracy = 0.5468614101409912\n",
            "Iter #614400:  Learning rate = 0.003914:   Batch Loss = 1.324375, Accuracy = 0.5546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3312654495239258, Accuracy = 0.5459920167922974\n",
            "Iter #618496:  Learning rate = 0.003914:   Batch Loss = 1.355076, Accuracy = 0.548828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3546109199523926, Accuracy = 0.5379933714866638\n",
            "Iter #622592:  Learning rate = 0.003914:   Batch Loss = 1.387550, Accuracy = 0.50390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3510096073150635, Accuracy = 0.541992723941803\n",
            "Iter #626688:  Learning rate = 0.003914:   Batch Loss = 1.316890, Accuracy = 0.5625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3302212953567505, Accuracy = 0.5486002564430237\n",
            "Iter #630784:  Learning rate = 0.003914:   Batch Loss = 1.316065, Accuracy = 0.6015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.313993215560913, Accuracy = 0.5578160285949707\n",
            "Iter #634880:  Learning rate = 0.003914:   Batch Loss = 1.347441, Accuracy = 0.5625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.339044213294983, Accuracy = 0.53712397813797\n",
            "Iter #638976:  Learning rate = 0.003914:   Batch Loss = 1.310910, Accuracy = 0.580078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3549877405166626, Accuracy = 0.5494696497917175\n",
            "Iter #643072:  Learning rate = 0.003914:   Batch Loss = 1.359842, Accuracy = 0.544921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.362635850906372, Accuracy = 0.537819504737854\n",
            "Iter #647168:  Learning rate = 0.003914:   Batch Loss = 1.294705, Accuracy = 0.5546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.34519362449646, Accuracy = 0.5517301559448242\n",
            "Iter #651264:  Learning rate = 0.003914:   Batch Loss = 1.317916, Accuracy = 0.568359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.369106411933899, Accuracy = 0.560424268245697\n",
            "Iter #655360:  Learning rate = 0.003914:   Batch Loss = 1.367448, Accuracy = 0.537109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3290646076202393, Accuracy = 0.5416449308395386\n",
            "Iter #659456:  Learning rate = 0.003914:   Batch Loss = 1.338765, Accuracy = 0.5625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3129979372024536, Accuracy = 0.5661624073982239\n",
            "Iter #663552:  Learning rate = 0.003914:   Batch Loss = 1.285512, Accuracy = 0.587890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.31795072555542, Accuracy = 0.5489479899406433\n",
            "Iter #667648:  Learning rate = 0.003914:   Batch Loss = 1.226741, Accuracy = 0.6171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3205896615982056, Accuracy = 0.559728741645813\n",
            "Iter #671744:  Learning rate = 0.003914:   Batch Loss = 1.305575, Accuracy = 0.580078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3621087074279785, Accuracy = 0.5632063746452332\n",
            "Iter #675840:  Learning rate = 0.003914:   Batch Loss = 1.262499, Accuracy = 0.62109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3196070194244385, Accuracy = 0.5719005465507507\n",
            "Iter #679936:  Learning rate = 0.003914:   Batch Loss = 1.325432, Accuracy = 0.619140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2910099029541016, Accuracy = 0.5998956561088562\n",
            "Iter #684032:  Learning rate = 0.003914:   Batch Loss = 1.287444, Accuracy = 0.619140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2809500694274902, Accuracy = 0.6228482127189636\n",
            "Iter #688128:  Learning rate = 0.003914:   Batch Loss = 1.276956, Accuracy = 0.6171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3012769222259521, Accuracy = 0.6061554551124573\n",
            "Iter #692224:  Learning rate = 0.003914:   Batch Loss = 1.249759, Accuracy = 0.62109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.301617980003357, Accuracy = 0.6016345024108887\n",
            "Iter #696320:  Learning rate = 0.003914:   Batch Loss = 1.268286, Accuracy = 0.634765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2595808506011963, Accuracy = 0.6038949489593506\n",
            "Iter #700416:  Learning rate = 0.003757:   Batch Loss = 1.259595, Accuracy = 0.634765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2744486331939697, Accuracy = 0.609111487865448\n",
            "Iter #704512:  Learning rate = 0.003757:   Batch Loss = 1.188803, Accuracy = 0.64453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2746633291244507, Accuracy = 0.6030255556106567\n",
            "Iter #708608:  Learning rate = 0.003757:   Batch Loss = 1.251866, Accuracy = 0.611328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.249142050743103, Accuracy = 0.6103286147117615\n",
            "Iter #712704:  Learning rate = 0.003757:   Batch Loss = 1.270228, Accuracy = 0.62890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2643547058105469, Accuracy = 0.6030255556106567\n",
            "Iter #716800:  Learning rate = 0.003757:   Batch Loss = 1.225364, Accuracy = 0.634765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2675305604934692, Accuracy = 0.6028516888618469\n",
            "Iter #720896:  Learning rate = 0.003757:   Batch Loss = 1.179966, Accuracy = 0.654296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2768292427062988, Accuracy = 0.6080681681632996\n",
            "Iter #724992:  Learning rate = 0.003757:   Batch Loss = 1.265967, Accuracy = 0.580078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3150038719177246, Accuracy = 0.5520778894424438\n",
            "Iter #729088:  Learning rate = 0.003757:   Batch Loss = 1.338949, Accuracy = 0.525390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3028825521469116, Accuracy = 0.5579898953437805\n",
            "Iter #733184:  Learning rate = 0.003757:   Batch Loss = 1.258467, Accuracy = 0.6015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2780879735946655, Accuracy = 0.5971135497093201\n",
            "Iter #737280:  Learning rate = 0.003757:   Batch Loss = 1.237252, Accuracy = 0.607421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.292335033416748, Accuracy = 0.599373996257782\n",
            "Iter #741376:  Learning rate = 0.003757:   Batch Loss = 1.287755, Accuracy = 0.609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2916290760040283, Accuracy = 0.5849417448043823\n",
            "Iter #745472:  Learning rate = 0.003757:   Batch Loss = 1.299389, Accuracy = 0.580078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.261127233505249, Accuracy = 0.603547215461731\n",
            "Iter #749568:  Learning rate = 0.003757:   Batch Loss = 1.235994, Accuracy = 0.63671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.239885926246643, Accuracy = 0.6117197275161743\n",
            "Iter #753664:  Learning rate = 0.003757:   Batch Loss = 1.231341, Accuracy = 0.634765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2472538948059082, Accuracy = 0.6066771149635315\n",
            "Iter #757760:  Learning rate = 0.003757:   Batch Loss = 1.255547, Accuracy = 0.609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.27137291431427, Accuracy = 0.5945053100585938\n",
            "Iter #761856:  Learning rate = 0.003757:   Batch Loss = 1.220146, Accuracy = 0.669921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2500827312469482, Accuracy = 0.6106764078140259\n",
            "Iter #765952:  Learning rate = 0.003757:   Batch Loss = 1.228239, Accuracy = 0.603515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2448956966400146, Accuracy = 0.560424268245697\n",
            "Iter #770048:  Learning rate = 0.003757:   Batch Loss = 1.149192, Accuracy = 0.67578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2612062692642212, Accuracy = 0.6115458011627197\n",
            "Iter #774144:  Learning rate = 0.003757:   Batch Loss = 1.172623, Accuracy = 0.662109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2361128330230713, Accuracy = 0.6205877065658569\n",
            "Iter #778240:  Learning rate = 0.003757:   Batch Loss = 1.239103, Accuracy = 0.58203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2557787895202637, Accuracy = 0.6063293218612671\n",
            "Iter #782336:  Learning rate = 0.003757:   Batch Loss = 1.266368, Accuracy = 0.59765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2571048736572266, Accuracy = 0.5835506916046143\n",
            "Iter #786432:  Learning rate = 0.003757:   Batch Loss = 1.240864, Accuracy = 0.595703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2958850860595703, Accuracy = 0.58302903175354\n",
            "Iter #790528:  Learning rate = 0.003757:   Batch Loss = 1.261715, Accuracy = 0.611328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2348697185516357, Accuracy = 0.6148495674133301\n",
            "Iter #794624:  Learning rate = 0.003757:   Batch Loss = 1.275913, Accuracy = 0.638671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2781221866607666, Accuracy = 0.5849417448043823\n",
            "Iter #798720:  Learning rate = 0.003757:   Batch Loss = 1.265389, Accuracy = 0.58984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2594101428985596, Accuracy = 0.6080681681632996\n",
            "Iter #802816:  Learning rate = 0.003607:   Batch Loss = 1.169205, Accuracy = 0.658203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2515912055969238, Accuracy = 0.5992001295089722\n",
            "Iter #806912:  Learning rate = 0.003607:   Batch Loss = 1.212500, Accuracy = 0.64453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2441072463989258, Accuracy = 0.6195444464683533\n",
            "Iter #811008:  Learning rate = 0.003607:   Batch Loss = 1.239952, Accuracy = 0.630859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2133967876434326, Accuracy = 0.6197183132171631\n",
            "Iter #815104:  Learning rate = 0.003607:   Batch Loss = 1.174018, Accuracy = 0.625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.20888352394104, Accuracy = 0.6181533932685852\n",
            "Iter #819200:  Learning rate = 0.003607:   Batch Loss = 1.178485, Accuracy = 0.634765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2285984754562378, Accuracy = 0.6195444464683533\n",
            "Iter #823296:  Learning rate = 0.003607:   Batch Loss = 1.190219, Accuracy = 0.625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2397775650024414, Accuracy = 0.613458514213562\n",
            "Iter #827392:  Learning rate = 0.003607:   Batch Loss = 1.164978, Accuracy = 0.6484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.234696865081787, Accuracy = 0.6056337952613831\n",
            "Iter #831488:  Learning rate = 0.003607:   Batch Loss = 1.198275, Accuracy = 0.625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.233893871307373, Accuracy = 0.6080681681632996\n",
            "Iter #835584:  Learning rate = 0.003607:   Batch Loss = 1.156124, Accuracy = 0.64453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2431836128234863, Accuracy = 0.6073726415634155\n",
            "Iter #839680:  Learning rate = 0.003607:   Batch Loss = 1.205798, Accuracy = 0.61328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.26948881149292, Accuracy = 0.5873761177062988\n",
            "Iter #843776:  Learning rate = 0.003607:   Batch Loss = 1.220394, Accuracy = 0.6328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2275346517562866, Accuracy = 0.6277168989181519\n",
            "Iter #847872:  Learning rate = 0.003607:   Batch Loss = 1.215069, Accuracy = 0.650390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3342832326889038, Accuracy = 0.5851156115531921\n",
            "Iter #851968:  Learning rate = 0.003607:   Batch Loss = 1.242121, Accuracy = 0.6171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2514665126800537, Accuracy = 0.5974612832069397\n",
            "Iter #856064:  Learning rate = 0.003607:   Batch Loss = 1.242246, Accuracy = 0.6015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2831590175628662, Accuracy = 0.6011128425598145\n",
            "Iter #860160:  Learning rate = 0.003607:   Batch Loss = 1.244405, Accuracy = 0.626953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2348945140838623, Accuracy = 0.613458514213562\n",
            "Iter #864256:  Learning rate = 0.003607:   Batch Loss = 1.194619, Accuracy = 0.640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2539246082305908, Accuracy = 0.6181533932685852\n",
            "Iter #868352:  Learning rate = 0.003607:   Batch Loss = 1.210182, Accuracy = 0.609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2043118476867676, Accuracy = 0.6155450940132141\n",
            "Iter #872448:  Learning rate = 0.003607:   Batch Loss = 1.190597, Accuracy = 0.654296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2136800289154053, Accuracy = 0.6211093664169312\n",
            "Iter #876544:  Learning rate = 0.003607:   Batch Loss = 1.228517, Accuracy = 0.59765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2435919046401978, Accuracy = 0.6012867093086243\n",
            "Iter #880640:  Learning rate = 0.003607:   Batch Loss = 1.211393, Accuracy = 0.642578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.244264841079712, Accuracy = 0.6185011267662048\n",
            "Iter #884736:  Learning rate = 0.003607:   Batch Loss = 1.115366, Accuracy = 0.6796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2358702421188354, Accuracy = 0.5998956561088562\n",
            "Iter #888832:  Learning rate = 0.003607:   Batch Loss = 1.149780, Accuracy = 0.697265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1892554759979248, Accuracy = 0.6204138398170471\n",
            "Iter #892928:  Learning rate = 0.003607:   Batch Loss = 1.167688, Accuracy = 0.619140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.221054196357727, Accuracy = 0.6164145469665527\n",
            "Iter #897024:  Learning rate = 0.003607:   Batch Loss = 1.089077, Accuracy = 0.662109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2050044536590576, Accuracy = 0.6315423250198364\n",
            "Iter #901120:  Learning rate = 0.003463:   Batch Loss = 1.184570, Accuracy = 0.625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1994609832763672, Accuracy = 0.621283233165741\n",
            "Iter #905216:  Learning rate = 0.003463:   Batch Loss = 1.180905, Accuracy = 0.630859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1980056762695312, Accuracy = 0.6117197275161743\n",
            "Iter #909312:  Learning rate = 0.003463:   Batch Loss = 1.140749, Accuracy = 0.662109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1917099952697754, Accuracy = 0.6310206651687622\n",
            "Iter #913408:  Learning rate = 0.003463:   Batch Loss = 1.082699, Accuracy = 0.677734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1854368448257446, Accuracy = 0.6188489198684692\n",
            "Iter #917504:  Learning rate = 0.003463:   Batch Loss = 1.162051, Accuracy = 0.662109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.234769582748413, Accuracy = 0.5955486297607422\n",
            "Iter #921600:  Learning rate = 0.003463:   Batch Loss = 1.172179, Accuracy = 0.658203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2070472240447998, Accuracy = 0.6291079521179199\n",
            "Iter #925696:  Learning rate = 0.003463:   Batch Loss = 1.196377, Accuracy = 0.623046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2281696796417236, Accuracy = 0.6207616329193115\n",
            "Iter #929792:  Learning rate = 0.003463:   Batch Loss = 1.126819, Accuracy = 0.662109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.206768274307251, Accuracy = 0.6151973605155945\n",
            "Iter #933888:  Learning rate = 0.003463:   Batch Loss = 1.148181, Accuracy = 0.623046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1575937271118164, Accuracy = 0.6252825856208801\n",
            "Iter #937984:  Learning rate = 0.003463:   Batch Loss = 1.058213, Accuracy = 0.732421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1821383237838745, Accuracy = 0.6191966533660889\n",
            "Iter #942080:  Learning rate = 0.003463:   Batch Loss = 1.160535, Accuracy = 0.630859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1766201257705688, Accuracy = 0.635367751121521\n",
            "Iter #946176:  Learning rate = 0.003463:   Batch Loss = 1.081297, Accuracy = 0.673828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1819586753845215, Accuracy = 0.6291079521179199\n",
            "Iter #950272:  Learning rate = 0.003463:   Batch Loss = 1.096825, Accuracy = 0.671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.156249761581421, Accuracy = 0.6510171890258789\n",
            "Iter #954368:  Learning rate = 0.003463:   Batch Loss = 1.110001, Accuracy = 0.685546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1743571758270264, Accuracy = 0.6311945915222168\n",
            "Iter #958464:  Learning rate = 0.003463:   Batch Loss = 1.071084, Accuracy = 0.720703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1788063049316406, Accuracy = 0.6440619230270386\n",
            "Iter #962560:  Learning rate = 0.003463:   Batch Loss = 1.081555, Accuracy = 0.70703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1937175989151, Accuracy = 0.6529299020767212\n",
            "Iter #966656:  Learning rate = 0.003463:   Batch Loss = 1.140538, Accuracy = 0.609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1618869304656982, Accuracy = 0.6400625705718994\n",
            "Iter #970752:  Learning rate = 0.003463:   Batch Loss = 1.310722, Accuracy = 0.578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 2.766246795654297, Accuracy = 0.373152494430542\n",
            "Iter #974848:  Learning rate = 0.003463:   Batch Loss = 1.728034, Accuracy = 0.40234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.6700387001037598, Accuracy = 0.43453311920166016\n",
            "Iter #978944:  Learning rate = 0.003463:   Batch Loss = 1.528607, Accuracy = 0.546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.5533982515335083, Accuracy = 0.4999130666255951\n",
            "Iter #983040:  Learning rate = 0.003463:   Batch Loss = 1.458944, Accuracy = 0.5546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4893181324005127, Accuracy = 0.5503390431404114\n",
            "Iter #987136:  Learning rate = 0.003463:   Batch Loss = 1.416381, Accuracy = 0.5859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.442888617515564, Accuracy = 0.5423404574394226\n",
            "Iter #991232:  Learning rate = 0.003463:   Batch Loss = 1.375093, Accuracy = 0.572265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4110339879989624, Accuracy = 0.5710311532020569\n",
            "Iter #995328:  Learning rate = 0.003463:   Batch Loss = 1.332444, Accuracy = 0.587890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4036753177642822, Accuracy = 0.5616414546966553\n",
            "Iter #999424:  Learning rate = 0.003463:   Batch Loss = 1.357112, Accuracy = 0.57421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.430597186088562, Accuracy = 0.5649452209472656\n",
            "Iter #1003520:  Learning rate = 0.003324:   Batch Loss = 1.232553, Accuracy = 0.630859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3580666780471802, Accuracy = 0.5826812982559204\n",
            "Iter #1007616:  Learning rate = 0.003324:   Batch Loss = 1.216156, Accuracy = 0.611328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.365759015083313, Accuracy = 0.5762476325035095\n",
            "Iter #1011712:  Learning rate = 0.003324:   Batch Loss = 1.253623, Accuracy = 0.587890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3225748538970947, Accuracy = 0.5896365642547607\n",
            "Iter #1015808:  Learning rate = 0.003324:   Batch Loss = 1.230661, Accuracy = 0.6171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3051844835281372, Accuracy = 0.5891149640083313\n",
            "Iter #1019904:  Learning rate = 0.003324:   Batch Loss = 1.212646, Accuracy = 0.607421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3108477592468262, Accuracy = 0.5845940113067627\n",
            "Iter #1024000:  Learning rate = 0.003324:   Batch Loss = 1.265899, Accuracy = 0.607421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4279754161834717, Accuracy = 0.5732915997505188\n",
            "Iter #1028096:  Learning rate = 0.003324:   Batch Loss = 1.295392, Accuracy = 0.60546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.4342896938323975, Accuracy = 0.5651190876960754\n",
            "Iter #1032192:  Learning rate = 0.003324:   Batch Loss = 1.338895, Accuracy = 0.59375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3802485466003418, Accuracy = 0.5734654664993286\n",
            "Iter #1036288:  Learning rate = 0.003324:   Batch Loss = 1.299638, Accuracy = 0.576171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3150266408920288, Accuracy = 0.5988523960113525\n",
            "Iter #1040384:  Learning rate = 0.003324:   Batch Loss = 1.252452, Accuracy = 0.6328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.354797124862671, Accuracy = 0.5802469253540039\n",
            "Iter #1044480:  Learning rate = 0.003324:   Batch Loss = 1.253627, Accuracy = 0.626953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3068355321884155, Accuracy = 0.581637978553772\n",
            "Iter #1048576:  Learning rate = 0.003324:   Batch Loss = 1.194725, Accuracy = 0.650390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2605822086334229, Accuracy = 0.6094592213630676\n",
            "Iter #1052672:  Learning rate = 0.003324:   Batch Loss = 1.290447, Accuracy = 0.576171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.3017070293426514, Accuracy = 0.6033733487129211\n",
            "Iter #1056768:  Learning rate = 0.003324:   Batch Loss = 1.186543, Accuracy = 0.634765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2370473146438599, Accuracy = 0.6138063073158264\n",
            "Iter #1060864:  Learning rate = 0.003324:   Batch Loss = 1.170625, Accuracy = 0.64453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.208662509918213, Accuracy = 0.6122413277626038\n",
            "Iter #1064960:  Learning rate = 0.003324:   Batch Loss = 1.148734, Accuracy = 0.638671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1948293447494507, Accuracy = 0.6376282572746277\n",
            "Iter #1069056:  Learning rate = 0.003324:   Batch Loss = 1.131974, Accuracy = 0.634765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.215591311454773, Accuracy = 0.6233698725700378\n",
            "Iter #1073152:  Learning rate = 0.003324:   Batch Loss = 1.192479, Accuracy = 0.626953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2336483001708984, Accuracy = 0.6171100735664368\n",
            "Iter #1077248:  Learning rate = 0.003324:   Batch Loss = 1.185959, Accuracy = 0.642578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.194156527519226, Accuracy = 0.6240653991699219\n",
            "Iter #1081344:  Learning rate = 0.003324:   Batch Loss = 1.117323, Accuracy = 0.669921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.2195255756378174, Accuracy = 0.6056337952613831\n",
            "Iter #1085440:  Learning rate = 0.003324:   Batch Loss = 1.152538, Accuracy = 0.623046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1782153844833374, Accuracy = 0.6211093664169312\n",
            "Iter #1089536:  Learning rate = 0.003324:   Batch Loss = 1.164685, Accuracy = 0.65625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.183037281036377, Accuracy = 0.6151973605155945\n",
            "Iter #1093632:  Learning rate = 0.003324:   Batch Loss = 1.086958, Accuracy = 0.6875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1785142421722412, Accuracy = 0.6357155442237854\n",
            "Iter #1097728:  Learning rate = 0.003324:   Batch Loss = 1.074451, Accuracy = 0.66796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1399126052856445, Accuracy = 0.65727698802948\n",
            "Iter #1101824:  Learning rate = 0.003191:   Batch Loss = 1.119147, Accuracy = 0.638671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1536593437194824, Accuracy = 0.6451051831245422\n",
            "Iter #1105920:  Learning rate = 0.003191:   Batch Loss = 1.114216, Accuracy = 0.677734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1616721153259277, Accuracy = 0.6484089493751526\n",
            "Iter #1110016:  Learning rate = 0.003191:   Batch Loss = 1.135148, Accuracy = 0.63671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1594829559326172, Accuracy = 0.6461485028266907\n",
            "Iter #1114112:  Learning rate = 0.003191:   Batch Loss = 1.078853, Accuracy = 0.6953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1560180187225342, Accuracy = 0.6298035383224487\n",
            "Iter #1118208:  Learning rate = 0.003191:   Batch Loss = 1.058044, Accuracy = 0.6796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1245900392532349, Accuracy = 0.6588419675827026\n",
            "Iter #1122304:  Learning rate = 0.003191:   Batch Loss = 1.027336, Accuracy = 0.71484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1729663610458374, Accuracy = 0.631716251373291\n",
            "Iter #1126400:  Learning rate = 0.003191:   Batch Loss = 1.106265, Accuracy = 0.662109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.131750226020813, Accuracy = 0.6416275501251221\n",
            "Iter #1130496:  Learning rate = 0.003191:   Batch Loss = 1.138925, Accuracy = 0.654296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1595262289047241, Accuracy = 0.6508433222770691\n",
            "Iter #1134592:  Learning rate = 0.003191:   Batch Loss = 1.083365, Accuracy = 0.65625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1591882705688477, Accuracy = 0.6411058902740479\n",
            "Iter #1138688:  Learning rate = 0.003191:   Batch Loss = 1.076237, Accuracy = 0.6953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.202734112739563, Accuracy = 0.6198921799659729\n",
            "Iter #1142784:  Learning rate = 0.003191:   Batch Loss = 1.026299, Accuracy = 0.71484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1458158493041992, Accuracy = 0.6329333782196045\n",
            "Iter #1146880:  Learning rate = 0.003191:   Batch Loss = 1.131303, Accuracy = 0.640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1130372285842896, Accuracy = 0.65727698802948\n",
            "Iter #1150976:  Learning rate = 0.003191:   Batch Loss = 1.062556, Accuracy = 0.6875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1392685174942017, Accuracy = 0.6273691654205322\n",
            "Iter #1155072:  Learning rate = 0.003191:   Batch Loss = 1.083136, Accuracy = 0.677734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1051092147827148, Accuracy = 0.6715353727340698\n",
            "Iter #1159168:  Learning rate = 0.003191:   Batch Loss = 1.175727, Accuracy = 0.6640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.159179449081421, Accuracy = 0.6513649821281433\n",
            "Iter #1163264:  Learning rate = 0.003191:   Batch Loss = 1.077732, Accuracy = 0.677734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.137784481048584, Accuracy = 0.6374543309211731\n",
            "Iter #1167360:  Learning rate = 0.003191:   Batch Loss = 1.053850, Accuracy = 0.677734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1213734149932861, Accuracy = 0.658668041229248\n",
            "Iter #1171456:  Learning rate = 0.003191:   Batch Loss = 1.050699, Accuracy = 0.732421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0966955423355103, Accuracy = 0.666492760181427\n",
            "Iter #1175552:  Learning rate = 0.003191:   Batch Loss = 1.018179, Accuracy = 0.708984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1825284957885742, Accuracy = 0.6411058902740479\n",
            "Iter #1179648:  Learning rate = 0.003191:   Batch Loss = 1.087935, Accuracy = 0.666015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1437045335769653, Accuracy = 0.635367751121521\n",
            "Iter #1183744:  Learning rate = 0.003191:   Batch Loss = 1.138894, Accuracy = 0.681640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.165653944015503, Accuracy = 0.6294557452201843\n",
            "Iter #1187840:  Learning rate = 0.003191:   Batch Loss = 1.086801, Accuracy = 0.67578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1437804698944092, Accuracy = 0.6536254286766052\n",
            "Iter #1191936:  Learning rate = 0.003191:   Batch Loss = 1.080799, Accuracy = 0.689453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1438701152801514, Accuracy = 0.6659711599349976\n",
            "Iter #1196032:  Learning rate = 0.003191:   Batch Loss = 1.030740, Accuracy = 0.697265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1743730306625366, Accuracy = 0.6510171890258789\n",
            "Iter #1200128:  Learning rate = 0.003064:   Batch Loss = 1.034543, Accuracy = 0.72265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0934853553771973, Accuracy = 0.6850982308387756\n",
            "Iter #1204224:  Learning rate = 0.003064:   Batch Loss = 1.057696, Accuracy = 0.685546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.110946536064148, Accuracy = 0.6816205978393555\n",
            "Iter #1208320:  Learning rate = 0.003064:   Batch Loss = 1.074646, Accuracy = 0.681640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.136268973350525, Accuracy = 0.6362372040748596\n",
            "Iter #1212416:  Learning rate = 0.003064:   Batch Loss = 1.094778, Accuracy = 0.685546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1023106575012207, Accuracy = 0.6619718074798584\n",
            "Iter #1216512:  Learning rate = 0.003064:   Batch Loss = 1.045114, Accuracy = 0.69140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0756371021270752, Accuracy = 0.6824899911880493\n",
            "Iter #1220608:  Learning rate = 0.003064:   Batch Loss = 1.017893, Accuracy = 0.69921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0614455938339233, Accuracy = 0.6950095891952515\n",
            "Iter #1224704:  Learning rate = 0.003064:   Batch Loss = 1.029585, Accuracy = 0.71484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0423879623413086, Accuracy = 0.7050947546958923\n",
            "Iter #1228800:  Learning rate = 0.003064:   Batch Loss = 1.023256, Accuracy = 0.703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0550501346588135, Accuracy = 0.6880542635917664\n",
            "Iter #1232896:  Learning rate = 0.003064:   Batch Loss = 1.011323, Accuracy = 0.724609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0969711542129517, Accuracy = 0.6404103636741638\n",
            "Iter #1236992:  Learning rate = 0.003064:   Batch Loss = 1.042341, Accuracy = 0.705078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.048122763633728, Accuracy = 0.6960528492927551\n",
            "Iter #1241088:  Learning rate = 0.003064:   Batch Loss = 0.986412, Accuracy = 0.74609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0682569742202759, Accuracy = 0.6917057633399963\n",
            "Iter #1245184:  Learning rate = 0.003064:   Batch Loss = 1.069756, Accuracy = 0.673828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.116552710533142, Accuracy = 0.6405842304229736\n",
            "Iter #1249280:  Learning rate = 0.003064:   Batch Loss = 1.074618, Accuracy = 0.66015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.102150797843933, Accuracy = 0.6626673340797424\n",
            "Iter #1253376:  Learning rate = 0.003064:   Batch Loss = 1.038967, Accuracy = 0.697265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0878956317901611, Accuracy = 0.6673622131347656\n",
            "Iter #1257472:  Learning rate = 0.003064:   Batch Loss = 1.062726, Accuracy = 0.677734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0567774772644043, Accuracy = 0.679881751537323\n",
            "Iter #1261568:  Learning rate = 0.003064:   Batch Loss = 0.999834, Accuracy = 0.732421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.081650733947754, Accuracy = 0.6770996451377869\n",
            "Iter #1265664:  Learning rate = 0.003064:   Batch Loss = 1.027162, Accuracy = 0.66796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0504873991012573, Accuracy = 0.702486515045166\n",
            "Iter #1269760:  Learning rate = 0.003064:   Batch Loss = 1.058899, Accuracy = 0.671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.066354513168335, Accuracy = 0.6729264259338379\n",
            "Iter #1273856:  Learning rate = 0.003064:   Batch Loss = 1.016148, Accuracy = 0.7109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0747783184051514, Accuracy = 0.6776213049888611\n",
            "Iter #1277952:  Learning rate = 0.003064:   Batch Loss = 1.128894, Accuracy = 0.666015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0912610292434692, Accuracy = 0.6659711599349976\n",
            "Iter #1282048:  Learning rate = 0.003064:   Batch Loss = 1.089707, Accuracy = 0.69921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1007835865020752, Accuracy = 0.6577986478805542\n",
            "Iter #1286144:  Learning rate = 0.003064:   Batch Loss = 1.045043, Accuracy = 0.6953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.069990634918213, Accuracy = 0.6777951717376709\n",
            "Iter #1290240:  Learning rate = 0.003064:   Batch Loss = 0.977906, Accuracy = 0.73046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0717065334320068, Accuracy = 0.6875326037406921\n",
            "Iter #1294336:  Learning rate = 0.003064:   Batch Loss = 1.010875, Accuracy = 0.712890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1062266826629639, Accuracy = 0.6755346655845642\n",
            "Iter #1298432:  Learning rate = 0.003064:   Batch Loss = 1.124535, Accuracy = 0.669921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.123786449432373, Accuracy = 0.6498000621795654\n",
            "Iter #1302528:  Learning rate = 0.002941:   Batch Loss = 1.056729, Accuracy = 0.677734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0914725065231323, Accuracy = 0.662841260433197\n",
            "Iter #1306624:  Learning rate = 0.002941:   Batch Loss = 1.043932, Accuracy = 0.705078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.088577389717102, Accuracy = 0.6722308993339539\n",
            "Iter #1310720:  Learning rate = 0.002941:   Batch Loss = 1.026851, Accuracy = 0.720703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0468204021453857, Accuracy = 0.6804034113883972\n",
            "Iter #1314816:  Learning rate = 0.002941:   Batch Loss = 0.964830, Accuracy = 0.74609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0269932746887207, Accuracy = 0.7009215950965881\n",
            "Iter #1318912:  Learning rate = 0.002941:   Batch Loss = 1.006631, Accuracy = 0.69140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0222934484481812, Accuracy = 0.7108328938484192\n",
            "Iter #1323008:  Learning rate = 0.002941:   Batch Loss = 1.010230, Accuracy = 0.716796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0366473197937012, Accuracy = 0.684750497341156\n",
            "Iter #1327104:  Learning rate = 0.002941:   Batch Loss = 1.065651, Accuracy = 0.669921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.058631420135498, Accuracy = 0.7040514945983887\n",
            "Iter #1331200:  Learning rate = 0.002941:   Batch Loss = 0.974534, Accuracy = 0.732421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0645451545715332, Accuracy = 0.6814467310905457\n",
            "Iter #1335296:  Learning rate = 0.002941:   Batch Loss = 1.058993, Accuracy = 0.67578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0378954410552979, Accuracy = 0.7038775682449341\n",
            "Iter #1339392:  Learning rate = 0.002941:   Batch Loss = 1.026291, Accuracy = 0.703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0792200565338135, Accuracy = 0.6753607988357544\n",
            "Iter #1343488:  Learning rate = 0.002941:   Batch Loss = 0.964543, Accuracy = 0.74609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0534532070159912, Accuracy = 0.6864892840385437\n",
            "Iter #1347584:  Learning rate = 0.002941:   Batch Loss = 1.024158, Accuracy = 0.697265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0232189893722534, Accuracy = 0.6965745091438293\n",
            "Iter #1351680:  Learning rate = 0.002941:   Batch Loss = 0.992456, Accuracy = 0.73046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0239830017089844, Accuracy = 0.6944879293441772\n",
            "Iter #1355776:  Learning rate = 0.002941:   Batch Loss = 1.026958, Accuracy = 0.69140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0476691722869873, Accuracy = 0.6951834559440613\n",
            "Iter #1359872:  Learning rate = 0.002941:   Batch Loss = 0.962565, Accuracy = 0.736328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0336906909942627, Accuracy = 0.7075291275978088\n",
            "Iter #1363968:  Learning rate = 0.002941:   Batch Loss = 1.055976, Accuracy = 0.69140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.068371295928955, Accuracy = 0.6685793995857239\n",
            "Iter #1368064:  Learning rate = 0.002941:   Batch Loss = 0.958834, Accuracy = 0.744140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1276086568832397, Accuracy = 0.6534515619277954\n",
            "Iter #1372160:  Learning rate = 0.002941:   Batch Loss = 0.981364, Accuracy = 0.728515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0230238437652588, Accuracy = 0.6970961689949036\n",
            "Iter #1376256:  Learning rate = 0.002941:   Batch Loss = 0.945073, Accuracy = 0.740234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.018772840499878, Accuracy = 0.7078769207000732\n",
            "Iter #1380352:  Learning rate = 0.002941:   Batch Loss = 0.995635, Accuracy = 0.73828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.012237310409546, Accuracy = 0.7148321866989136\n",
            "Iter #1384448:  Learning rate = 0.002941:   Batch Loss = 0.915899, Accuracy = 0.7421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0098203420639038, Accuracy = 0.7094418406486511\n",
            "Iter #1388544:  Learning rate = 0.002941:   Batch Loss = 0.971108, Accuracy = 0.724609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0526564121246338, Accuracy = 0.6882281303405762\n",
            "Iter #1392640:  Learning rate = 0.002941:   Batch Loss = 1.092961, Accuracy = 0.666015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1089229583740234, Accuracy = 0.6590158343315125\n",
            "Iter #1396736:  Learning rate = 0.002941:   Batch Loss = 1.049999, Accuracy = 0.673828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0344557762145996, Accuracy = 0.6955311894416809\n",
            "Iter #1400832:  Learning rate = 0.002823:   Batch Loss = 1.002405, Accuracy = 0.71484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0027981996536255, Accuracy = 0.7026603817939758\n",
            "Iter #1404928:  Learning rate = 0.002823:   Batch Loss = 1.033232, Accuracy = 0.68359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0616135597229004, Accuracy = 0.6644061803817749\n",
            "Iter #1409024:  Learning rate = 0.002823:   Batch Loss = 0.929024, Accuracy = 0.740234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.978989839553833, Accuracy = 0.7221353054046631\n",
            "Iter #1413120:  Learning rate = 0.002823:   Batch Loss = 0.929779, Accuracy = 0.7578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0345616340637207, Accuracy = 0.7056164145469666\n",
            "Iter #1417216:  Learning rate = 0.002823:   Batch Loss = 1.002254, Accuracy = 0.724609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0012723207473755, Accuracy = 0.7195270657539368\n",
            "Iter #1421312:  Learning rate = 0.002823:   Batch Loss = 0.924075, Accuracy = 0.732421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9953408241271973, Accuracy = 0.7047470211982727\n",
            "Iter #1425408:  Learning rate = 0.002823:   Batch Loss = 0.963669, Accuracy = 0.71875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9850444793701172, Accuracy = 0.7217875123023987\n",
            "Iter #1429504:  Learning rate = 0.002823:   Batch Loss = 0.933560, Accuracy = 0.740234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.043910264968872, Accuracy = 0.6924013495445251\n",
            "Iter #1433600:  Learning rate = 0.002823:   Batch Loss = 0.933355, Accuracy = 0.7265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0201497077941895, Accuracy = 0.6983133554458618\n",
            "Iter #1437696:  Learning rate = 0.002823:   Batch Loss = 0.935242, Accuracy = 0.74609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.019212245941162, Accuracy = 0.6922274231910706\n",
            "Iter #1441792:  Learning rate = 0.002823:   Batch Loss = 1.026693, Accuracy = 0.6953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.052611231803894, Accuracy = 0.6863154172897339\n",
            "Iter #1445888:  Learning rate = 0.002823:   Batch Loss = 0.987969, Accuracy = 0.728515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0147576332092285, Accuracy = 0.6906625032424927\n",
            "Iter #1449984:  Learning rate = 0.002823:   Batch Loss = 0.914180, Accuracy = 0.771484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9661746025085449, Accuracy = 0.7257868051528931\n",
            "Iter #1454080:  Learning rate = 0.002823:   Batch Loss = 2.061344, Accuracy = 0.568359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.5324171781539917, Accuracy = 0.6115458011627197\n",
            "Iter #1458176:  Learning rate = 0.002823:   Batch Loss = 1.198874, Accuracy = 0.650390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.1812412738800049, Accuracy = 0.6463223695755005\n",
            "Iter #1462272:  Learning rate = 0.002823:   Batch Loss = 1.013729, Accuracy = 0.73046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0689880847930908, Accuracy = 0.702486515045166\n",
            "Iter #1466368:  Learning rate = 0.002823:   Batch Loss = 0.987601, Accuracy = 0.728515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9750449657440186, Accuracy = 0.728568971157074\n",
            "Iter #1470464:  Learning rate = 0.002823:   Batch Loss = 1.023897, Accuracy = 0.701171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9708551168441772, Accuracy = 0.7247435450553894\n",
            "Iter #1474560:  Learning rate = 0.002823:   Batch Loss = 0.931707, Accuracy = 0.73828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9520635604858398, Accuracy = 0.7334376573562622\n",
            "Iter #1478656:  Learning rate = 0.002823:   Batch Loss = 0.994224, Accuracy = 0.705078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0938432216644287, Accuracy = 0.7030081748962402\n",
            "Iter #1482752:  Learning rate = 0.002823:   Batch Loss = 0.923548, Accuracy = 0.736328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9674923419952393, Accuracy = 0.729960024356842\n",
            "Iter #1486848:  Learning rate = 0.002823:   Batch Loss = 0.935432, Accuracy = 0.732421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.008620262145996, Accuracy = 0.6998782753944397\n",
            "Iter #1490944:  Learning rate = 0.002823:   Batch Loss = 0.950811, Accuracy = 0.703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9837222099304199, Accuracy = 0.7143105268478394\n",
            "Iter #1495040:  Learning rate = 0.002823:   Batch Loss = 0.884898, Accuracy = 0.75\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9381192922592163, Accuracy = 0.7466527819633484\n",
            "Iter #1499136:  Learning rate = 0.002823:   Batch Loss = 0.937270, Accuracy = 0.734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9784666299819946, Accuracy = 0.7330899238586426\n",
            "Iter #1503232:  Learning rate = 0.002710:   Batch Loss = 0.903612, Accuracy = 0.76953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9665529131889343, Accuracy = 0.7092679738998413\n",
            "Iter #1507328:  Learning rate = 0.002710:   Batch Loss = 0.907999, Accuracy = 0.73828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9311827421188354, Accuracy = 0.7280473113059998\n",
            "Iter #1511424:  Learning rate = 0.002710:   Batch Loss = 0.953213, Accuracy = 0.7265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.957728385925293, Accuracy = 0.7313510775566101\n",
            "Iter #1515520:  Learning rate = 0.002710:   Batch Loss = 0.907056, Accuracy = 0.74609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9835633039474487, Accuracy = 0.7190054059028625\n",
            "Iter #1519616:  Learning rate = 0.002710:   Batch Loss = 0.895113, Accuracy = 0.751953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0006459951400757, Accuracy = 0.7167448997497559\n",
            "Iter #1523712:  Learning rate = 0.002710:   Batch Loss = 0.919690, Accuracy = 0.73828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0339692831039429, Accuracy = 0.7078769207000732\n",
            "Iter #1527808:  Learning rate = 0.002710:   Batch Loss = 0.889961, Accuracy = 0.76953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0299233198165894, Accuracy = 0.6972700357437134\n",
            "Iter #1531904:  Learning rate = 0.002710:   Batch Loss = 0.908608, Accuracy = 0.763671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9548585414886475, Accuracy = 0.7308294177055359\n",
            "Iter #1536000:  Learning rate = 0.002710:   Batch Loss = 0.913752, Accuracy = 0.75\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9748115539550781, Accuracy = 0.7122239470481873\n",
            "Iter #1540096:  Learning rate = 0.002710:   Batch Loss = 0.921058, Accuracy = 0.734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9555273056030273, Accuracy = 0.7163971662521362\n",
            "Iter #1544192:  Learning rate = 0.002710:   Batch Loss = 0.915462, Accuracy = 0.736328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9558156132698059, Accuracy = 0.7249174118041992\n",
            "Iter #1548288:  Learning rate = 0.002710:   Batch Loss = 0.915225, Accuracy = 0.74609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9487535357475281, Accuracy = 0.7170926928520203\n",
            "Iter #1552384:  Learning rate = 0.002710:   Batch Loss = 0.960325, Accuracy = 0.728515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9738966226577759, Accuracy = 0.7127456068992615\n",
            "Iter #1556480:  Learning rate = 0.002710:   Batch Loss = 1.007262, Accuracy = 0.7109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9728724956512451, Accuracy = 0.7163971662521362\n",
            "Iter #1560576:  Learning rate = 0.002710:   Batch Loss = 0.948212, Accuracy = 0.734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9722142219543457, Accuracy = 0.711006760597229\n",
            "Iter #1564672:  Learning rate = 0.002710:   Batch Loss = 0.879237, Accuracy = 0.767578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.971969485282898, Accuracy = 0.7221353054046631\n",
            "Iter #1568768:  Learning rate = 0.002710:   Batch Loss = 0.883556, Accuracy = 0.7578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9551277160644531, Accuracy = 0.7247435450553894\n",
            "Iter #1572864:  Learning rate = 0.002710:   Batch Loss = 0.881434, Accuracy = 0.74609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.944049596786499, Accuracy = 0.7195270657539368\n",
            "Iter #1576960:  Learning rate = 0.002710:   Batch Loss = 1.017766, Accuracy = 0.67578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9821773767471313, Accuracy = 0.7238740921020508\n",
            "Iter #1581056:  Learning rate = 0.002710:   Batch Loss = 0.922938, Accuracy = 0.767578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9650808572769165, Accuracy = 0.7203964591026306\n",
            "Iter #1585152:  Learning rate = 0.002710:   Batch Loss = 0.925648, Accuracy = 0.740234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9641046524047852, Accuracy = 0.7207441926002502\n",
            "Iter #1589248:  Learning rate = 0.002710:   Batch Loss = 0.958906, Accuracy = 0.740234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9519564509391785, Accuracy = 0.7170926928520203\n",
            "Iter #1593344:  Learning rate = 0.002710:   Batch Loss = 0.946970, Accuracy = 0.716796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.000425934791565, Accuracy = 0.7157016396522522\n",
            "Iter #1597440:  Learning rate = 0.002710:   Batch Loss = 0.970070, Accuracy = 0.7109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9595087766647339, Accuracy = 0.718831479549408\n",
            "Iter #1601536:  Learning rate = 0.002602:   Batch Loss = 0.936238, Accuracy = 0.71484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9285125732421875, Accuracy = 0.7363936901092529\n",
            "Iter #1605632:  Learning rate = 0.002602:   Batch Loss = 0.906082, Accuracy = 0.75390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9502884149551392, Accuracy = 0.7221353054046631\n",
            "Iter #1609728:  Learning rate = 0.002602:   Batch Loss = 0.986775, Accuracy = 0.720703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.977138102054596, Accuracy = 0.7207441926002502\n",
            "Iter #1613824:  Learning rate = 0.002602:   Batch Loss = 0.934105, Accuracy = 0.751953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0252742767333984, Accuracy = 0.6955311894416809\n",
            "Iter #1617920:  Learning rate = 0.002602:   Batch Loss = 0.934779, Accuracy = 0.73046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9564163684844971, Accuracy = 0.7315249443054199\n",
            "Iter #1622016:  Learning rate = 0.002602:   Batch Loss = 0.857744, Accuracy = 0.771484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9474306106567383, Accuracy = 0.7325682640075684\n",
            "Iter #1626112:  Learning rate = 0.002602:   Batch Loss = 0.908512, Accuracy = 0.71875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9239590167999268, Accuracy = 0.7327421307563782\n",
            "Iter #1630208:  Learning rate = 0.002602:   Batch Loss = 0.907490, Accuracy = 0.73828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9610124826431274, Accuracy = 0.7118762135505676\n",
            "Iter #1634304:  Learning rate = 0.002602:   Batch Loss = 0.853951, Accuracy = 0.755859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9526718854904175, Accuracy = 0.7263084650039673\n",
            "Iter #1638400:  Learning rate = 0.002602:   Batch Loss = 0.952804, Accuracy = 0.740234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9791673421859741, Accuracy = 0.7264823317527771\n",
            "Iter #1642496:  Learning rate = 0.002602:   Batch Loss = 0.896362, Accuracy = 0.767578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9536645412445068, Accuracy = 0.7297860980033875\n",
            "Iter #1646592:  Learning rate = 0.002602:   Batch Loss = 0.954432, Accuracy = 0.72265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.006532907485962, Accuracy = 0.7153538465499878\n",
            "Iter #1650688:  Learning rate = 0.002602:   Batch Loss = 0.900414, Accuracy = 0.771484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.962132453918457, Accuracy = 0.7217875123023987\n",
            "Iter #1654784:  Learning rate = 0.002602:   Batch Loss = 0.844743, Accuracy = 0.79296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9551092386245728, Accuracy = 0.7320466041564941\n",
            "Iter #1658880:  Learning rate = 0.002602:   Batch Loss = 0.897981, Accuracy = 0.759765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9652465581893921, Accuracy = 0.7304816842079163\n",
            "Iter #1662976:  Learning rate = 0.002602:   Batch Loss = 0.881597, Accuracy = 0.75390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9501349925994873, Accuracy = 0.7191792726516724\n",
            "Iter #1667072:  Learning rate = 0.002602:   Batch Loss = 0.896329, Accuracy = 0.751953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9365460872650146, Accuracy = 0.7410885095596313\n",
            "Iter #1671168:  Learning rate = 0.002602:   Batch Loss = 0.860081, Accuracy = 0.76953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9459576606750488, Accuracy = 0.7238740921020508\n",
            "Iter #1675264:  Learning rate = 0.002602:   Batch Loss = 0.934251, Accuracy = 0.748046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9592550992965698, Accuracy = 0.7348287105560303\n",
            "Iter #1679360:  Learning rate = 0.002602:   Batch Loss = 0.913266, Accuracy = 0.728515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9337103962898254, Accuracy = 0.7270039916038513\n",
            "Iter #1683456:  Learning rate = 0.002602:   Batch Loss = 0.906273, Accuracy = 0.7578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9745692014694214, Accuracy = 0.7301338911056519\n",
            "Iter #1687552:  Learning rate = 0.002602:   Batch Loss = 0.945673, Accuracy = 0.7265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9484739303588867, Accuracy = 0.7282211780548096\n",
            "Iter #1691648:  Learning rate = 0.002602:   Batch Loss = 0.882864, Accuracy = 0.779296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9644680023193359, Accuracy = 0.7318727374076843\n",
            "Iter #1695744:  Learning rate = 0.002602:   Batch Loss = 1.057416, Accuracy = 0.69921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.928890585899353, Accuracy = 0.7341331839561462\n",
            "Iter #1699840:  Learning rate = 0.002602:   Batch Loss = 0.945675, Accuracy = 0.74609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9982588887214661, Accuracy = 0.7050947546958923\n",
            "Iter #1703936:  Learning rate = 0.002498:   Batch Loss = 0.900590, Accuracy = 0.75\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9133898615837097, Accuracy = 0.7341331839561462\n",
            "Iter #1708032:  Learning rate = 0.002498:   Batch Loss = 0.859726, Accuracy = 0.767578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9291577339172363, Accuracy = 0.7390019297599792\n",
            "Iter #1712128:  Learning rate = 0.002498:   Batch Loss = 0.881421, Accuracy = 0.767578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9441993236541748, Accuracy = 0.729960024356842\n",
            "Iter #1716224:  Learning rate = 0.002498:   Batch Loss = 0.886496, Accuracy = 0.748046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9344633221626282, Accuracy = 0.7391757965087891\n",
            "Iter #1720320:  Learning rate = 0.002498:   Batch Loss = 0.843692, Accuracy = 0.75390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9169026017189026, Accuracy = 0.732220470905304\n",
            "Iter #1724416:  Learning rate = 0.002498:   Batch Loss = 0.877166, Accuracy = 0.765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9360536932945251, Accuracy = 0.7306555509567261\n",
            "Iter #1728512:  Learning rate = 0.002498:   Batch Loss = 0.894025, Accuracy = 0.76171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9295552968978882, Accuracy = 0.737784743309021\n",
            "Iter #1732608:  Learning rate = 0.002498:   Batch Loss = 0.848115, Accuracy = 0.771484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.911037027835846, Accuracy = 0.7360458970069885\n",
            "Iter #1736704:  Learning rate = 0.002498:   Batch Loss = 0.925368, Accuracy = 0.73046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9754369258880615, Accuracy = 0.6990088820457458\n",
            "Iter #1740800:  Learning rate = 0.002498:   Batch Loss = 0.920161, Accuracy = 0.7265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9773365259170532, Accuracy = 0.7197009325027466\n",
            "Iter #1744896:  Learning rate = 0.002498:   Batch Loss = 0.882923, Accuracy = 0.77734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0090614557266235, Accuracy = 0.7224830389022827\n",
            "Iter #1748992:  Learning rate = 0.002498:   Batch Loss = 0.861799, Accuracy = 0.755859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9290266036987305, Accuracy = 0.7306555509567261\n",
            "Iter #1753088:  Learning rate = 0.002498:   Batch Loss = 0.865186, Accuracy = 0.767578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9174356460571289, Accuracy = 0.7402191162109375\n",
            "Iter #1757184:  Learning rate = 0.002498:   Batch Loss = 0.871093, Accuracy = 0.767578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9361757040023804, Accuracy = 0.7390019297599792\n",
            "Iter #1761280:  Learning rate = 0.002498:   Batch Loss = 0.901412, Accuracy = 0.7578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9845856428146362, Accuracy = 0.724395751953125\n",
            "Iter #1765376:  Learning rate = 0.002498:   Batch Loss = 0.863946, Accuracy = 0.755859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9166547060012817, Accuracy = 0.7386541366577148\n",
            "Iter #1769472:  Learning rate = 0.002498:   Batch Loss = 0.831967, Accuracy = 0.765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.965898871421814, Accuracy = 0.7190054059028625\n",
            "Iter #1773568:  Learning rate = 0.002498:   Batch Loss = 0.952659, Accuracy = 0.693359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9845212697982788, Accuracy = 0.680577278137207\n",
            "Iter #1777664:  Learning rate = 0.002498:   Batch Loss = 0.958492, Accuracy = 0.701171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9310950040817261, Accuracy = 0.723700225353241\n",
            "Iter #1781760:  Learning rate = 0.002498:   Batch Loss = 0.915182, Accuracy = 0.708984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9843730926513672, Accuracy = 0.7125717401504517\n",
            "Iter #1785856:  Learning rate = 0.002498:   Batch Loss = 0.865723, Accuracy = 0.75\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9267188310623169, Accuracy = 0.723700225353241\n",
            "Iter #1789952:  Learning rate = 0.002498:   Batch Loss = 0.937486, Accuracy = 0.73046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9311912059783936, Accuracy = 0.7210919857025146\n",
            "Iter #1794048:  Learning rate = 0.002498:   Batch Loss = 0.913435, Accuracy = 0.734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9898488521575928, Accuracy = 0.6990088820457458\n",
            "Iter #1798144:  Learning rate = 0.002498:   Batch Loss = 0.908093, Accuracy = 0.7578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9879941940307617, Accuracy = 0.7177882194519043\n",
            "Iter #1802240:  Learning rate = 0.002398:   Batch Loss = 0.922338, Accuracy = 0.744140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0351285934448242, Accuracy = 0.6918796896934509\n",
            "Iter #1806336:  Learning rate = 0.002398:   Batch Loss = 0.888181, Accuracy = 0.734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9409892559051514, Accuracy = 0.7146583199501038\n",
            "Iter #1810432:  Learning rate = 0.002398:   Batch Loss = 0.873962, Accuracy = 0.7421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9251654148101807, Accuracy = 0.7334376573562622\n",
            "Iter #1814528:  Learning rate = 0.002398:   Batch Loss = 0.906782, Accuracy = 0.728515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9245126247406006, Accuracy = 0.7386541366577148\n",
            "Iter #1818624:  Learning rate = 0.002398:   Batch Loss = 0.893701, Accuracy = 0.72265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9012872576713562, Accuracy = 0.7416101694107056\n",
            "Iter #1822720:  Learning rate = 0.002398:   Batch Loss = 0.852409, Accuracy = 0.74609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9114505052566528, Accuracy = 0.732220470905304\n",
            "Iter #1826816:  Learning rate = 0.002398:   Batch Loss = 0.945678, Accuracy = 0.689453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9884307384490967, Accuracy = 0.7170926928520203\n",
            "Iter #1830912:  Learning rate = 0.002398:   Batch Loss = 0.953457, Accuracy = 0.724609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9562387466430664, Accuracy = 0.7197009325027466\n",
            "Iter #1835008:  Learning rate = 0.002398:   Batch Loss = 0.936981, Accuracy = 0.734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9347900152206421, Accuracy = 0.7315249443054199\n",
            "Iter #1839104:  Learning rate = 0.002398:   Batch Loss = 0.832514, Accuracy = 0.76953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9548286199569702, Accuracy = 0.7283950448036194\n",
            "Iter #1843200:  Learning rate = 0.002398:   Batch Loss = 0.943617, Accuracy = 0.7421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0121798515319824, Accuracy = 0.7130934000015259\n",
            "Iter #1847296:  Learning rate = 0.002398:   Batch Loss = 0.856969, Accuracy = 0.73828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9160560369491577, Accuracy = 0.7395235896110535\n",
            "Iter #1851392:  Learning rate = 0.002398:   Batch Loss = 0.817724, Accuracy = 0.78125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9148528575897217, Accuracy = 0.7410885095596313\n",
            "Iter #1855488:  Learning rate = 0.002398:   Batch Loss = 0.906216, Accuracy = 0.74609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9400550723075867, Accuracy = 0.7275256514549255\n",
            "Iter #1859584:  Learning rate = 0.002398:   Batch Loss = 0.920893, Accuracy = 0.728515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9611606597900391, Accuracy = 0.7167448997497559\n",
            "Iter #1863680:  Learning rate = 0.002398:   Batch Loss = 0.911874, Accuracy = 0.7421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9533861875534058, Accuracy = 0.7330899238586426\n",
            "Iter #1867776:  Learning rate = 0.002398:   Batch Loss = 0.878034, Accuracy = 0.767578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9082207679748535, Accuracy = 0.7346548438072205\n",
            "Iter #1871872:  Learning rate = 0.002398:   Batch Loss = 0.829051, Accuracy = 0.751953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8970277309417725, Accuracy = 0.7381324768066406\n",
            "Iter #1875968:  Learning rate = 0.002398:   Batch Loss = 0.855419, Accuracy = 0.76171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9138752222061157, Accuracy = 0.7358720302581787\n",
            "Iter #1880064:  Learning rate = 0.002398:   Batch Loss = 0.874331, Accuracy = 0.75\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8905192017555237, Accuracy = 0.737089216709137\n",
            "Iter #1884160:  Learning rate = 0.002398:   Batch Loss = 0.839106, Accuracy = 0.775390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9134697914123535, Accuracy = 0.738480269908905\n",
            "Iter #1888256:  Learning rate = 0.002398:   Batch Loss = 0.922854, Accuracy = 0.703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9219149351119995, Accuracy = 0.7363936901092529\n",
            "Iter #1892352:  Learning rate = 0.002398:   Batch Loss = 0.897321, Accuracy = 0.7578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8970194458961487, Accuracy = 0.7351765036582947\n",
            "Iter #1896448:  Learning rate = 0.002398:   Batch Loss = 0.865238, Accuracy = 0.7578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9121859669685364, Accuracy = 0.7379586100578308\n",
            "Iter #1900544:  Learning rate = 0.002302:   Batch Loss = 0.933520, Accuracy = 0.73046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0096113681793213, Accuracy = 0.6906625032424927\n",
            "Iter #1904640:  Learning rate = 0.002302:   Batch Loss = 0.881293, Accuracy = 0.73828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9122393131256104, Accuracy = 0.7383064031600952\n",
            "Iter #1908736:  Learning rate = 0.002302:   Batch Loss = 0.867534, Accuracy = 0.765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9238653182983398, Accuracy = 0.7334376573562622\n",
            "Iter #1912832:  Learning rate = 0.002302:   Batch Loss = 0.858645, Accuracy = 0.7734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9456369280815125, Accuracy = 0.733611524105072\n",
            "Iter #1916928:  Learning rate = 0.002302:   Batch Loss = 0.818134, Accuracy = 0.76953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8841596245765686, Accuracy = 0.7483915686607361\n",
            "Iter #1921024:  Learning rate = 0.002302:   Batch Loss = 0.890343, Accuracy = 0.744140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.974515438079834, Accuracy = 0.718831479549408\n",
            "Iter #1925120:  Learning rate = 0.002302:   Batch Loss = 0.903404, Accuracy = 0.73046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8919201493263245, Accuracy = 0.7369152903556824\n",
            "Iter #1929216:  Learning rate = 0.002302:   Batch Loss = 0.819468, Accuracy = 0.74609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8894791603088379, Accuracy = 0.7403929829597473\n",
            "Iter #1933312:  Learning rate = 0.002302:   Batch Loss = 0.905050, Accuracy = 0.7265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9366095066070557, Accuracy = 0.7297860980033875\n",
            "Iter #1937408:  Learning rate = 0.002302:   Batch Loss = 0.818539, Accuracy = 0.78125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9233496785163879, Accuracy = 0.741436243057251\n",
            "Iter #1941504:  Learning rate = 0.002302:   Batch Loss = 0.840320, Accuracy = 0.763671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9080383777618408, Accuracy = 0.737784743309021\n",
            "Iter #1945600:  Learning rate = 0.002302:   Batch Loss = 0.906590, Accuracy = 0.744140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9158197045326233, Accuracy = 0.7362197637557983\n",
            "Iter #1949696:  Learning rate = 0.002302:   Batch Loss = 0.915603, Accuracy = 0.736328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9003685712814331, Accuracy = 0.7450878024101257\n",
            "Iter #1953792:  Learning rate = 0.002302:   Batch Loss = 0.871022, Accuracy = 0.712890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9130516052246094, Accuracy = 0.7362197637557983\n",
            "Iter #1957888:  Learning rate = 0.002302:   Batch Loss = 0.820381, Accuracy = 0.78125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8875041604042053, Accuracy = 0.7490870952606201\n",
            "Iter #1961984:  Learning rate = 0.002302:   Batch Loss = 0.779600, Accuracy = 0.7890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8891218900680542, Accuracy = 0.7447400689125061\n",
            "Iter #1966080:  Learning rate = 0.002302:   Batch Loss = 0.907648, Accuracy = 0.755859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9708188772201538, Accuracy = 0.7198747992515564\n",
            "Iter #1970176:  Learning rate = 0.002302:   Batch Loss = 0.956061, Accuracy = 0.734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 1.0424473285675049, Accuracy = 0.6814467310905457\n",
            "Iter #1974272:  Learning rate = 0.002302:   Batch Loss = 0.936824, Accuracy = 0.724609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.896410346031189, Accuracy = 0.7386541366577148\n",
            "Iter #1978368:  Learning rate = 0.002302:   Batch Loss = 0.910745, Accuracy = 0.7265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9010182619094849, Accuracy = 0.7417840361595154\n",
            "Iter #1982464:  Learning rate = 0.002302:   Batch Loss = 0.878623, Accuracy = 0.76171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9073628187179565, Accuracy = 0.7421318292617798\n",
            "Iter #1986560:  Learning rate = 0.002302:   Batch Loss = 0.956178, Accuracy = 0.72265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.957672655582428, Accuracy = 0.7210919857025146\n",
            "Iter #1990656:  Learning rate = 0.002302:   Batch Loss = 0.813942, Accuracy = 0.794921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8951171636581421, Accuracy = 0.7275256514549255\n",
            "Iter #1994752:  Learning rate = 0.002302:   Batch Loss = 0.897503, Accuracy = 0.708984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8861459493637085, Accuracy = 0.7376108765602112\n",
            "Iter #1998848:  Learning rate = 0.002302:   Batch Loss = 0.776047, Accuracy = 0.7890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8769182562828064, Accuracy = 0.7466527819633484\n",
            "Iter #2002944:  Learning rate = 0.002210:   Batch Loss = 0.867857, Accuracy = 0.783203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8952286839485168, Accuracy = 0.7442184090614319\n",
            "Iter #2007040:  Learning rate = 0.002210:   Batch Loss = 0.854808, Accuracy = 0.751953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9392123222351074, Accuracy = 0.7210919857025146\n",
            "Iter #2011136:  Learning rate = 0.002210:   Batch Loss = 0.840784, Accuracy = 0.76953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.907993495464325, Accuracy = 0.7327421307563782\n",
            "Iter #2015232:  Learning rate = 0.002210:   Batch Loss = 0.888133, Accuracy = 0.755859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8933467268943787, Accuracy = 0.7423056960105896\n",
            "Iter #2019328:  Learning rate = 0.002210:   Batch Loss = 0.913983, Accuracy = 0.740234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9321611523628235, Accuracy = 0.732220470905304\n",
            "Iter #2023424:  Learning rate = 0.002210:   Batch Loss = 0.902453, Accuracy = 0.73046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8960943818092346, Accuracy = 0.7348287105560303\n",
            "Iter #2027520:  Learning rate = 0.002210:   Batch Loss = 0.931494, Accuracy = 0.7265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9267354011535645, Accuracy = 0.7301338911056519\n",
            "Iter #2031616:  Learning rate = 0.002210:   Batch Loss = 0.841103, Accuracy = 0.755859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9272600412368774, Accuracy = 0.7200486660003662\n",
            "Iter #2035712:  Learning rate = 0.002210:   Batch Loss = 0.832841, Accuracy = 0.76953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9064978361129761, Accuracy = 0.7306555509567261\n",
            "Iter #2039808:  Learning rate = 0.002210:   Batch Loss = 0.862183, Accuracy = 0.74609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9120025634765625, Accuracy = 0.7313510775566101\n",
            "Iter #2043904:  Learning rate = 0.002210:   Batch Loss = 0.884793, Accuracy = 0.744140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9265612363815308, Accuracy = 0.7311772108078003\n",
            "Iter #2048000:  Learning rate = 0.002210:   Batch Loss = 0.826220, Accuracy = 0.787109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8990681171417236, Accuracy = 0.728568971157074\n",
            "Iter #2052096:  Learning rate = 0.002210:   Batch Loss = 0.837668, Accuracy = 0.77734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9062142372131348, Accuracy = 0.7339593172073364\n",
            "Iter #2056192:  Learning rate = 0.002210:   Batch Loss = 0.959228, Accuracy = 0.68359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8705853819847107, Accuracy = 0.7426534295082092\n",
            "Iter #2060288:  Learning rate = 0.002210:   Batch Loss = 0.858023, Accuracy = 0.755859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8969257473945618, Accuracy = 0.7480438351631165\n",
            "Iter #2064384:  Learning rate = 0.002210:   Batch Loss = 0.844104, Accuracy = 0.736328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9108693599700928, Accuracy = 0.7341331839561462\n",
            "Iter #2068480:  Learning rate = 0.002210:   Batch Loss = 0.851338, Accuracy = 0.76953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8592205047607422, Accuracy = 0.7532603144645691\n",
            "Iter #2072576:  Learning rate = 0.002210:   Batch Loss = 0.810534, Accuracy = 0.7734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8541620969772339, Accuracy = 0.7341331839561462\n",
            "Iter #2076672:  Learning rate = 0.002210:   Batch Loss = 0.855771, Accuracy = 0.75\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8750811815261841, Accuracy = 0.7567379474639893\n",
            "Iter #2080768:  Learning rate = 0.002210:   Batch Loss = 0.794770, Accuracy = 0.771484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8712935447692871, Accuracy = 0.746304988861084\n",
            "Iter #2084864:  Learning rate = 0.002210:   Batch Loss = 0.842978, Accuracy = 0.74609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8703776597976685, Accuracy = 0.7549991011619568\n",
            "Iter #2088960:  Learning rate = 0.002210:   Batch Loss = 0.819080, Accuracy = 0.796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8728591203689575, Accuracy = 0.7374369502067566\n",
            "Iter #2093056:  Learning rate = 0.002210:   Batch Loss = 0.873066, Accuracy = 0.73046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.915209949016571, Accuracy = 0.7348287105560303\n",
            "Iter #2097152:  Learning rate = 0.002210:   Batch Loss = 0.832522, Accuracy = 0.765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9455130696296692, Accuracy = 0.725091278553009\n",
            "Iter #2101248:  Learning rate = 0.002122:   Batch Loss = 0.892315, Accuracy = 0.73046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8701900243759155, Accuracy = 0.7602156400680542\n",
            "Iter #2105344:  Learning rate = 0.002122:   Batch Loss = 0.847870, Accuracy = 0.763671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8624124526977539, Accuracy = 0.7513476014137268\n",
            "Iter #2109440:  Learning rate = 0.002122:   Batch Loss = 0.792960, Accuracy = 0.794921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.890653669834137, Accuracy = 0.7395235896110535\n",
            "Iter #2113536:  Learning rate = 0.002122:   Batch Loss = 0.840442, Accuracy = 0.78515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8654965162277222, Accuracy = 0.7536080479621887\n",
            "Iter #2117632:  Learning rate = 0.002122:   Batch Loss = 0.846336, Accuracy = 0.771484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8860533237457275, Accuracy = 0.7480438351631165\n",
            "Iter #2121728:  Learning rate = 0.002122:   Batch Loss = 0.842213, Accuracy = 0.783203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8713570833206177, Accuracy = 0.7483915686607361\n",
            "Iter #2125824:  Learning rate = 0.002122:   Batch Loss = 0.880576, Accuracy = 0.732421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8504966497421265, Accuracy = 0.7569118142127991\n",
            "Iter #2129920:  Learning rate = 0.002122:   Batch Loss = 0.821696, Accuracy = 0.775390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8574764728546143, Accuracy = 0.746304988861084\n",
            "Iter #2134016:  Learning rate = 0.002122:   Batch Loss = 0.848066, Accuracy = 0.775390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8820409774780273, Accuracy = 0.7457833290100098\n",
            "Iter #2138112:  Learning rate = 0.002122:   Batch Loss = 0.899337, Accuracy = 0.728515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9143730401992798, Accuracy = 0.7360458970069885\n",
            "Iter #2142208:  Learning rate = 0.002122:   Batch Loss = 0.920362, Accuracy = 0.724609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9336822628974915, Accuracy = 0.7410885095596313\n",
            "Iter #2146304:  Learning rate = 0.002122:   Batch Loss = 0.873043, Accuracy = 0.734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8869221806526184, Accuracy = 0.7315249443054199\n",
            "Iter #2150400:  Learning rate = 0.002122:   Batch Loss = 0.785763, Accuracy = 0.765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8645287752151489, Accuracy = 0.7310032844543457\n",
            "Iter #2154496:  Learning rate = 0.002122:   Batch Loss = 0.765522, Accuracy = 0.80078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8820409774780273, Accuracy = 0.7473483085632324\n",
            "Iter #2158592:  Learning rate = 0.002122:   Batch Loss = 0.806245, Accuracy = 0.771484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8553417921066284, Accuracy = 0.7586506605148315\n",
            "Iter #2162688:  Learning rate = 0.002122:   Batch Loss = 0.796771, Accuracy = 0.76171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.846062183380127, Accuracy = 0.758998453617096\n",
            "Iter #2166784:  Learning rate = 0.002122:   Batch Loss = 0.816394, Accuracy = 0.78515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8601030111312866, Accuracy = 0.741436243057251\n",
            "Iter #2170880:  Learning rate = 0.002122:   Batch Loss = 0.834310, Accuracy = 0.771484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8502524495124817, Accuracy = 0.7576074004173279\n",
            "Iter #2174976:  Learning rate = 0.002122:   Batch Loss = 0.805303, Accuracy = 0.767578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8980998396873474, Accuracy = 0.715875506401062\n",
            "Iter #2179072:  Learning rate = 0.002122:   Batch Loss = 0.813604, Accuracy = 0.7734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8708646893501282, Accuracy = 0.7593461871147156\n",
            "Iter #2183168:  Learning rate = 0.002122:   Batch Loss = 0.847543, Accuracy = 0.76171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8737167119979858, Accuracy = 0.7480438351631165\n",
            "Iter #2187264:  Learning rate = 0.002122:   Batch Loss = 0.836330, Accuracy = 0.76953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8668547868728638, Accuracy = 0.7560424208641052\n",
            "Iter #2191360:  Learning rate = 0.002122:   Batch Loss = 0.812175, Accuracy = 0.779296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8684543371200562, Accuracy = 0.7482177019119263\n",
            "Iter #2195456:  Learning rate = 0.002122:   Batch Loss = 0.776404, Accuracy = 0.791015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8557026982307434, Accuracy = 0.7492610216140747\n",
            "Iter #2199552:  Learning rate = 0.002122:   Batch Loss = 0.811181, Accuracy = 0.78515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8762617111206055, Accuracy = 0.7471743822097778\n",
            "Iter #2203648:  Learning rate = 0.002037:   Batch Loss = 0.755794, Accuracy = 0.810546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8506723642349243, Accuracy = 0.758998453617096\n",
            "Iter #2207744:  Learning rate = 0.002037:   Batch Loss = 0.792794, Accuracy = 0.77734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8428186178207397, Accuracy = 0.7525647878646851\n",
            "Iter #2211840:  Learning rate = 0.002037:   Batch Loss = 0.833877, Accuracy = 0.765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9154056906700134, Accuracy = 0.7468266487121582\n",
            "Iter #2215936:  Learning rate = 0.002037:   Batch Loss = 0.815450, Accuracy = 0.759765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8414618968963623, Accuracy = 0.7546513676643372\n",
            "Iter #2220032:  Learning rate = 0.002037:   Batch Loss = 0.806461, Accuracy = 0.783203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.832194447517395, Accuracy = 0.7743000984191895\n",
            "Iter #2224128:  Learning rate = 0.002037:   Batch Loss = 0.773459, Accuracy = 0.8125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8259749412536621, Accuracy = 0.7709963321685791\n",
            "Iter #2228224:  Learning rate = 0.002037:   Batch Loss = 0.799559, Accuracy = 0.775390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8604754209518433, Accuracy = 0.764041006565094\n",
            "Iter #2232320:  Learning rate = 0.002037:   Batch Loss = 0.768971, Accuracy = 0.806640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.83847576379776, Accuracy = 0.7616066932678223\n",
            "Iter #2236416:  Learning rate = 0.002037:   Batch Loss = 0.863784, Accuracy = 0.771484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8464928865432739, Accuracy = 0.7598678469657898\n",
            "Iter #2240512:  Learning rate = 0.002037:   Batch Loss = 0.817933, Accuracy = 0.779296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9012596607208252, Accuracy = 0.7638671398162842\n",
            "Iter #2244608:  Learning rate = 0.002037:   Batch Loss = 0.828171, Accuracy = 0.79296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8567510843276978, Accuracy = 0.7553468942642212\n",
            "Iter #2248704:  Learning rate = 0.002037:   Batch Loss = 0.861991, Accuracy = 0.75\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9084341526031494, Accuracy = 0.7400451898574829\n",
            "Iter #2252800:  Learning rate = 0.002037:   Batch Loss = 0.860253, Accuracy = 0.732421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8300967216491699, Accuracy = 0.768214225769043\n",
            "Iter #2256896:  Learning rate = 0.002037:   Batch Loss = 0.811412, Accuracy = 0.7734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8607550263404846, Accuracy = 0.7556946873664856\n",
            "Iter #2260992:  Learning rate = 0.002037:   Batch Loss = 0.822742, Accuracy = 0.77734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8337658643722534, Accuracy = 0.7656059861183167\n",
            "Iter #2265088:  Learning rate = 0.002037:   Batch Loss = 0.837381, Accuracy = 0.765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.833864688873291, Accuracy = 0.7636932730674744\n",
            "Iter #2269184:  Learning rate = 0.002037:   Batch Loss = 0.767861, Accuracy = 0.8125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8178489208221436, Accuracy = 0.7654321193695068\n",
            "Iter #2273280:  Learning rate = 0.002037:   Batch Loss = 0.760788, Accuracy = 0.810546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8096561431884766, Accuracy = 0.7732568383216858\n",
            "Iter #2277376:  Learning rate = 0.002037:   Batch Loss = 0.758985, Accuracy = 0.82421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8355152606964111, Accuracy = 0.7609111666679382\n",
            "Iter #2281472:  Learning rate = 0.002037:   Batch Loss = 0.755235, Accuracy = 0.810546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8877972364425659, Accuracy = 0.7569118142127991\n",
            "Iter #2285568:  Learning rate = 0.002037:   Batch Loss = 0.789378, Accuracy = 0.794921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8691383600234985, Accuracy = 0.7565640807151794\n",
            "Iter #2289664:  Learning rate = 0.002037:   Batch Loss = 0.811301, Accuracy = 0.787109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8317477703094482, Accuracy = 0.7676925659179688\n",
            "Iter #2293760:  Learning rate = 0.002037:   Batch Loss = 0.838326, Accuracy = 0.7578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8350590467453003, Accuracy = 0.7715179920196533\n",
            "Iter #2297856:  Learning rate = 0.002037:   Batch Loss = 0.810279, Accuracy = 0.80859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8485575914382935, Accuracy = 0.7602156400680542\n",
            "Iter #2301952:  Learning rate = 0.001955:   Batch Loss = 0.799141, Accuracy = 0.80078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8179576396942139, Accuracy = 0.7654321193695068\n",
            "Iter #2306048:  Learning rate = 0.001955:   Batch Loss = 0.800686, Accuracy = 0.78515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8335820436477661, Accuracy = 0.7671709060668945\n",
            "Iter #2310144:  Learning rate = 0.001955:   Batch Loss = 0.787722, Accuracy = 0.78125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8172401785850525, Accuracy = 0.7659537196159363\n",
            "Iter #2314240:  Learning rate = 0.001955:   Batch Loss = 0.755269, Accuracy = 0.8125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8100981712341309, Accuracy = 0.776734471321106\n",
            "Iter #2318336:  Learning rate = 0.001955:   Batch Loss = 0.780975, Accuracy = 0.78515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8184349536895752, Accuracy = 0.769605278968811\n",
            "Iter #2322432:  Learning rate = 0.001955:   Batch Loss = 0.746501, Accuracy = 0.806640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7951903343200684, Accuracy = 0.7864719033241272\n",
            "Iter #2326528:  Learning rate = 0.001955:   Batch Loss = 0.766956, Accuracy = 0.7890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.817643404006958, Accuracy = 0.7713441252708435\n",
            "Iter #2330624:  Learning rate = 0.001955:   Batch Loss = 0.745038, Accuracy = 0.8125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8345870971679688, Accuracy = 0.7541297078132629\n",
            "Iter #2334720:  Learning rate = 0.001955:   Batch Loss = 0.795323, Accuracy = 0.7890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8227417469024658, Accuracy = 0.768214225769043\n",
            "Iter #2338816:  Learning rate = 0.001955:   Batch Loss = 0.779696, Accuracy = 0.796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8302339315414429, Accuracy = 0.7715179920196533\n",
            "Iter #2342912:  Learning rate = 0.001955:   Batch Loss = 0.780449, Accuracy = 0.80078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8057664632797241, Accuracy = 0.7716918587684631\n",
            "Iter #2347008:  Learning rate = 0.001955:   Batch Loss = 0.765972, Accuracy = 0.796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8113967180252075, Accuracy = 0.7800382375717163\n",
            "Iter #2351104:  Learning rate = 0.001955:   Batch Loss = 0.791581, Accuracy = 0.767578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.812364935874939, Accuracy = 0.7869935631752014\n",
            "Iter #2355200:  Learning rate = 0.001955:   Batch Loss = 0.776757, Accuracy = 0.775390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7893723249435425, Accuracy = 0.7842114567756653\n",
            "Iter #2359296:  Learning rate = 0.001955:   Batch Loss = 0.742497, Accuracy = 0.79296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7858635783195496, Accuracy = 0.778125524520874\n",
            "Iter #2363392:  Learning rate = 0.001955:   Batch Loss = 0.797771, Accuracy = 0.80078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7426438927650452, Accuracy = 0.8191618919372559\n",
            "Iter #2367488:  Learning rate = 0.001955:   Batch Loss = 0.725364, Accuracy = 0.810546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.752519428730011, Accuracy = 0.7925578355789185\n",
            "Iter #2371584:  Learning rate = 0.001955:   Batch Loss = 0.824915, Accuracy = 0.767578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8024304509162903, Accuracy = 0.7782994508743286\n",
            "Iter #2375680:  Learning rate = 0.001955:   Batch Loss = 0.746610, Accuracy = 0.79296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8391026258468628, Accuracy = 0.7673448324203491\n",
            "Iter #2379776:  Learning rate = 0.001955:   Batch Loss = 0.738278, Accuracy = 0.806640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8012479543685913, Accuracy = 0.782298743724823\n",
            "Iter #2383872:  Learning rate = 0.001955:   Batch Loss = 0.712803, Accuracy = 0.83984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7597523927688599, Accuracy = 0.812728226184845\n",
            "Iter #2387968:  Learning rate = 0.001955:   Batch Loss = 0.752930, Accuracy = 0.791015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7894446849822998, Accuracy = 0.7704746723175049\n",
            "Iter #2392064:  Learning rate = 0.001955:   Batch Loss = 0.798051, Accuracy = 0.7578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7897646427154541, Accuracy = 0.7739523649215698\n",
            "Iter #2396160:  Learning rate = 0.001955:   Batch Loss = 0.768011, Accuracy = 0.806640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8102103471755981, Accuracy = 0.7762128114700317\n",
            "Iter #2400256:  Learning rate = 0.001877:   Batch Loss = 0.739677, Accuracy = 0.826171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7988786101341248, Accuracy = 0.7856025099754333\n",
            "Iter #2404352:  Learning rate = 0.001877:   Batch Loss = 0.913723, Accuracy = 0.73828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9919551014900208, Accuracy = 0.6915318965911865\n",
            "Iter #2408448:  Learning rate = 0.001877:   Batch Loss = 0.854972, Accuracy = 0.75390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.9235449433326721, Accuracy = 0.7289167046546936\n",
            "Iter #2412544:  Learning rate = 0.001877:   Batch Loss = 0.871655, Accuracy = 0.748046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8754109740257263, Accuracy = 0.7541297078132629\n",
            "Iter #2416640:  Learning rate = 0.001877:   Batch Loss = 0.812104, Accuracy = 0.79296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.842604398727417, Accuracy = 0.7708224654197693\n",
            "Iter #2420736:  Learning rate = 0.001877:   Batch Loss = 0.884511, Accuracy = 0.744140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8073411583900452, Accuracy = 0.77377849817276\n",
            "Iter #2424832:  Learning rate = 0.001877:   Batch Loss = 0.787970, Accuracy = 0.794921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8342667818069458, Accuracy = 0.7711702585220337\n",
            "Iter #2428928:  Learning rate = 0.001877:   Batch Loss = 0.771470, Accuracy = 0.791015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.804055392742157, Accuracy = 0.7675186991691589\n",
            "Iter #2433024:  Learning rate = 0.001877:   Batch Loss = 0.819816, Accuracy = 0.767578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7876322269439697, Accuracy = 0.778125524520874\n",
            "Iter #2437120:  Learning rate = 0.001877:   Batch Loss = 0.796080, Accuracy = 0.79296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8123706579208374, Accuracy = 0.7763867378234863\n",
            "Iter #2441216:  Learning rate = 0.001877:   Batch Loss = 0.793427, Accuracy = 0.79296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7806191444396973, Accuracy = 0.7779516577720642\n",
            "Iter #2445312:  Learning rate = 0.001877:   Batch Loss = 0.746790, Accuracy = 0.802734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7945919036865234, Accuracy = 0.7868196964263916\n",
            "Iter #2449408:  Learning rate = 0.001877:   Batch Loss = 0.735093, Accuracy = 0.818359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8656771183013916, Accuracy = 0.7586506605148315\n",
            "Iter #2453504:  Learning rate = 0.001877:   Batch Loss = 0.814242, Accuracy = 0.7578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8107191324234009, Accuracy = 0.7619544267654419\n",
            "Iter #2457600:  Learning rate = 0.001877:   Batch Loss = 0.785550, Accuracy = 0.787109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8065341711044312, Accuracy = 0.7838636636734009\n",
            "Iter #2461696:  Learning rate = 0.001877:   Batch Loss = 0.764325, Accuracy = 0.78515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7700462341308594, Accuracy = 0.7755172848701477\n",
            "Iter #2465792:  Learning rate = 0.001877:   Batch Loss = 0.836240, Accuracy = 0.755859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8573094606399536, Accuracy = 0.7433489561080933\n",
            "Iter #2469888:  Learning rate = 0.001877:   Batch Loss = 0.769711, Accuracy = 0.802734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8128808736801147, Accuracy = 0.7723873853683472\n",
            "Iter #2473984:  Learning rate = 0.001877:   Batch Loss = 0.727635, Accuracy = 0.802734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8234052062034607, Accuracy = 0.7673448324203491\n",
            "Iter #2478080:  Learning rate = 0.001877:   Batch Loss = 0.743845, Accuracy = 0.771484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7958575487136841, Accuracy = 0.7669970393180847\n",
            "Iter #2482176:  Learning rate = 0.001877:   Batch Loss = 0.710118, Accuracy = 0.837890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7891933917999268, Accuracy = 0.7991653680801392\n",
            "Iter #2486272:  Learning rate = 0.001877:   Batch Loss = 0.709431, Accuracy = 0.83203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7449358701705933, Accuracy = 0.7913406491279602\n",
            "Iter #2490368:  Learning rate = 0.001877:   Batch Loss = 0.935360, Accuracy = 0.724609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8954685926437378, Accuracy = 0.7445661425590515\n",
            "Iter #2494464:  Learning rate = 0.001877:   Batch Loss = 0.739498, Accuracy = 0.80859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8476930856704712, Accuracy = 0.7395235896110535\n",
            "Iter #2498560:  Learning rate = 0.001877:   Batch Loss = 0.783591, Accuracy = 0.775390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.818882942199707, Accuracy = 0.7656059861183167\n",
            "Iter #2502656:  Learning rate = 0.001802:   Batch Loss = 0.813701, Accuracy = 0.732421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8455162048339844, Accuracy = 0.7716918587684631\n",
            "Iter #2506752:  Learning rate = 0.001802:   Batch Loss = 0.749224, Accuracy = 0.775390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8113963603973389, Accuracy = 0.7652581930160522\n",
            "Iter #2510848:  Learning rate = 0.001802:   Batch Loss = 0.777960, Accuracy = 0.77734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7747121453285217, Accuracy = 0.774474024772644\n",
            "Iter #2514944:  Learning rate = 0.001802:   Batch Loss = 0.686444, Accuracy = 0.833984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7265828847885132, Accuracy = 0.8195096254348755\n",
            "Iter #2519040:  Learning rate = 0.001802:   Batch Loss = 0.727069, Accuracy = 0.8046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.8658040761947632, Accuracy = 0.7501304149627686\n",
            "Iter #2523136:  Learning rate = 0.001802:   Batch Loss = 0.692858, Accuracy = 0.830078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7635585069656372, Accuracy = 0.7876890897750854\n",
            "Iter #2527232:  Learning rate = 0.001802:   Batch Loss = 0.748807, Accuracy = 0.7734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.764625072479248, Accuracy = 0.7722135186195374\n",
            "Iter #2531328:  Learning rate = 0.001802:   Batch Loss = 0.717047, Accuracy = 0.8203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7492958307266235, Accuracy = 0.8179447054862976\n",
            "Iter #2535424:  Learning rate = 0.001802:   Batch Loss = 0.695936, Accuracy = 0.833984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7200303673744202, Accuracy = 0.8108155131340027\n",
            "Iter #2539520:  Learning rate = 0.001802:   Batch Loss = 0.779463, Accuracy = 0.806640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7896779179573059, Accuracy = 0.7896018028259277\n",
            "Iter #2543616:  Learning rate = 0.001802:   Batch Loss = 0.740313, Accuracy = 0.8125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7307841181755066, Accuracy = 0.8132498860359192\n",
            "Iter #2547712:  Learning rate = 0.001802:   Batch Loss = 0.687275, Accuracy = 0.837890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7010217308998108, Accuracy = 0.8283776640892029\n",
            "Iter #2551808:  Learning rate = 0.001802:   Batch Loss = 0.633367, Accuracy = 0.853515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6826195120811462, Accuracy = 0.8362023830413818\n",
            "Iter #2555904:  Learning rate = 0.001802:   Batch Loss = 0.616812, Accuracy = 0.861328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7255619764328003, Accuracy = 0.8089028000831604\n",
            "Iter #2560000:  Learning rate = 0.001802:   Batch Loss = 0.676668, Accuracy = 0.837890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6874545812606812, Accuracy = 0.8362023830413818\n",
            "Iter #2564096:  Learning rate = 0.001802:   Batch Loss = 0.636466, Accuracy = 0.83984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7128162980079651, Accuracy = 0.8209007382392883\n",
            "Iter #2568192:  Learning rate = 0.001802:   Batch Loss = 0.622705, Accuracy = 0.859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7248187065124512, Accuracy = 0.7991653680801392\n",
            "Iter #2572288:  Learning rate = 0.001802:   Batch Loss = 0.649550, Accuracy = 0.849609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7494093179702759, Accuracy = 0.8028169274330139\n",
            "Iter #2576384:  Learning rate = 0.001802:   Batch Loss = 0.677977, Accuracy = 0.802734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7805323600769043, Accuracy = 0.7934272289276123\n",
            "Iter #2580480:  Learning rate = 0.001802:   Batch Loss = 0.658619, Accuracy = 0.83984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7030924558639526, Accuracy = 0.8179447054862976\n",
            "Iter #2584576:  Learning rate = 0.001802:   Batch Loss = 0.671192, Accuracy = 0.830078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7214721441268921, Accuracy = 0.8089028000831604\n",
            "Iter #2588672:  Learning rate = 0.001802:   Batch Loss = 0.691834, Accuracy = 0.818359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6860111951828003, Accuracy = 0.8245522379875183\n",
            "Iter #2592768:  Learning rate = 0.001802:   Batch Loss = 0.646049, Accuracy = 0.83984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.687647819519043, Accuracy = 0.8322030901908875\n",
            "Iter #2596864:  Learning rate = 0.001802:   Batch Loss = 0.629965, Accuracy = 0.853515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6523973941802979, Accuracy = 0.850113034248352\n",
            "Iter #2600960:  Learning rate = 0.001730:   Batch Loss = 0.612731, Accuracy = 0.841796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6661248803138733, Accuracy = 0.8327247500419617\n",
            "Iter #2605056:  Learning rate = 0.001730:   Batch Loss = 0.607346, Accuracy = 0.85546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6678775548934937, Accuracy = 0.8374195694923401\n",
            "Iter #2609152:  Learning rate = 0.001730:   Batch Loss = 0.607700, Accuracy = 0.861328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6727057695388794, Accuracy = 0.8386367559432983\n",
            "Iter #2613248:  Learning rate = 0.001730:   Batch Loss = 0.624299, Accuracy = 0.859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6617701649665833, Accuracy = 0.845244288444519\n",
            "Iter #2617344:  Learning rate = 0.001730:   Batch Loss = 0.645678, Accuracy = 0.8359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7067369222640991, Accuracy = 0.8148148059844971\n",
            "Iter #2621440:  Learning rate = 0.001730:   Batch Loss = 0.688792, Accuracy = 0.8203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7468585968017578, Accuracy = 0.8120326995849609\n",
            "Iter #2625536:  Learning rate = 0.001730:   Batch Loss = 0.632385, Accuracy = 0.859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6532298922538757, Accuracy = 0.8485480546951294\n",
            "Iter #2629632:  Learning rate = 0.001730:   Batch Loss = 0.630961, Accuracy = 0.8515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6628984212875366, Accuracy = 0.8464614748954773\n",
            "Iter #2633728:  Learning rate = 0.001730:   Batch Loss = 0.614081, Accuracy = 0.87890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6913806200027466, Accuracy = 0.830464243888855\n",
            "Iter #2637824:  Learning rate = 0.001730:   Batch Loss = 0.653675, Accuracy = 0.818359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.659104585647583, Accuracy = 0.8381150960922241\n",
            "Iter #2641920:  Learning rate = 0.001730:   Batch Loss = 0.588318, Accuracy = 0.873046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6426773071289062, Accuracy = 0.8506346940994263\n",
            "Iter #2646016:  Learning rate = 0.001730:   Batch Loss = 0.621103, Accuracy = 0.857421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6435235142707825, Accuracy = 0.8461137413978577\n",
            "Iter #2650112:  Learning rate = 0.001730:   Batch Loss = 0.619242, Accuracy = 0.85546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.716591477394104, Accuracy = 0.8228134512901306\n",
            "Iter #2654208:  Learning rate = 0.001730:   Batch Loss = 0.606740, Accuracy = 0.86328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7058501243591309, Accuracy = 0.8224656581878662\n",
            "Iter #2658304:  Learning rate = 0.001730:   Batch Loss = 0.599996, Accuracy = 0.87109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.662961483001709, Accuracy = 0.8412449955940247\n",
            "Iter #2662400:  Learning rate = 0.001730:   Batch Loss = 0.619427, Accuracy = 0.84375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7027658224105835, Accuracy = 0.8328986167907715\n",
            "Iter #2666496:  Learning rate = 0.001730:   Batch Loss = 0.730991, Accuracy = 0.80859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.755638837814331, Accuracy = 0.8026430010795593\n",
            "Iter #2670592:  Learning rate = 0.001730:   Batch Loss = 0.692232, Accuracy = 0.8359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7294230461120605, Accuracy = 0.8146409392356873\n",
            "Iter #2674688:  Learning rate = 0.001730:   Batch Loss = 0.608025, Accuracy = 0.87890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7416552901268005, Accuracy = 0.8158581256866455\n",
            "Iter #2678784:  Learning rate = 0.001730:   Batch Loss = 0.686714, Accuracy = 0.83984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7287898063659668, Accuracy = 0.8073378801345825\n",
            "Iter #2682880:  Learning rate = 0.001730:   Batch Loss = 0.585322, Accuracy = 0.865234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6671735644340515, Accuracy = 0.8377673625946045\n",
            "Iter #2686976:  Learning rate = 0.001730:   Batch Loss = 0.645202, Accuracy = 0.8359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6638205051422119, Accuracy = 0.840897262096405\n",
            "Iter #2691072:  Learning rate = 0.001730:   Batch Loss = 0.658837, Accuracy = 0.84765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7199382781982422, Accuracy = 0.8174230456352234\n",
            "Iter #2695168:  Learning rate = 0.001730:   Batch Loss = 0.571857, Accuracy = 0.904296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6671560406684875, Accuracy = 0.8436793684959412\n",
            "Iter #2699264:  Learning rate = 0.001730:   Batch Loss = 0.673953, Accuracy = 0.826171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6898316144943237, Accuracy = 0.8334202766418457\n",
            "Iter #2703360:  Learning rate = 0.001661:   Batch Loss = 0.643393, Accuracy = 0.83984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6389821171760559, Accuracy = 0.8462876081466675\n",
            "Iter #2707456:  Learning rate = 0.001661:   Batch Loss = 0.560483, Accuracy = 0.880859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6693731546401978, Accuracy = 0.8429838418960571\n",
            "Iter #2711552:  Learning rate = 0.001661:   Batch Loss = 0.600317, Accuracy = 0.861328125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6827034950256348, Accuracy = 0.8245522379875183\n",
            "Iter #2715648:  Learning rate = 0.001661:   Batch Loss = 0.603317, Accuracy = 0.857421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6400746703147888, Accuracy = 0.8555033802986145\n",
            "Iter #2719744:  Learning rate = 0.001661:   Batch Loss = 0.577189, Accuracy = 0.873046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6162116527557373, Accuracy = 0.857242226600647\n",
            "Iter #2723840:  Learning rate = 0.001661:   Batch Loss = 0.623194, Accuracy = 0.859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.7452083826065063, Accuracy = 0.826117217540741\n",
            "Iter #2727936:  Learning rate = 0.001661:   Batch Loss = 0.565190, Accuracy = 0.869140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6343185901641846, Accuracy = 0.853069007396698\n",
            "Iter #2732032:  Learning rate = 0.001661:   Batch Loss = 0.536424, Accuracy = 0.888671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6440147161483765, Accuracy = 0.857242226600647\n",
            "Iter #2736128:  Learning rate = 0.001661:   Batch Loss = 0.572328, Accuracy = 0.875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6161454916000366, Accuracy = 0.8605459928512573\n",
            "Iter #2740224:  Learning rate = 0.001661:   Batch Loss = 0.577928, Accuracy = 0.890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6242294311523438, Accuracy = 0.8542861938476562\n",
            "Iter #2744320:  Learning rate = 0.001661:   Batch Loss = 0.509095, Accuracy = 0.9140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6059516668319702, Accuracy = 0.8614153861999512\n",
            "Iter #2748416:  Learning rate = 0.001661:   Batch Loss = 0.554609, Accuracy = 0.875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6396015882492065, Accuracy = 0.8480264544487\n",
            "Iter #2752512:  Learning rate = 0.001661:   Batch Loss = 0.530052, Accuracy = 0.89453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6212937235832214, Accuracy = 0.8542861938476562\n",
            "Iter #2756608:  Learning rate = 0.001661:   Batch Loss = 0.503451, Accuracy = 0.900390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6342713832855225, Accuracy = 0.853764533996582\n",
            "Iter #2760704:  Learning rate = 0.001661:   Batch Loss = 0.527158, Accuracy = 0.90234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6173540353775024, Accuracy = 0.8575899600982666\n",
            "Iter #2764800:  Learning rate = 0.001661:   Batch Loss = 0.533117, Accuracy = 0.89453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6383262872695923, Accuracy = 0.852373480796814\n",
            "Iter #2768896:  Learning rate = 0.001661:   Batch Loss = 0.558039, Accuracy = 0.875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6301323175430298, Accuracy = 0.8600243330001831\n",
            "Iter #2772992:  Learning rate = 0.001661:   Batch Loss = 0.504822, Accuracy = 0.8984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.650276780128479, Accuracy = 0.8488958477973938\n",
            "Iter #2777088:  Learning rate = 0.001661:   Batch Loss = 0.545412, Accuracy = 0.87890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6334764361381531, Accuracy = 0.852373480796814\n",
            "Iter #2781184:  Learning rate = 0.001661:   Batch Loss = 0.576840, Accuracy = 0.849609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6163859367370605, Accuracy = 0.8612415194511414\n",
            "Iter #2785280:  Learning rate = 0.001661:   Batch Loss = 0.541511, Accuracy = 0.87890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6171320676803589, Accuracy = 0.8469831347465515\n",
            "Iter #2789376:  Learning rate = 0.001661:   Batch Loss = 0.562236, Accuracy = 0.880859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6353933215141296, Accuracy = 0.8497652411460876\n",
            "Iter #2793472:  Learning rate = 0.001661:   Batch Loss = 0.619809, Accuracy = 0.873046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6707499027252197, Accuracy = 0.8433315753936768\n",
            "Iter #2797568:  Learning rate = 0.001661:   Batch Loss = 0.548478, Accuracy = 0.892578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6407041549682617, Accuracy = 0.8535906672477722\n",
            "Iter #2801664:  Learning rate = 0.001594:   Batch Loss = 0.547253, Accuracy = 0.875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6480748057365417, Accuracy = 0.8506346940994263\n",
            "Iter #2805760:  Learning rate = 0.001594:   Batch Loss = 0.594068, Accuracy = 0.873046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6536165475845337, Accuracy = 0.8424621820449829\n",
            "Iter #2809856:  Learning rate = 0.001594:   Batch Loss = 0.541441, Accuracy = 0.888671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6372476816177368, Accuracy = 0.8513302206993103\n",
            "Iter #2813952:  Learning rate = 0.001594:   Batch Loss = 0.511894, Accuracy = 0.89453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6221586465835571, Accuracy = 0.8443748950958252\n",
            "Iter #2818048:  Learning rate = 0.001594:   Batch Loss = 0.530315, Accuracy = 0.8984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5998207926750183, Accuracy = 0.8676751852035522\n",
            "Iter #2822144:  Learning rate = 0.001594:   Batch Loss = 0.529623, Accuracy = 0.90625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6260474920272827, Accuracy = 0.8502869009971619\n",
            "Iter #2826240:  Learning rate = 0.001594:   Batch Loss = 0.556376, Accuracy = 0.88671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5906776189804077, Accuracy = 0.8687185049057007\n",
            "Iter #2830336:  Learning rate = 0.001594:   Batch Loss = 0.513832, Accuracy = 0.908203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5902976393699646, Accuracy = 0.8746305108070374\n",
            "Iter #2834432:  Learning rate = 0.001594:   Batch Loss = 0.501547, Accuracy = 0.91015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6342169642448425, Accuracy = 0.844548761844635\n",
            "Iter #2838528:  Learning rate = 0.001594:   Batch Loss = 0.559448, Accuracy = 0.873046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5885905027389526, Accuracy = 0.8716744780540466\n",
            "Iter #2842624:  Learning rate = 0.001594:   Batch Loss = 0.547988, Accuracy = 0.89453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5946099758148193, Accuracy = 0.8708050847053528\n",
            "Iter #2846720:  Learning rate = 0.001594:   Batch Loss = 0.515049, Accuracy = 0.896484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6077805757522583, Accuracy = 0.8582854866981506\n",
            "Iter #2850816:  Learning rate = 0.001594:   Batch Loss = 0.487559, Accuracy = 0.91796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6011866331100464, Accuracy = 0.8681968450546265\n",
            "Iter #2854912:  Learning rate = 0.001594:   Batch Loss = 0.546947, Accuracy = 0.892578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6120392680168152, Accuracy = 0.8516779541969299\n",
            "Iter #2859008:  Learning rate = 0.001594:   Batch Loss = 0.528123, Accuracy = 0.87890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5929186344146729, Accuracy = 0.8614153861999512\n",
            "Iter #2863104:  Learning rate = 0.001594:   Batch Loss = 0.502522, Accuracy = 0.9140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6105999946594238, Accuracy = 0.8615893125534058\n",
            "Iter #2867200:  Learning rate = 0.001594:   Batch Loss = 0.591452, Accuracy = 0.87109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5911825895309448, Accuracy = 0.8638497591018677\n",
            "Iter #2871296:  Learning rate = 0.001594:   Batch Loss = 0.577221, Accuracy = 0.876953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5960444211959839, Accuracy = 0.8614153861999512\n",
            "Iter #2875392:  Learning rate = 0.001594:   Batch Loss = 0.559854, Accuracy = 0.888671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6261246204376221, Accuracy = 0.8464614748954773\n",
            "Iter #2879488:  Learning rate = 0.001594:   Batch Loss = 0.516926, Accuracy = 0.8984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6200215816497803, Accuracy = 0.8591549396514893\n",
            "Iter #2883584:  Learning rate = 0.001594:   Batch Loss = 0.546507, Accuracy = 0.892578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6059873104095459, Accuracy = 0.8643714189529419\n",
            "Iter #2887680:  Learning rate = 0.001594:   Batch Loss = 0.548072, Accuracy = 0.90625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6084240674972534, Accuracy = 0.8661102652549744\n",
            "Iter #2891776:  Learning rate = 0.001594:   Batch Loss = 0.552266, Accuracy = 0.87109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5818546414375305, Accuracy = 0.8654147386550903\n",
            "Iter #2895872:  Learning rate = 0.001594:   Batch Loss = 0.507988, Accuracy = 0.904296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5932638645172119, Accuracy = 0.8709789514541626\n",
            "Iter #2899968:  Learning rate = 0.001594:   Batch Loss = 0.546634, Accuracy = 0.88671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5764390230178833, Accuracy = 0.8767170906066895\n",
            "Iter #2904064:  Learning rate = 0.001531:   Batch Loss = 0.586987, Accuracy = 0.869140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5924007892608643, Accuracy = 0.8701095581054688\n",
            "Iter #2908160:  Learning rate = 0.001531:   Batch Loss = 0.490155, Accuracy = 0.8984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6202017068862915, Accuracy = 0.8617631793022156\n",
            "Iter #2912256:  Learning rate = 0.001531:   Batch Loss = 0.497037, Accuracy = 0.900390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.603877604007721, Accuracy = 0.8648930788040161\n",
            "Iter #2916352:  Learning rate = 0.001531:   Batch Loss = 0.544943, Accuracy = 0.876953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5917644500732422, Accuracy = 0.853069007396698\n",
            "Iter #2920448:  Learning rate = 0.001531:   Batch Loss = 0.555899, Accuracy = 0.875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5771059989929199, Accuracy = 0.8760215640068054\n",
            "Iter #2924544:  Learning rate = 0.001531:   Batch Loss = 0.556743, Accuracy = 0.8828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5791692137718201, Accuracy = 0.8751521706581116\n",
            "Iter #2928640:  Learning rate = 0.001531:   Batch Loss = 0.534905, Accuracy = 0.87109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6119594573974609, Accuracy = 0.8600243330001831\n",
            "Iter #2932736:  Learning rate = 0.001531:   Batch Loss = 0.558062, Accuracy = 0.88671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6142497062683105, Accuracy = 0.8704572916030884\n",
            "Iter #2936832:  Learning rate = 0.001531:   Batch Loss = 0.559175, Accuracy = 0.904296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6418443918228149, Accuracy = 0.835332989692688\n",
            "Iter #2940928:  Learning rate = 0.001531:   Batch Loss = 0.534824, Accuracy = 0.896484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5814380645751953, Accuracy = 0.8629803657531738\n",
            "Iter #2945024:  Learning rate = 0.001531:   Batch Loss = 0.532605, Accuracy = 0.900390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.568461000919342, Accuracy = 0.8732394576072693\n",
            "Iter #2949120:  Learning rate = 0.001531:   Batch Loss = 0.475564, Accuracy = 0.91796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.570888876914978, Accuracy = 0.8704572916030884\n",
            "Iter #2953216:  Learning rate = 0.001531:   Batch Loss = 0.520069, Accuracy = 0.884765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5617678761482239, Accuracy = 0.8810641765594482\n",
            "Iter #2957312:  Learning rate = 0.001531:   Batch Loss = 0.501260, Accuracy = 0.8984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5836721062660217, Accuracy = 0.8767170906066895\n",
            "Iter #2961408:  Learning rate = 0.001531:   Batch Loss = 0.530866, Accuracy = 0.900390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5788556337356567, Accuracy = 0.8708050847053528\n",
            "Iter #2965504:  Learning rate = 0.001531:   Batch Loss = 0.527534, Accuracy = 0.896484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5797264575958252, Accuracy = 0.8690662384033203\n",
            "Iter #2969600:  Learning rate = 0.001531:   Batch Loss = 0.542671, Accuracy = 0.888671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5626113414764404, Accuracy = 0.8765432238578796\n",
            "Iter #2973696:  Learning rate = 0.001531:   Batch Loss = 0.526240, Accuracy = 0.90625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5655134916305542, Accuracy = 0.8732394576072693\n",
            "Iter #2977792:  Learning rate = 0.001531:   Batch Loss = 0.514561, Accuracy = 0.892578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5552917122840881, Accuracy = 0.8857589960098267\n",
            "Iter #2981888:  Learning rate = 0.001531:   Batch Loss = 0.427711, Accuracy = 0.927734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5641870498657227, Accuracy = 0.8843679428100586\n",
            "Iter #2985984:  Learning rate = 0.001531:   Batch Loss = 0.494956, Accuracy = 0.91015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.555412769317627, Accuracy = 0.8826290965080261\n",
            "Iter #2990080:  Learning rate = 0.001531:   Batch Loss = 0.544050, Accuracy = 0.88671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6172783374786377, Accuracy = 0.8544601202011108\n",
            "Iter #2994176:  Learning rate = 0.001531:   Batch Loss = 0.510914, Accuracy = 0.892578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.569549024105072, Accuracy = 0.8614153861999512\n",
            "Iter #2998272:  Learning rate = 0.001531:   Batch Loss = 0.497883, Accuracy = 0.90234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5669160485267639, Accuracy = 0.8728916645050049\n",
            "Iter #3002368:  Learning rate = 0.001469:   Batch Loss = 0.511780, Accuracy = 0.892578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5531933307647705, Accuracy = 0.8812380433082581\n",
            "Iter #3006464:  Learning rate = 0.001469:   Batch Loss = 0.506096, Accuracy = 0.89453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5518549680709839, Accuracy = 0.8817597031593323\n",
            "Iter #3010560:  Learning rate = 0.001469:   Batch Loss = 0.486049, Accuracy = 0.91796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5474278926849365, Accuracy = 0.8817597031593323\n",
            "Iter #3014656:  Learning rate = 0.001469:   Batch Loss = 0.554831, Accuracy = 0.888671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5545902252197266, Accuracy = 0.8836724162101746\n",
            "Iter #3018752:  Learning rate = 0.001469:   Batch Loss = 0.464885, Accuracy = 0.921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5504418015480042, Accuracy = 0.8833246231079102\n",
            "Iter #3022848:  Learning rate = 0.001469:   Batch Loss = 0.485012, Accuracy = 0.912109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5420304536819458, Accuracy = 0.8864545226097107\n",
            "Iter #3026944:  Learning rate = 0.001469:   Batch Loss = 0.480764, Accuracy = 0.921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5517087578773499, Accuracy = 0.8859328627586365\n",
            "Iter #3031040:  Learning rate = 0.001469:   Batch Loss = 0.456951, Accuracy = 0.91796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5599806904792786, Accuracy = 0.8812380433082581\n",
            "Iter #3035136:  Learning rate = 0.001469:   Batch Loss = 0.501407, Accuracy = 0.896484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.579766035079956, Accuracy = 0.8728916645050049\n",
            "Iter #3039232:  Learning rate = 0.001469:   Batch Loss = 0.537827, Accuracy = 0.88671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5665745735168457, Accuracy = 0.8788036704063416\n",
            "Iter #3043328:  Learning rate = 0.001469:   Batch Loss = 0.521497, Accuracy = 0.908203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5726344585418701, Accuracy = 0.8669796586036682\n",
            "Iter #3047424:  Learning rate = 0.001469:   Batch Loss = 0.541028, Accuracy = 0.880859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5921502113342285, Accuracy = 0.8753260374069214\n",
            "Iter #3051520:  Learning rate = 0.001469:   Batch Loss = 0.497142, Accuracy = 0.912109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5381296873092651, Accuracy = 0.8857589960098267\n",
            "Iter #3055616:  Learning rate = 0.001469:   Batch Loss = 0.452214, Accuracy = 0.9296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5765748023986816, Accuracy = 0.8819335699081421\n",
            "Iter #3059712:  Learning rate = 0.001469:   Batch Loss = 0.445821, Accuracy = 0.9296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5317260026931763, Accuracy = 0.8833246231079102\n",
            "Iter #3063808:  Learning rate = 0.001469:   Batch Loss = 0.457637, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5313053131103516, Accuracy = 0.8880195021629333\n",
            "Iter #3067904:  Learning rate = 0.001469:   Batch Loss = 0.495292, Accuracy = 0.912109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5413343906402588, Accuracy = 0.8840201497077942\n",
            "Iter #3072000:  Learning rate = 0.001469:   Batch Loss = 0.438156, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5378284454345703, Accuracy = 0.8850634694099426\n",
            "Iter #3076096:  Learning rate = 0.001469:   Batch Loss = 0.450816, Accuracy = 0.9296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5634673833847046, Accuracy = 0.8794991970062256\n",
            "Iter #3080192:  Learning rate = 0.001469:   Batch Loss = 0.463486, Accuracy = 0.921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.538067638874054, Accuracy = 0.8772387504577637\n",
            "Iter #3084288:  Learning rate = 0.001469:   Batch Loss = 0.457908, Accuracy = 0.923828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5476381778717041, Accuracy = 0.8732394576072693\n",
            "Iter #3088384:  Learning rate = 0.001469:   Batch Loss = 0.464137, Accuracy = 0.919921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5804606080055237, Accuracy = 0.8690662384033203\n",
            "Iter #3092480:  Learning rate = 0.001469:   Batch Loss = 0.463664, Accuracy = 0.91796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5598666667938232, Accuracy = 0.8821074366569519\n",
            "Iter #3096576:  Learning rate = 0.001469:   Batch Loss = 0.488626, Accuracy = 0.90625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5481011867523193, Accuracy = 0.8841940760612488\n",
            "Iter #3100672:  Learning rate = 0.001411:   Batch Loss = 0.483276, Accuracy = 0.91015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5516849756240845, Accuracy = 0.8808902502059937\n",
            "Iter #3104768:  Learning rate = 0.001411:   Batch Loss = 0.446475, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.539645791053772, Accuracy = 0.8822813630104065\n",
            "Iter #3108864:  Learning rate = 0.001411:   Batch Loss = 0.480691, Accuracy = 0.908203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5319042205810547, Accuracy = 0.8941053748130798\n",
            "Iter #3112960:  Learning rate = 0.001411:   Batch Loss = 0.493213, Accuracy = 0.90625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5382571816444397, Accuracy = 0.889062762260437\n",
            "Iter #3117056:  Learning rate = 0.001411:   Batch Loss = 0.450966, Accuracy = 0.923828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5765369534492493, Accuracy = 0.871326744556427\n",
            "Iter #3121152:  Learning rate = 0.001411:   Batch Loss = 0.504371, Accuracy = 0.904296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.656906247138977, Accuracy = 0.8608937859535217\n",
            "Iter #3125248:  Learning rate = 0.001411:   Batch Loss = 0.562632, Accuracy = 0.890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6323246359825134, Accuracy = 0.8633280992507935\n",
            "Iter #3129344:  Learning rate = 0.001411:   Batch Loss = 0.496776, Accuracy = 0.916015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6080257892608643, Accuracy = 0.8690662384033203\n",
            "Iter #3133440:  Learning rate = 0.001411:   Batch Loss = 0.474389, Accuracy = 0.91796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.55169677734375, Accuracy = 0.8840201497077942\n",
            "Iter #3137536:  Learning rate = 0.001411:   Batch Loss = 0.512247, Accuracy = 0.904296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5457360744476318, Accuracy = 0.8847156763076782\n",
            "Iter #3141632:  Learning rate = 0.001411:   Batch Loss = 0.561530, Accuracy = 0.900390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5857237577438354, Accuracy = 0.8725439310073853\n",
            "Iter #3145728:  Learning rate = 0.001411:   Batch Loss = 0.517859, Accuracy = 0.90234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5607030987739563, Accuracy = 0.8796731233596802\n",
            "Iter #3149824:  Learning rate = 0.001411:   Batch Loss = 0.501177, Accuracy = 0.904296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5514036417007446, Accuracy = 0.8803686499595642\n",
            "Iter #3153920:  Learning rate = 0.001411:   Batch Loss = 0.482662, Accuracy = 0.908203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5504935383796692, Accuracy = 0.8904538154602051\n",
            "Iter #3158016:  Learning rate = 0.001411:   Batch Loss = 0.486313, Accuracy = 0.908203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5479863882064819, Accuracy = 0.8930620551109314\n",
            "Iter #3162112:  Learning rate = 0.001411:   Batch Loss = 0.495474, Accuracy = 0.91015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5293092131614685, Accuracy = 0.8920187950134277\n",
            "Iter #3166208:  Learning rate = 0.001411:   Batch Loss = 0.457324, Accuracy = 0.927734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5369373559951782, Accuracy = 0.8831507563591003\n",
            "Iter #3170304:  Learning rate = 0.001411:   Batch Loss = 0.467958, Accuracy = 0.921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5377088189125061, Accuracy = 0.8944531679153442\n",
            "Iter #3174400:  Learning rate = 0.001411:   Batch Loss = 0.475723, Accuracy = 0.916015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.523352861404419, Accuracy = 0.8941053748130798\n",
            "Iter #3178496:  Learning rate = 0.001411:   Batch Loss = 0.407723, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5307698845863342, Accuracy = 0.8864545226097107\n",
            "Iter #3182592:  Learning rate = 0.001411:   Batch Loss = 0.478033, Accuracy = 0.9140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5348659753799438, Accuracy = 0.8868023157119751\n",
            "Iter #3186688:  Learning rate = 0.001411:   Batch Loss = 0.510488, Accuracy = 0.880859375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5201754570007324, Accuracy = 0.8941053748130798\n",
            "Iter #3190784:  Learning rate = 0.001411:   Batch Loss = 0.467103, Accuracy = 0.91015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5755248069763184, Accuracy = 0.8845418095588684\n",
            "Iter #3194880:  Learning rate = 0.001411:   Batch Loss = 0.482135, Accuracy = 0.916015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5205035209655762, Accuracy = 0.8873239159584045\n",
            "Iter #3198976:  Learning rate = 0.001411:   Batch Loss = 0.437309, Accuracy = 0.931640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5157638192176819, Accuracy = 0.8923665285110474\n",
            "Iter #3203072:  Learning rate = 0.001354:   Batch Loss = 0.437074, Accuracy = 0.931640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5032491087913513, Accuracy = 0.8937575817108154\n",
            "Iter #3207168:  Learning rate = 0.001354:   Batch Loss = 0.403645, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5191500186920166, Accuracy = 0.8986263275146484\n",
            "Iter #3211264:  Learning rate = 0.001354:   Batch Loss = 0.451677, Accuracy = 0.923828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5306509733200073, Accuracy = 0.8948009014129639\n",
            "Iter #3215360:  Learning rate = 0.001354:   Batch Loss = 0.431133, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5252708792686462, Accuracy = 0.888367235660553\n",
            "Iter #3219456:  Learning rate = 0.001354:   Batch Loss = 0.453490, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5564159154891968, Accuracy = 0.8852373361587524\n",
            "Iter #3223552:  Learning rate = 0.001354:   Batch Loss = 0.470440, Accuracy = 0.912109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5364047288894653, Accuracy = 0.8902799487113953\n",
            "Iter #3227648:  Learning rate = 0.001354:   Batch Loss = 0.410576, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5303487777709961, Accuracy = 0.889062762260437\n",
            "Iter #3231744:  Learning rate = 0.001354:   Batch Loss = 0.434694, Accuracy = 0.9296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5281516909599304, Accuracy = 0.8847156763076782\n",
            "Iter #3235840:  Learning rate = 0.001354:   Batch Loss = 0.482661, Accuracy = 0.90234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.516552209854126, Accuracy = 0.8899322152137756\n",
            "Iter #3239936:  Learning rate = 0.001354:   Batch Loss = 0.441096, Accuracy = 0.9296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5131711959838867, Accuracy = 0.8970614075660706\n",
            "Iter #3244032:  Learning rate = 0.001354:   Batch Loss = 0.530091, Accuracy = 0.90234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5510586500167847, Accuracy = 0.8781081438064575\n",
            "Iter #3248128:  Learning rate = 0.001354:   Batch Loss = 0.459672, Accuracy = 0.923828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5593937039375305, Accuracy = 0.8767170906066895\n",
            "Iter #3252224:  Learning rate = 0.001354:   Batch Loss = 0.470446, Accuracy = 0.919921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5526341199874878, Accuracy = 0.8819335699081421\n",
            "Iter #3256320:  Learning rate = 0.001354:   Batch Loss = 0.468366, Accuracy = 0.9140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5253412127494812, Accuracy = 0.8871500492095947\n",
            "Iter #3260416:  Learning rate = 0.001354:   Batch Loss = 0.462008, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5102089643478394, Accuracy = 0.8948009014129639\n",
            "Iter #3264512:  Learning rate = 0.001354:   Batch Loss = 0.503195, Accuracy = 0.904296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5246222019195557, Accuracy = 0.8843679428100586\n",
            "Iter #3268608:  Learning rate = 0.001354:   Batch Loss = 0.500938, Accuracy = 0.904296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5380014181137085, Accuracy = 0.8871500492095947\n",
            "Iter #3272704:  Learning rate = 0.001354:   Batch Loss = 0.416371, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5251586437225342, Accuracy = 0.8833246231079102\n",
            "Iter #3276800:  Learning rate = 0.001354:   Batch Loss = 0.453333, Accuracy = 0.91796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5449093580245972, Accuracy = 0.8836724162101746\n",
            "Iter #3280896:  Learning rate = 0.001354:   Batch Loss = 0.480400, Accuracy = 0.908203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5036091804504395, Accuracy = 0.8984524607658386\n",
            "Iter #3284992:  Learning rate = 0.001354:   Batch Loss = 0.464378, Accuracy = 0.91796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5355139970779419, Accuracy = 0.8914971351623535\n",
            "Iter #3289088:  Learning rate = 0.001354:   Batch Loss = 0.483450, Accuracy = 0.904296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.569036602973938, Accuracy = 0.8728916645050049\n",
            "Iter #3293184:  Learning rate = 0.001354:   Batch Loss = 0.479314, Accuracy = 0.908203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5132215023040771, Accuracy = 0.8848896026611328\n",
            "Iter #3297280:  Learning rate = 0.001354:   Batch Loss = 0.434555, Accuracy = 0.931640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5332698225975037, Accuracy = 0.8908016085624695\n",
            "Iter #3301376:  Learning rate = 0.001300:   Batch Loss = 0.439404, Accuracy = 0.916015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5202707648277283, Accuracy = 0.8902799487113953\n",
            "Iter #3305472:  Learning rate = 0.001300:   Batch Loss = 0.489534, Accuracy = 0.892578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5173598527908325, Accuracy = 0.8838462829589844\n",
            "Iter #3309568:  Learning rate = 0.001300:   Batch Loss = 0.572447, Accuracy = 0.865234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5459363460540771, Accuracy = 0.8838462829589844\n",
            "Iter #3313664:  Learning rate = 0.001300:   Batch Loss = 0.449262, Accuracy = 0.9296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5500130653381348, Accuracy = 0.8930620551109314\n",
            "Iter #3317760:  Learning rate = 0.001300:   Batch Loss = 0.467460, Accuracy = 0.912109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5235701203346252, Accuracy = 0.8871500492095947\n",
            "Iter #3321856:  Learning rate = 0.001300:   Batch Loss = 0.482840, Accuracy = 0.90234375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5160567760467529, Accuracy = 0.89393150806427\n",
            "Iter #3325952:  Learning rate = 0.001300:   Batch Loss = 0.454864, Accuracy = 0.9140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5254213809967041, Accuracy = 0.8874978423118591\n",
            "Iter #3330048:  Learning rate = 0.001300:   Batch Loss = 0.442816, Accuracy = 0.927734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5376836061477661, Accuracy = 0.8906277418136597\n",
            "Iter #3334144:  Learning rate = 0.001300:   Batch Loss = 0.451961, Accuracy = 0.923828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5132979154586792, Accuracy = 0.8909754753112793\n",
            "Iter #3338240:  Learning rate = 0.001300:   Batch Loss = 0.444163, Accuracy = 0.9140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.535061240196228, Accuracy = 0.8800208568572998\n",
            "Iter #3342336:  Learning rate = 0.001300:   Batch Loss = 0.465158, Accuracy = 0.908203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5396738052368164, Accuracy = 0.885411262512207\n",
            "Iter #3346432:  Learning rate = 0.001300:   Batch Loss = 0.470527, Accuracy = 0.912109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5425689220428467, Accuracy = 0.8908016085624695\n",
            "Iter #3350528:  Learning rate = 0.001300:   Batch Loss = 0.416763, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5220469236373901, Accuracy = 0.8847156763076782\n",
            "Iter #3354624:  Learning rate = 0.001300:   Batch Loss = 0.459120, Accuracy = 0.921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.537998616695404, Accuracy = 0.8838462829589844\n",
            "Iter #3358720:  Learning rate = 0.001300:   Batch Loss = 0.471813, Accuracy = 0.9140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5103956460952759, Accuracy = 0.8913232684135437\n",
            "Iter #3362816:  Learning rate = 0.001300:   Batch Loss = 0.501164, Accuracy = 0.892578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5221537351608276, Accuracy = 0.8902799487113953\n",
            "Iter #3366912:  Learning rate = 0.001300:   Batch Loss = 0.423218, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5079476833343506, Accuracy = 0.8909754753112793\n",
            "Iter #3371008:  Learning rate = 0.001300:   Batch Loss = 0.428242, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5125678181648254, Accuracy = 0.8965397477149963\n",
            "Iter #3375104:  Learning rate = 0.001300:   Batch Loss = 0.537746, Accuracy = 0.892578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5327550172805786, Accuracy = 0.8848896026611328\n",
            "Iter #3379200:  Learning rate = 0.001300:   Batch Loss = 0.441902, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5208330154418945, Accuracy = 0.896887481212616\n",
            "Iter #3383296:  Learning rate = 0.001300:   Batch Loss = 0.416482, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5201183557510376, Accuracy = 0.8954964280128479\n",
            "Iter #3387392:  Learning rate = 0.001300:   Batch Loss = 0.425299, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5044893622398376, Accuracy = 0.9034950733184814\n",
            "Iter #3391488:  Learning rate = 0.001300:   Batch Loss = 0.474297, Accuracy = 0.91015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5003039836883545, Accuracy = 0.9014084339141846\n",
            "Iter #3395584:  Learning rate = 0.001300:   Batch Loss = 0.434798, Accuracy = 0.921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5170665383338928, Accuracy = 0.8993218541145325\n",
            "Iter #3399680:  Learning rate = 0.001300:   Batch Loss = 0.454373, Accuracy = 0.908203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5200759172439575, Accuracy = 0.8994957208633423\n",
            "Iter #3403776:  Learning rate = 0.001248:   Batch Loss = 0.426120, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5092930793762207, Accuracy = 0.8908016085624695\n",
            "Iter #3407872:  Learning rate = 0.001248:   Batch Loss = 0.393038, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5222212076187134, Accuracy = 0.8977569341659546\n",
            "Iter #3411968:  Learning rate = 0.001248:   Batch Loss = 0.415266, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5017037987709045, Accuracy = 0.8998435139656067\n",
            "Iter #3416064:  Learning rate = 0.001248:   Batch Loss = 0.473548, Accuracy = 0.919921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6008175015449524, Accuracy = 0.875673770904541\n",
            "Iter #3420160:  Learning rate = 0.001248:   Batch Loss = 0.558138, Accuracy = 0.88671875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5425987243652344, Accuracy = 0.8814119100570679\n",
            "Iter #3424256:  Learning rate = 0.001248:   Batch Loss = 0.525801, Accuracy = 0.892578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.598991870880127, Accuracy = 0.8701095581054688\n",
            "Iter #3428352:  Learning rate = 0.001248:   Batch Loss = 0.457243, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5627011656761169, Accuracy = 0.8881933689117432\n",
            "Iter #3432448:  Learning rate = 0.001248:   Batch Loss = 0.448482, Accuracy = 0.91796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.54646235704422, Accuracy = 0.8843679428100586\n",
            "Iter #3436544:  Learning rate = 0.001248:   Batch Loss = 0.471337, Accuracy = 0.89453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5338585376739502, Accuracy = 0.8974091410636902\n",
            "Iter #3440640:  Learning rate = 0.001248:   Batch Loss = 0.483550, Accuracy = 0.904296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5234145522117615, Accuracy = 0.8814119100570679\n",
            "Iter #3444736:  Learning rate = 0.001248:   Batch Loss = 0.427742, Accuracy = 0.923828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5210903882980347, Accuracy = 0.8892366290092468\n",
            "Iter #3448832:  Learning rate = 0.001248:   Batch Loss = 0.436337, Accuracy = 0.931640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5260529518127441, Accuracy = 0.8960180878639221\n",
            "Iter #3452928:  Learning rate = 0.001248:   Batch Loss = 0.462167, Accuracy = 0.916015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.524523138999939, Accuracy = 0.8796731233596802\n",
            "Iter #3457024:  Learning rate = 0.001248:   Batch Loss = 0.443460, Accuracy = 0.923828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5087330341339111, Accuracy = 0.8904538154602051\n",
            "Iter #3461120:  Learning rate = 0.001248:   Batch Loss = 0.463742, Accuracy = 0.908203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.514700174331665, Accuracy = 0.8841940760612488\n",
            "Iter #3465216:  Learning rate = 0.001248:   Batch Loss = 0.435207, Accuracy = 0.927734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5347596406936646, Accuracy = 0.8819335699081421\n",
            "Iter #3469312:  Learning rate = 0.001248:   Batch Loss = 0.461163, Accuracy = 0.916015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5275394320487976, Accuracy = 0.8934098482131958\n",
            "Iter #3473408:  Learning rate = 0.001248:   Batch Loss = 0.461577, Accuracy = 0.912109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5702536106109619, Accuracy = 0.8633280992507935\n",
            "Iter #3477504:  Learning rate = 0.001248:   Batch Loss = 0.427695, Accuracy = 0.9140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5428048968315125, Accuracy = 0.8801947236061096\n",
            "Iter #3481600:  Learning rate = 0.001248:   Batch Loss = 0.444118, Accuracy = 0.912109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5246556997299194, Accuracy = 0.8826290965080261\n",
            "Iter #3485696:  Learning rate = 0.001248:   Batch Loss = 0.413264, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5131521224975586, Accuracy = 0.8993218541145325\n",
            "Iter #3489792:  Learning rate = 0.001248:   Batch Loss = 0.460504, Accuracy = 0.9140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5278058052062988, Accuracy = 0.8921926617622375\n",
            "Iter #3493888:  Learning rate = 0.001248:   Batch Loss = 0.438952, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5163018107414246, Accuracy = 0.8963658213615417\n",
            "Iter #3497984:  Learning rate = 0.001248:   Batch Loss = 0.491559, Accuracy = 0.900390625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5133060216903687, Accuracy = 0.9026256203651428\n",
            "Iter #3502080:  Learning rate = 0.001198:   Batch Loss = 0.448219, Accuracy = 0.91015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5168777704238892, Accuracy = 0.8781081438064575\n",
            "Iter #3506176:  Learning rate = 0.001198:   Batch Loss = 0.520334, Accuracy = 0.8984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5381281971931458, Accuracy = 0.8779342770576477\n",
            "Iter #3510272:  Learning rate = 0.001198:   Batch Loss = 0.481864, Accuracy = 0.904296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5463852882385254, Accuracy = 0.8716744780540466\n",
            "Iter #3514368:  Learning rate = 0.001198:   Batch Loss = 0.440110, Accuracy = 0.9296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5587798357009888, Accuracy = 0.8695878982543945\n",
            "Iter #3518464:  Learning rate = 0.001198:   Batch Loss = 0.429664, Accuracy = 0.921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5190798044204712, Accuracy = 0.8888888955116272\n",
            "Iter #3522560:  Learning rate = 0.001198:   Batch Loss = 0.405852, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.537183403968811, Accuracy = 0.8873239159584045\n",
            "Iter #3526656:  Learning rate = 0.001198:   Batch Loss = 0.413948, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5268129706382751, Accuracy = 0.8847156763076782\n",
            "Iter #3530752:  Learning rate = 0.001198:   Batch Loss = 0.441827, Accuracy = 0.921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5613458156585693, Accuracy = 0.8688923716545105\n",
            "Iter #3534848:  Learning rate = 0.001198:   Batch Loss = 0.462751, Accuracy = 0.904296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4930352568626404, Accuracy = 0.8894105553627014\n",
            "Iter #3538944:  Learning rate = 0.001198:   Batch Loss = 0.422041, Accuracy = 0.919921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5114730596542358, Accuracy = 0.8908016085624695\n",
            "Iter #3543040:  Learning rate = 0.001198:   Batch Loss = 0.459122, Accuracy = 0.90625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5529660582542419, Accuracy = 0.8695878982543945\n",
            "Iter #3547136:  Learning rate = 0.001198:   Batch Loss = 0.459812, Accuracy = 0.904296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5729565620422363, Accuracy = 0.8675013184547424\n",
            "Iter #3551232:  Learning rate = 0.001198:   Batch Loss = 0.463742, Accuracy = 0.916015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5508037805557251, Accuracy = 0.870631217956543\n",
            "Iter #3555328:  Learning rate = 0.001198:   Batch Loss = 0.454984, Accuracy = 0.91015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5399357080459595, Accuracy = 0.8822813630104065\n",
            "Iter #3559424:  Learning rate = 0.001198:   Batch Loss = 0.494608, Accuracy = 0.90625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5262495279312134, Accuracy = 0.8754999041557312\n",
            "Iter #3563520:  Learning rate = 0.001198:   Batch Loss = 0.412681, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5140224695205688, Accuracy = 0.8824552297592163\n",
            "Iter #3567616:  Learning rate = 0.001198:   Batch Loss = 0.457457, Accuracy = 0.919921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5031401515007019, Accuracy = 0.8821074366569519\n",
            "Iter #3571712:  Learning rate = 0.001198:   Batch Loss = 0.477459, Accuracy = 0.908203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5077346563339233, Accuracy = 0.8977569341659546\n",
            "Iter #3575808:  Learning rate = 0.001198:   Batch Loss = 0.407500, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4957733750343323, Accuracy = 0.9123630523681641\n",
            "Iter #3579904:  Learning rate = 0.001198:   Batch Loss = 0.415831, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5256297588348389, Accuracy = 0.893235981464386\n",
            "Iter #3584000:  Learning rate = 0.001198:   Batch Loss = 0.453111, Accuracy = 0.921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5443190336227417, Accuracy = 0.8789775967597961\n",
            "Iter #3588096:  Learning rate = 0.001198:   Batch Loss = 0.451991, Accuracy = 0.91015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.533439040184021, Accuracy = 0.8878455758094788\n",
            "Iter #3592192:  Learning rate = 0.001198:   Batch Loss = 0.446214, Accuracy = 0.919921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5717896819114685, Accuracy = 0.8662841320037842\n",
            "Iter #3596288:  Learning rate = 0.001198:   Batch Loss = 0.424626, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5246328711509705, Accuracy = 0.8887150287628174\n",
            "Iter #3600384:  Learning rate = 0.001150:   Batch Loss = 0.408273, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5550161600112915, Accuracy = 0.889758288860321\n",
            "Iter #3604480:  Learning rate = 0.001150:   Batch Loss = 0.458453, Accuracy = 0.912109375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5273388028144836, Accuracy = 0.9021039605140686\n",
            "Iter #3608576:  Learning rate = 0.001150:   Batch Loss = 0.416801, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5164247751235962, Accuracy = 0.8894105553627014\n",
            "Iter #3612672:  Learning rate = 0.001150:   Batch Loss = 0.430402, Accuracy = 0.91796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5140509605407715, Accuracy = 0.9012345671653748\n",
            "Iter #3616768:  Learning rate = 0.001150:   Batch Loss = 0.464670, Accuracy = 0.90625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4847014546394348, Accuracy = 0.9090592861175537\n",
            "Iter #3620864:  Learning rate = 0.001150:   Batch Loss = 0.501992, Accuracy = 0.904296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5102524757385254, Accuracy = 0.8993218541145325\n",
            "Iter #3624960:  Learning rate = 0.001150:   Batch Loss = 0.398763, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5105094313621521, Accuracy = 0.9034950733184814\n",
            "Iter #3629056:  Learning rate = 0.001150:   Batch Loss = 0.380326, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5360182523727417, Accuracy = 0.893235981464386\n",
            "Iter #3633152:  Learning rate = 0.001150:   Batch Loss = 0.416710, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5295842885971069, Accuracy = 0.8892366290092468\n",
            "Iter #3637248:  Learning rate = 0.001150:   Batch Loss = 0.438290, Accuracy = 0.916015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49647781252861023, Accuracy = 0.8972352743148804\n",
            "Iter #3641344:  Learning rate = 0.001150:   Batch Loss = 0.418885, Accuracy = 0.923828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48273777961730957, Accuracy = 0.9033211469650269\n",
            "Iter #3645440:  Learning rate = 0.001150:   Batch Loss = 0.520601, Accuracy = 0.91015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48637714982032776, Accuracy = 0.903147280216217\n",
            "Iter #3649536:  Learning rate = 0.001150:   Batch Loss = 0.394486, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5362370014190674, Accuracy = 0.8963658213615417\n",
            "Iter #3653632:  Learning rate = 0.001150:   Batch Loss = 0.449128, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4852377772331238, Accuracy = 0.902451753616333\n",
            "Iter #3657728:  Learning rate = 0.001150:   Batch Loss = 0.426065, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5093214511871338, Accuracy = 0.9111458659172058\n",
            "Iter #3661824:  Learning rate = 0.001150:   Batch Loss = 0.404454, Accuracy = 0.9296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.524283766746521, Accuracy = 0.8960180878639221\n",
            "Iter #3665920:  Learning rate = 0.001150:   Batch Loss = 0.382536, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5145450830459595, Accuracy = 0.8965397477149963\n",
            "Iter #3670016:  Learning rate = 0.001150:   Batch Loss = 0.421770, Accuracy = 0.927734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5055577754974365, Accuracy = 0.8942792415618896\n",
            "Iter #3674112:  Learning rate = 0.001150:   Batch Loss = 0.431407, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5203819870948792, Accuracy = 0.8866283893585205\n",
            "Iter #3678208:  Learning rate = 0.001150:   Batch Loss = 0.422189, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4915338158607483, Accuracy = 0.905407726764679\n",
            "Iter #3682304:  Learning rate = 0.001150:   Batch Loss = 0.457678, Accuracy = 0.908203125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5085013508796692, Accuracy = 0.8873239159584045\n",
            "Iter #3686400:  Learning rate = 0.001150:   Batch Loss = 0.424314, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5060302019119263, Accuracy = 0.8956702947616577\n",
            "Iter #3690496:  Learning rate = 0.001150:   Batch Loss = 0.418503, Accuracy = 0.923828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4980778694152832, Accuracy = 0.8958442211151123\n",
            "Iter #3694592:  Learning rate = 0.001150:   Batch Loss = 0.393170, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4737425148487091, Accuracy = 0.9073204398155212\n",
            "Iter #3698688:  Learning rate = 0.001150:   Batch Loss = 0.428636, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49521708488464355, Accuracy = 0.9008867740631104\n",
            "Iter #3702784:  Learning rate = 0.001104:   Batch Loss = 0.451308, Accuracy = 0.91015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4974044859409332, Accuracy = 0.8996696472167969\n",
            "Iter #3706880:  Learning rate = 0.001104:   Batch Loss = 0.404352, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47620418667793274, Accuracy = 0.9021039605140686\n",
            "Iter #3710976:  Learning rate = 0.001104:   Batch Loss = 0.448682, Accuracy = 0.923828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.508154034614563, Accuracy = 0.8944531679153442\n",
            "Iter #3715072:  Learning rate = 0.001104:   Batch Loss = 0.410789, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5060960650444031, Accuracy = 0.896887481212616\n",
            "Iter #3719168:  Learning rate = 0.001104:   Batch Loss = 0.404953, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4959847629070282, Accuracy = 0.8984524607658386\n",
            "Iter #3723264:  Learning rate = 0.001104:   Batch Loss = 0.460897, Accuracy = 0.8984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5020666718482971, Accuracy = 0.8974091410636902\n",
            "Iter #3727360:  Learning rate = 0.001104:   Batch Loss = 0.405985, Accuracy = 0.931640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5386961698532104, Accuracy = 0.8904538154602051\n",
            "Iter #3731456:  Learning rate = 0.001104:   Batch Loss = 0.442760, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5142207741737366, Accuracy = 0.888367235660553\n",
            "Iter #3735552:  Learning rate = 0.001104:   Batch Loss = 0.427924, Accuracy = 0.927734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48893535137176514, Accuracy = 0.9034950733184814\n",
            "Iter #3739648:  Learning rate = 0.001104:   Batch Loss = 0.406930, Accuracy = 0.927734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49350297451019287, Accuracy = 0.9034950733184814\n",
            "Iter #3743744:  Learning rate = 0.001104:   Batch Loss = 0.420275, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.501183807849884, Accuracy = 0.9021039605140686\n",
            "Iter #3747840:  Learning rate = 0.001104:   Batch Loss = 0.396219, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5115188360214233, Accuracy = 0.89393150806427\n",
            "Iter #3751936:  Learning rate = 0.001104:   Batch Loss = 0.450966, Accuracy = 0.91796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5058565139770508, Accuracy = 0.8951486945152283\n",
            "Iter #3756032:  Learning rate = 0.001104:   Batch Loss = 0.447217, Accuracy = 0.931640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49211907386779785, Accuracy = 0.9019300937652588\n",
            "Iter #3760128:  Learning rate = 0.001104:   Batch Loss = 0.424019, Accuracy = 0.923828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5106183886528015, Accuracy = 0.9008867740631104\n",
            "Iter #3764224:  Learning rate = 0.001104:   Batch Loss = 0.435084, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5049125552177429, Accuracy = 0.8880195021629333\n",
            "Iter #3768320:  Learning rate = 0.001104:   Batch Loss = 0.437444, Accuracy = 0.927734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5186407566070557, Accuracy = 0.8977569341659546\n",
            "Iter #3772416:  Learning rate = 0.001104:   Batch Loss = 0.397260, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4896131157875061, Accuracy = 0.901756227016449\n",
            "Iter #3776512:  Learning rate = 0.001104:   Batch Loss = 0.373514, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48802411556243896, Accuracy = 0.9021039605140686\n",
            "Iter #3780608:  Learning rate = 0.001104:   Batch Loss = 0.406774, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48875856399536133, Accuracy = 0.9113197922706604\n",
            "Iter #3784704:  Learning rate = 0.001104:   Batch Loss = 0.437626, Accuracy = 0.927734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5003183484077454, Accuracy = 0.9000173807144165\n",
            "Iter #3788800:  Learning rate = 0.001104:   Batch Loss = 0.403185, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5021754503250122, Accuracy = 0.9057555198669434\n",
            "Iter #3792896:  Learning rate = 0.001104:   Batch Loss = 0.419294, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5191681981086731, Accuracy = 0.8977569341659546\n",
            "Iter #3796992:  Learning rate = 0.001104:   Batch Loss = 0.390970, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4891955852508545, Accuracy = 0.8960180878639221\n",
            "Iter #3801088:  Learning rate = 0.001060:   Batch Loss = 0.444592, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5062812566757202, Accuracy = 0.898278534412384\n",
            "Iter #3805184:  Learning rate = 0.001060:   Batch Loss = 0.406286, Accuracy = 0.931640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4711450934410095, Accuracy = 0.9047122001647949\n",
            "Iter #3809280:  Learning rate = 0.001060:   Batch Loss = 0.398896, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49685609340667725, Accuracy = 0.8977569341659546\n",
            "Iter #3813376:  Learning rate = 0.001060:   Batch Loss = 0.381593, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4895040690898895, Accuracy = 0.9040166735649109\n",
            "Iter #3817472:  Learning rate = 0.001060:   Batch Loss = 0.401703, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4833648204803467, Accuracy = 0.9041905999183655\n",
            "Iter #3821568:  Learning rate = 0.001060:   Batch Loss = 0.388369, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5362632870674133, Accuracy = 0.8920187950134277\n",
            "Iter #3825664:  Learning rate = 0.001060:   Batch Loss = 0.387949, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5239980220794678, Accuracy = 0.888367235660553\n",
            "Iter #3829760:  Learning rate = 0.001060:   Batch Loss = 0.438625, Accuracy = 0.921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5078592300415039, Accuracy = 0.8986263275146484\n",
            "Iter #3833856:  Learning rate = 0.001060:   Batch Loss = 0.395475, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4985349178314209, Accuracy = 0.9019300937652588\n",
            "Iter #3837952:  Learning rate = 0.001060:   Batch Loss = 0.458532, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5143312811851501, Accuracy = 0.889758288860321\n",
            "Iter #3842048:  Learning rate = 0.001060:   Batch Loss = 0.357469, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49930667877197266, Accuracy = 0.8941053748130798\n",
            "Iter #3846144:  Learning rate = 0.001060:   Batch Loss = 0.457261, Accuracy = 0.91015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5096011757850647, Accuracy = 0.9074943661689758\n",
            "Iter #3850240:  Learning rate = 0.001060:   Batch Loss = 0.420637, Accuracy = 0.923828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5164827108383179, Accuracy = 0.9038428068161011\n",
            "Iter #3854336:  Learning rate = 0.001060:   Batch Loss = 0.366969, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4930269122123718, Accuracy = 0.9130585789680481\n",
            "Iter #3858432:  Learning rate = 0.001060:   Batch Loss = 0.424196, Accuracy = 0.916015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.498576283454895, Accuracy = 0.8989741206169128\n",
            "Iter #3862528:  Learning rate = 0.001060:   Batch Loss = 0.376270, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4996829032897949, Accuracy = 0.9061033129692078\n",
            "Iter #3866624:  Learning rate = 0.001060:   Batch Loss = 0.423303, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4796236753463745, Accuracy = 0.903147280216217\n",
            "Iter #3870720:  Learning rate = 0.001060:   Batch Loss = 0.407657, Accuracy = 0.91796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5173207521438599, Accuracy = 0.903147280216217\n",
            "Iter #3874816:  Learning rate = 0.001060:   Batch Loss = 0.417878, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4849051833152771, Accuracy = 0.902451753616333\n",
            "Iter #3878912:  Learning rate = 0.001060:   Batch Loss = 0.387914, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49916139245033264, Accuracy = 0.9005390405654907\n",
            "Iter #3883008:  Learning rate = 0.001060:   Batch Loss = 0.388526, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5165600180625916, Accuracy = 0.8892366290092468\n",
            "Iter #3887104:  Learning rate = 0.001060:   Batch Loss = 0.438013, Accuracy = 0.919921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47995197772979736, Accuracy = 0.9137541055679321\n",
            "Iter #3891200:  Learning rate = 0.001060:   Batch Loss = 0.421556, Accuracy = 0.931640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49706435203552246, Accuracy = 0.9073204398155212\n",
            "Iter #3895296:  Learning rate = 0.001060:   Batch Loss = 0.390198, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47220343351364136, Accuracy = 0.9184489846229553\n",
            "Iter #3899392:  Learning rate = 0.001060:   Batch Loss = 0.378876, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48495370149612427, Accuracy = 0.9139280319213867\n",
            "Iter #3903488:  Learning rate = 0.001018:   Batch Loss = 0.417741, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49227994680404663, Accuracy = 0.9021039605140686\n",
            "Iter #3907584:  Learning rate = 0.001018:   Batch Loss = 0.443100, Accuracy = 0.927734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5236693620681763, Accuracy = 0.9113197922706604\n",
            "Iter #3911680:  Learning rate = 0.001018:   Batch Loss = 0.426552, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49836117029190063, Accuracy = 0.9069727063179016\n",
            "Iter #3915776:  Learning rate = 0.001018:   Batch Loss = 0.430960, Accuracy = 0.919921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47796329855918884, Accuracy = 0.9090592861175537\n",
            "Iter #3919872:  Learning rate = 0.001018:   Batch Loss = 0.415357, Accuracy = 0.91796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4710012674331665, Accuracy = 0.9125369787216187\n",
            "Iter #3923968:  Learning rate = 0.001018:   Batch Loss = 0.383729, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4758591055870056, Accuracy = 0.9123630523681641\n",
            "Iter #3928064:  Learning rate = 0.001018:   Batch Loss = 0.405082, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48849594593048096, Accuracy = 0.9078420996665955\n",
            "Iter #3932160:  Learning rate = 0.001018:   Batch Loss = 0.428870, Accuracy = 0.916015625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.469024121761322, Accuracy = 0.9107981324195862\n",
            "Iter #3936256:  Learning rate = 0.001018:   Batch Loss = 0.392999, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5113500356674194, Accuracy = 0.9034950733184814\n",
            "Iter #3940352:  Learning rate = 0.001018:   Batch Loss = 0.413608, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46826082468032837, Accuracy = 0.9163623452186584\n",
            "Iter #3944448:  Learning rate = 0.001018:   Batch Loss = 0.403591, Accuracy = 0.931640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4464776813983917, Accuracy = 0.9266214370727539\n",
            "Iter #3948544:  Learning rate = 0.001018:   Batch Loss = 0.394973, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5088219046592712, Accuracy = 0.9057555198669434\n",
            "Iter #3952640:  Learning rate = 0.001018:   Batch Loss = 0.394180, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48331218957901, Accuracy = 0.9125369787216187\n",
            "Iter #3956736:  Learning rate = 0.001018:   Batch Loss = 0.418962, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4732324481010437, Accuracy = 0.9099286794662476\n",
            "Iter #3960832:  Learning rate = 0.001018:   Batch Loss = 0.423689, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4949553310871124, Accuracy = 0.9135802388191223\n",
            "Iter #3964928:  Learning rate = 0.001018:   Batch Loss = 0.413471, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.498607873916626, Accuracy = 0.9087115526199341\n",
            "Iter #3969024:  Learning rate = 0.001018:   Batch Loss = 0.370199, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4869379997253418, Accuracy = 0.9106242656707764\n",
            "Iter #3973120:  Learning rate = 0.001018:   Batch Loss = 0.440656, Accuracy = 0.931640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4734513759613037, Accuracy = 0.9107981324195862\n",
            "Iter #3977216:  Learning rate = 0.001018:   Batch Loss = 0.368864, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4626060724258423, Accuracy = 0.9181011915206909\n",
            "Iter #3981312:  Learning rate = 0.001018:   Batch Loss = 0.367675, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.507360577583313, Accuracy = 0.9012345671653748\n",
            "Iter #3985408:  Learning rate = 0.001018:   Batch Loss = 0.355258, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5045215487480164, Accuracy = 0.9137541055679321\n",
            "Iter #3989504:  Learning rate = 0.001018:   Batch Loss = 0.342223, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4747689962387085, Accuracy = 0.9076682329177856\n",
            "Iter #3993600:  Learning rate = 0.001018:   Batch Loss = 0.375625, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48691415786743164, Accuracy = 0.9097548127174377\n",
            "Iter #3997696:  Learning rate = 0.001018:   Batch Loss = 0.435870, Accuracy = 0.91796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5102370977401733, Accuracy = 0.9099286794662476\n",
            "Iter #4001792:  Learning rate = 0.000977:   Batch Loss = 0.431532, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45715397596359253, Accuracy = 0.9174056649208069\n",
            "Iter #4005888:  Learning rate = 0.000977:   Batch Loss = 0.384873, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5091376900672913, Accuracy = 0.9029734134674072\n",
            "Iter #4009984:  Learning rate = 0.000977:   Batch Loss = 0.404720, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.512954592704773, Accuracy = 0.902451753616333\n",
            "Iter #4014080:  Learning rate = 0.000977:   Batch Loss = 0.412927, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49968287348747253, Accuracy = 0.915840744972229\n",
            "Iter #4018176:  Learning rate = 0.000977:   Batch Loss = 0.445785, Accuracy = 0.921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5106985569000244, Accuracy = 0.9134063720703125\n",
            "Iter #4022272:  Learning rate = 0.000977:   Batch Loss = 0.385711, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48180174827575684, Accuracy = 0.9052338600158691\n",
            "Iter #4026368:  Learning rate = 0.000977:   Batch Loss = 0.358954, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4734880030155182, Accuracy = 0.9085376262664795\n",
            "Iter #4030464:  Learning rate = 0.000977:   Batch Loss = 0.371281, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46313995122909546, Accuracy = 0.910276472568512\n",
            "Iter #4034560:  Learning rate = 0.000977:   Batch Loss = 0.406958, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47780027985572815, Accuracy = 0.9092331528663635\n",
            "Iter #4038656:  Learning rate = 0.000977:   Batch Loss = 0.414092, Accuracy = 0.9296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46908658742904663, Accuracy = 0.9106242656707764\n",
            "Iter #4042752:  Learning rate = 0.000977:   Batch Loss = 0.423223, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48556965589523315, Accuracy = 0.9067988395690918\n",
            "Iter #4046848:  Learning rate = 0.000977:   Batch Loss = 0.392367, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47661852836608887, Accuracy = 0.9146235585212708\n",
            "Iter #4050944:  Learning rate = 0.000977:   Batch Loss = 0.378875, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46314379572868347, Accuracy = 0.9163623452186584\n",
            "Iter #4055040:  Learning rate = 0.000977:   Batch Loss = 0.344012, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4785895347595215, Accuracy = 0.9168840050697327\n",
            "Iter #4059136:  Learning rate = 0.000977:   Batch Loss = 0.414986, Accuracy = 0.931640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4871637225151062, Accuracy = 0.9189705848693848\n",
            "Iter #4063232:  Learning rate = 0.000977:   Batch Loss = 0.378291, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4746619760990143, Accuracy = 0.9074943661689758\n",
            "Iter #4067328:  Learning rate = 0.000977:   Batch Loss = 0.422998, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5086787343025208, Accuracy = 0.9033211469650269\n",
            "Iter #4071424:  Learning rate = 0.000977:   Batch Loss = 0.401750, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4993937909603119, Accuracy = 0.9038428068161011\n",
            "Iter #4075520:  Learning rate = 0.000977:   Batch Loss = 0.416258, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5070037245750427, Accuracy = 0.9001912474632263\n",
            "Iter #4079616:  Learning rate = 0.000977:   Batch Loss = 0.393562, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4655402600765228, Accuracy = 0.9139280319213867\n",
            "Iter #4083712:  Learning rate = 0.000977:   Batch Loss = 0.363035, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4860677123069763, Accuracy = 0.9134063720703125\n",
            "Iter #4087808:  Learning rate = 0.000977:   Batch Loss = 0.440615, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4726333022117615, Accuracy = 0.9161884784698486\n",
            "Iter #4091904:  Learning rate = 0.000977:   Batch Loss = 0.437634, Accuracy = 0.919921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48958635330200195, Accuracy = 0.9047122001647949\n",
            "Iter #4096000:  Learning rate = 0.000977:   Batch Loss = 0.364816, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4761736989021301, Accuracy = 0.905407726764679\n",
            "Iter #4100096:  Learning rate = 0.000938:   Batch Loss = 0.395632, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4699515700340271, Accuracy = 0.9064510464668274\n",
            "Iter #4104192:  Learning rate = 0.000938:   Batch Loss = 0.415182, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5040278434753418, Accuracy = 0.9069727063179016\n",
            "Iter #4108288:  Learning rate = 0.000938:   Batch Loss = 0.387897, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47961336374282837, Accuracy = 0.9036689400672913\n",
            "Iter #4112384:  Learning rate = 0.000938:   Batch Loss = 0.350174, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46479523181915283, Accuracy = 0.910276472568512\n",
            "Iter #4116480:  Learning rate = 0.000938:   Batch Loss = 0.366336, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4837307333946228, Accuracy = 0.90801602602005\n",
            "Iter #4120576:  Learning rate = 0.000938:   Batch Loss = 0.383757, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47643420100212097, Accuracy = 0.9182750582695007\n",
            "Iter #4124672:  Learning rate = 0.000938:   Batch Loss = 0.361186, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5033324360847473, Accuracy = 0.9154929518699646\n",
            "Iter #4128768:  Learning rate = 0.000938:   Batch Loss = 0.406088, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4995832145214081, Accuracy = 0.9052338600158691\n",
            "Iter #4132864:  Learning rate = 0.000938:   Batch Loss = 0.361858, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45541614294052124, Accuracy = 0.915840744972229\n",
            "Iter #4136960:  Learning rate = 0.000938:   Batch Loss = 0.378969, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4482712149620056, Accuracy = 0.9189705848693848\n",
            "Iter #4141056:  Learning rate = 0.000938:   Batch Loss = 0.347044, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46134406328201294, Accuracy = 0.9207094311714172\n",
            "Iter #4145152:  Learning rate = 0.000938:   Batch Loss = 0.410488, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48267656564712524, Accuracy = 0.9083637595176697\n",
            "Iter #4149248:  Learning rate = 0.000938:   Batch Loss = 0.324376, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4659476578235626, Accuracy = 0.9156668186187744\n",
            "Iter #4153344:  Learning rate = 0.000938:   Batch Loss = 0.331931, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45805823802948, Accuracy = 0.9227960109710693\n",
            "Iter #4157440:  Learning rate = 0.000938:   Batch Loss = 0.356744, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.458129346370697, Accuracy = 0.9276647567749023\n",
            "Iter #4161536:  Learning rate = 0.000938:   Batch Loss = 0.396670, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4739922285079956, Accuracy = 0.9106242656707764\n",
            "Iter #4165632:  Learning rate = 0.000938:   Batch Loss = 0.372031, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46286097168922424, Accuracy = 0.9135802388191223\n",
            "Iter #4169728:  Learning rate = 0.000938:   Batch Loss = 0.406108, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4745617210865021, Accuracy = 0.9127108454704285\n",
            "Iter #4173824:  Learning rate = 0.000938:   Batch Loss = 0.351113, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4508070945739746, Accuracy = 0.9210572242736816\n",
            "Iter #4177920:  Learning rate = 0.000938:   Batch Loss = 0.374314, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4700780510902405, Accuracy = 0.9147974252700806\n",
            "Iter #4182016:  Learning rate = 0.000938:   Batch Loss = 0.381213, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4853995144367218, Accuracy = 0.9029734134674072\n",
            "Iter #4186112:  Learning rate = 0.000938:   Batch Loss = 0.422160, Accuracy = 0.931640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49422481656074524, Accuracy = 0.9041905999183655\n",
            "Iter #4190208:  Learning rate = 0.000938:   Batch Loss = 0.394406, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5116493105888367, Accuracy = 0.9045383334159851\n",
            "Iter #4194304:  Learning rate = 0.000938:   Batch Loss = 0.353476, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47452425956726074, Accuracy = 0.9106242656707764\n",
            "Iter #4198400:  Learning rate = 0.000938:   Batch Loss = 0.437888, Accuracy = 0.91796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49458983540534973, Accuracy = 0.9179273247718811\n",
            "Iter #4202496:  Learning rate = 0.000900:   Batch Loss = 0.402697, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5044297575950623, Accuracy = 0.9040166735649109\n",
            "Iter #4206592:  Learning rate = 0.000900:   Batch Loss = 0.372301, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4864874482154846, Accuracy = 0.9092331528663635\n",
            "Iter #4210688:  Learning rate = 0.000900:   Batch Loss = 0.362115, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4567772150039673, Accuracy = 0.9132325053215027\n",
            "Iter #4214784:  Learning rate = 0.000900:   Batch Loss = 0.371110, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48758459091186523, Accuracy = 0.9101026058197021\n",
            "Iter #4218880:  Learning rate = 0.000900:   Batch Loss = 0.343235, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4506176710128784, Accuracy = 0.920883297920227\n",
            "Iter #4222976:  Learning rate = 0.000900:   Batch Loss = 0.398735, Accuracy = 0.927734375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4643692374229431, Accuracy = 0.9198400378227234\n",
            "Iter #4227072:  Learning rate = 0.000900:   Batch Loss = 0.359479, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46418654918670654, Accuracy = 0.9107981324195862\n",
            "Iter #4231168:  Learning rate = 0.000900:   Batch Loss = 0.363174, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4592849016189575, Accuracy = 0.9238393306732178\n",
            "Iter #4235264:  Learning rate = 0.000900:   Batch Loss = 0.364056, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47539404034614563, Accuracy = 0.9200139045715332\n",
            "Iter #4239360:  Learning rate = 0.000900:   Batch Loss = 0.371666, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47554492950439453, Accuracy = 0.9125369787216187\n",
            "Iter #4243456:  Learning rate = 0.000900:   Batch Loss = 0.348356, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46123674511909485, Accuracy = 0.9198400378227234\n",
            "Iter #4247552:  Learning rate = 0.000900:   Batch Loss = 0.387576, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.450969398021698, Accuracy = 0.9137541055679321\n",
            "Iter #4251648:  Learning rate = 0.000900:   Batch Loss = 0.412233, Accuracy = 0.9296875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45860499143600464, Accuracy = 0.9144496321678162\n",
            "Iter #4255744:  Learning rate = 0.000900:   Batch Loss = 0.379227, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4535807967185974, Accuracy = 0.9156668186187744\n",
            "Iter #4259840:  Learning rate = 0.000900:   Batch Loss = 0.347431, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4756835401058197, Accuracy = 0.9170579314231873\n",
            "Iter #4263936:  Learning rate = 0.000900:   Batch Loss = 0.392742, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47057831287384033, Accuracy = 0.9163623452186584\n",
            "Iter #4268032:  Learning rate = 0.000900:   Batch Loss = 0.364648, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4857572019100189, Accuracy = 0.910276472568512\n",
            "Iter #4272128:  Learning rate = 0.000900:   Batch Loss = 0.402920, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49412649869918823, Accuracy = 0.9078420996665955\n",
            "Iter #4276224:  Learning rate = 0.000900:   Batch Loss = 0.376371, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48418593406677246, Accuracy = 0.9134063720703125\n",
            "Iter #4280320:  Learning rate = 0.000900:   Batch Loss = 0.380765, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.486672580242157, Accuracy = 0.9200139045715332\n",
            "Iter #4284416:  Learning rate = 0.000900:   Batch Loss = 0.426610, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5028855800628662, Accuracy = 0.9099286794662476\n",
            "Iter #4288512:  Learning rate = 0.000900:   Batch Loss = 0.352660, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48765015602111816, Accuracy = 0.9142757654190063\n",
            "Iter #4292608:  Learning rate = 0.000900:   Batch Loss = 0.385011, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46355879306793213, Accuracy = 0.9184489846229553\n",
            "Iter #4296704:  Learning rate = 0.000900:   Batch Loss = 0.373840, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46474215388298035, Accuracy = 0.9127108454704285\n",
            "Iter #4300800:  Learning rate = 0.000864:   Batch Loss = 0.395954, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4552430510520935, Accuracy = 0.9175795316696167\n",
            "Iter #4304896:  Learning rate = 0.000864:   Batch Loss = 0.386488, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4567683935165405, Accuracy = 0.9191445112228394\n",
            "Iter #4308992:  Learning rate = 0.000864:   Batch Loss = 0.352050, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46919211745262146, Accuracy = 0.9245348572731018\n",
            "Iter #4313088:  Learning rate = 0.000864:   Batch Loss = 0.352009, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45085957646369934, Accuracy = 0.9184489846229553\n",
            "Iter #4317184:  Learning rate = 0.000864:   Batch Loss = 0.392513, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48296189308166504, Accuracy = 0.9078420996665955\n",
            "Iter #4321280:  Learning rate = 0.000864:   Batch Loss = 0.405610, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4924432635307312, Accuracy = 0.9153190851211548\n",
            "Iter #4325376:  Learning rate = 0.000864:   Batch Loss = 0.341361, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49179938435554504, Accuracy = 0.9161884784698486\n",
            "Iter #4329472:  Learning rate = 0.000864:   Batch Loss = 0.374877, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4639859199523926, Accuracy = 0.9193183779716492\n",
            "Iter #4333568:  Learning rate = 0.000864:   Batch Loss = 0.348811, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4582274556159973, Accuracy = 0.915145218372345\n",
            "Iter #4337664:  Learning rate = 0.000864:   Batch Loss = 0.368818, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46247661113739014, Accuracy = 0.9189705848693848\n",
            "Iter #4341760:  Learning rate = 0.000864:   Batch Loss = 0.377758, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4682503044605255, Accuracy = 0.9153190851211548\n",
            "Iter #4345856:  Learning rate = 0.000864:   Batch Loss = 0.373538, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4561005234718323, Accuracy = 0.9205355644226074\n",
            "Iter #4349952:  Learning rate = 0.000864:   Batch Loss = 0.363400, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46226388216018677, Accuracy = 0.9147974252700806\n",
            "Iter #4354048:  Learning rate = 0.000864:   Batch Loss = 0.370988, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4522722661495209, Accuracy = 0.9156668186187744\n",
            "Iter #4358144:  Learning rate = 0.000864:   Batch Loss = 0.386064, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47839412093162537, Accuracy = 0.9193183779716492\n",
            "Iter #4362240:  Learning rate = 0.000864:   Batch Loss = 0.385041, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48689812421798706, Accuracy = 0.9071465730667114\n",
            "Iter #4366336:  Learning rate = 0.000864:   Batch Loss = 0.347783, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4604555368423462, Accuracy = 0.9248826503753662\n",
            "Iter #4370432:  Learning rate = 0.000864:   Batch Loss = 0.396082, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4877637028694153, Accuracy = 0.9069727063179016\n",
            "Iter #4374528:  Learning rate = 0.000864:   Batch Loss = 0.387548, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4692647457122803, Accuracy = 0.9172317981719971\n",
            "Iter #4378624:  Learning rate = 0.000864:   Batch Loss = 0.359391, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4619658589363098, Accuracy = 0.9224482774734497\n",
            "Iter #4382720:  Learning rate = 0.000864:   Batch Loss = 0.350882, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4432792663574219, Accuracy = 0.9273169636726379\n",
            "Iter #4386816:  Learning rate = 0.000864:   Batch Loss = 0.359866, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4447628855705261, Accuracy = 0.9198400378227234\n",
            "Iter #4390912:  Learning rate = 0.000864:   Batch Loss = 0.353705, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46915000677108765, Accuracy = 0.9132325053215027\n",
            "Iter #4395008:  Learning rate = 0.000864:   Batch Loss = 0.398941, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43521279096603394, Accuracy = 0.923665463924408\n",
            "Iter #4399104:  Learning rate = 0.000864:   Batch Loss = 0.394325, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46371397376060486, Accuracy = 0.9141018986701965\n",
            "Iter #4403200:  Learning rate = 0.000830:   Batch Loss = 0.345928, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4719541370868683, Accuracy = 0.9141018986701965\n",
            "Iter #4407296:  Learning rate = 0.000830:   Batch Loss = 0.326596, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4644058048725128, Accuracy = 0.9229699373245239\n",
            "Iter #4411392:  Learning rate = 0.000830:   Batch Loss = 0.371009, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45894110202789307, Accuracy = 0.9163623452186584\n",
            "Iter #4415488:  Learning rate = 0.000830:   Batch Loss = 0.367954, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5148773789405823, Accuracy = 0.9052338600158691\n",
            "Iter #4419584:  Learning rate = 0.000830:   Batch Loss = 0.356923, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47111546993255615, Accuracy = 0.915145218372345\n",
            "Iter #4423680:  Learning rate = 0.000830:   Batch Loss = 0.353108, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4592830240726471, Accuracy = 0.9179273247718811\n",
            "Iter #4427776:  Learning rate = 0.000830:   Batch Loss = 0.367665, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4777243733406067, Accuracy = 0.9118413925170898\n",
            "Iter #4431872:  Learning rate = 0.000830:   Batch Loss = 0.362339, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47575265169143677, Accuracy = 0.9167101383209229\n",
            "Iter #4435968:  Learning rate = 0.000830:   Batch Loss = 0.385991, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44921767711639404, Accuracy = 0.9245348572731018\n",
            "Iter #4440064:  Learning rate = 0.000830:   Batch Loss = 0.348045, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4744938611984253, Accuracy = 0.9172317981719971\n",
            "Iter #4444160:  Learning rate = 0.000830:   Batch Loss = 0.350735, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4772820472717285, Accuracy = 0.9111458659172058\n",
            "Iter #4448256:  Learning rate = 0.000830:   Batch Loss = 0.350164, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4753391146659851, Accuracy = 0.9137541055679321\n",
            "Iter #4452352:  Learning rate = 0.000830:   Batch Loss = 0.377640, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4765121340751648, Accuracy = 0.9149712920188904\n",
            "Iter #4456448:  Learning rate = 0.000830:   Batch Loss = 0.387778, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44054338335990906, Accuracy = 0.9260998368263245\n",
            "Iter #4460544:  Learning rate = 0.000830:   Batch Loss = 0.376817, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5242043137550354, Accuracy = 0.8993218541145325\n",
            "Iter #4464640:  Learning rate = 0.000830:   Batch Loss = 0.355047, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4636765420436859, Accuracy = 0.9134063720703125\n",
            "Iter #4468736:  Learning rate = 0.000830:   Batch Loss = 0.388987, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4902484714984894, Accuracy = 0.9048861265182495\n",
            "Iter #4472832:  Learning rate = 0.000830:   Batch Loss = 0.404683, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4459614157676697, Accuracy = 0.9290558099746704\n",
            "Iter #4476928:  Learning rate = 0.000830:   Batch Loss = 0.375478, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4681101441383362, Accuracy = 0.9085376262664795\n",
            "Iter #4481024:  Learning rate = 0.000830:   Batch Loss = 0.397885, Accuracy = 0.931640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5085208415985107, Accuracy = 0.910971999168396\n",
            "Iter #4485120:  Learning rate = 0.000830:   Batch Loss = 0.353929, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4484255313873291, Accuracy = 0.9172317981719971\n",
            "Iter #4489216:  Learning rate = 0.000830:   Batch Loss = 0.372572, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4554011821746826, Accuracy = 0.9168840050697327\n",
            "Iter #4493312:  Learning rate = 0.000830:   Batch Loss = 0.369623, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47602471709251404, Accuracy = 0.9081898927688599\n",
            "Iter #4497408:  Learning rate = 0.000830:   Batch Loss = 0.331859, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45811140537261963, Accuracy = 0.9167101383209229\n",
            "Iter #4501504:  Learning rate = 0.000796:   Batch Loss = 0.344842, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4565260112285614, Accuracy = 0.9149712920188904\n",
            "Iter #4505600:  Learning rate = 0.000796:   Batch Loss = 0.347512, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46818435192108154, Accuracy = 0.9146235585212708\n",
            "Iter #4509696:  Learning rate = 0.000796:   Batch Loss = 0.374535, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4715209901332855, Accuracy = 0.91166752576828\n",
            "Iter #4513792:  Learning rate = 0.000796:   Batch Loss = 0.325458, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4586027264595032, Accuracy = 0.9227960109710693\n",
            "Iter #4517888:  Learning rate = 0.000796:   Batch Loss = 0.383132, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46573832631111145, Accuracy = 0.910971999168396\n",
            "Iter #4521984:  Learning rate = 0.000796:   Batch Loss = 0.350939, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4865395128726959, Accuracy = 0.9142757654190063\n",
            "Iter #4526080:  Learning rate = 0.000796:   Batch Loss = 0.397377, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4669727087020874, Accuracy = 0.9170579314231873\n",
            "Iter #4530176:  Learning rate = 0.000796:   Batch Loss = 0.362388, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4594494104385376, Accuracy = 0.920883297920227\n",
            "Iter #4534272:  Learning rate = 0.000796:   Batch Loss = 0.355737, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45750510692596436, Accuracy = 0.9106242656707764\n",
            "Iter #4538368:  Learning rate = 0.000796:   Batch Loss = 0.349259, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4517348110675812, Accuracy = 0.918796718120575\n",
            "Iter #4542464:  Learning rate = 0.000796:   Batch Loss = 0.408309, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4990994930267334, Accuracy = 0.9125369787216187\n",
            "Iter #4546560:  Learning rate = 0.000796:   Batch Loss = 0.350690, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4599751830101013, Accuracy = 0.9130585789680481\n",
            "Iter #4550656:  Learning rate = 0.000796:   Batch Loss = 0.352453, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.461261510848999, Accuracy = 0.9234915375709534\n",
            "Iter #4554752:  Learning rate = 0.000796:   Batch Loss = 0.398717, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47005337476730347, Accuracy = 0.9149712920188904\n",
            "Iter #4558848:  Learning rate = 0.000796:   Batch Loss = 0.381786, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4609321653842926, Accuracy = 0.9154929518699646\n",
            "Iter #4562944:  Learning rate = 0.000796:   Batch Loss = 0.469891, Accuracy = 0.92578125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5260757207870483, Accuracy = 0.9019300937652588\n",
            "Iter #4567040:  Learning rate = 0.000796:   Batch Loss = 0.371876, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47955945134162903, Accuracy = 0.9050599932670593\n",
            "Iter #4571136:  Learning rate = 0.000796:   Batch Loss = 0.401121, Accuracy = 0.931640625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.507805585861206, Accuracy = 0.9027994871139526\n",
            "Iter #4575232:  Learning rate = 0.000796:   Batch Loss = 0.355172, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5010717511177063, Accuracy = 0.9095809459686279\n",
            "Iter #4579328:  Learning rate = 0.000796:   Batch Loss = 0.336529, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4458812475204468, Accuracy = 0.925056517124176\n",
            "Iter #4583424:  Learning rate = 0.000796:   Batch Loss = 0.365464, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4663289785385132, Accuracy = 0.9229699373245239\n",
            "Iter #4587520:  Learning rate = 0.000796:   Batch Loss = 0.377811, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.478701114654541, Accuracy = 0.9087115526199341\n",
            "Iter #4591616:  Learning rate = 0.000796:   Batch Loss = 0.367348, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44477522373199463, Accuracy = 0.9186228513717651\n",
            "Iter #4595712:  Learning rate = 0.000796:   Batch Loss = 0.390097, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45813897252082825, Accuracy = 0.9153190851211548\n",
            "Iter #4599808:  Learning rate = 0.000796:   Batch Loss = 0.363301, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45377838611602783, Accuracy = 0.9193183779716492\n",
            "Iter #4603904:  Learning rate = 0.000765:   Batch Loss = 0.358743, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4589870572090149, Accuracy = 0.9240131974220276\n",
            "Iter #4608000:  Learning rate = 0.000765:   Batch Loss = 0.337858, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46556925773620605, Accuracy = 0.9153190851211548\n",
            "Iter #4612096:  Learning rate = 0.000765:   Batch Loss = 0.466774, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.6479912400245667, Accuracy = 0.8655886054039001\n",
            "Iter #4616192:  Learning rate = 0.000765:   Batch Loss = 0.344639, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47353464365005493, Accuracy = 0.90801602602005\n",
            "Iter #4620288:  Learning rate = 0.000765:   Batch Loss = 0.379455, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4805179238319397, Accuracy = 0.9081898927688599\n",
            "Iter #4624384:  Learning rate = 0.000765:   Batch Loss = 0.406074, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47325700521469116, Accuracy = 0.9095809459686279\n",
            "Iter #4628480:  Learning rate = 0.000765:   Batch Loss = 0.351649, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46364539861679077, Accuracy = 0.9137541055679321\n",
            "Iter #4632576:  Learning rate = 0.000765:   Batch Loss = 0.327720, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44218575954437256, Accuracy = 0.9189705848693848\n",
            "Iter #4636672:  Learning rate = 0.000765:   Batch Loss = 0.361045, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45714515447616577, Accuracy = 0.9207094311714172\n",
            "Iter #4640768:  Learning rate = 0.000765:   Batch Loss = 0.362870, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.441765159368515, Accuracy = 0.9191445112228394\n",
            "Iter #4644864:  Learning rate = 0.000765:   Batch Loss = 0.351778, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45692139863967896, Accuracy = 0.9266214370727539\n",
            "Iter #4648960:  Learning rate = 0.000765:   Batch Loss = 0.344964, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4281983971595764, Accuracy = 0.9229699373245239\n",
            "Iter #4653056:  Learning rate = 0.000765:   Batch Loss = 0.343596, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.438246488571167, Accuracy = 0.9267953634262085\n",
            "Iter #4657152:  Learning rate = 0.000765:   Batch Loss = 0.353623, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44083520770072937, Accuracy = 0.9233176708221436\n",
            "Iter #4661248:  Learning rate = 0.000765:   Batch Loss = 0.311636, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4448886513710022, Accuracy = 0.9200139045715332\n",
            "Iter #4665344:  Learning rate = 0.000765:   Batch Loss = 0.345601, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4661141633987427, Accuracy = 0.9181011915206909\n",
            "Iter #4669440:  Learning rate = 0.000765:   Batch Loss = 0.322271, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45127034187316895, Accuracy = 0.920187771320343\n",
            "Iter #4673536:  Learning rate = 0.000765:   Batch Loss = 0.339792, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46173369884490967, Accuracy = 0.9141018986701965\n",
            "Iter #4677632:  Learning rate = 0.000765:   Batch Loss = 0.418683, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4576501250267029, Accuracy = 0.9170579314231873\n",
            "Iter #4681728:  Learning rate = 0.000765:   Batch Loss = 0.350169, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4331432282924652, Accuracy = 0.9281864166259766\n",
            "Iter #4685824:  Learning rate = 0.000765:   Batch Loss = 0.387299, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46543365716934204, Accuracy = 0.9144496321678162\n",
            "Iter #4689920:  Learning rate = 0.000765:   Batch Loss = 0.372916, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47015106678009033, Accuracy = 0.9120153188705444\n",
            "Iter #4694016:  Learning rate = 0.000765:   Batch Loss = 0.412752, Accuracy = 0.9375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5195883512496948, Accuracy = 0.9026256203651428\n",
            "Iter #4698112:  Learning rate = 0.000765:   Batch Loss = 0.373584, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4665680527687073, Accuracy = 0.9097548127174377\n",
            "Iter #4702208:  Learning rate = 0.000734:   Batch Loss = 0.357312, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46847331523895264, Accuracy = 0.9149712920188904\n",
            "Iter #4706304:  Learning rate = 0.000734:   Batch Loss = 0.374587, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44353175163269043, Accuracy = 0.919492244720459\n",
            "Iter #4710400:  Learning rate = 0.000734:   Batch Loss = 0.362340, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43235039710998535, Accuracy = 0.9207094311714172\n",
            "Iter #4714496:  Learning rate = 0.000734:   Batch Loss = 0.360341, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43911463022232056, Accuracy = 0.9267953634262085\n",
            "Iter #4718592:  Learning rate = 0.000734:   Batch Loss = 0.329360, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4401916563510895, Accuracy = 0.9203616976737976\n",
            "Iter #4722688:  Learning rate = 0.000734:   Batch Loss = 0.350840, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41919854283332825, Accuracy = 0.9344461560249329\n",
            "Iter #4726784:  Learning rate = 0.000734:   Batch Loss = 0.342728, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43109044432640076, Accuracy = 0.9316640496253967\n",
            "Iter #4730880:  Learning rate = 0.000734:   Batch Loss = 0.334680, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4564996361732483, Accuracy = 0.919492244720459\n",
            "Iter #4734976:  Learning rate = 0.000734:   Batch Loss = 0.364799, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47228240966796875, Accuracy = 0.9099286794662476\n",
            "Iter #4739072:  Learning rate = 0.000734:   Batch Loss = 0.371211, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4543842375278473, Accuracy = 0.9167101383209229\n",
            "Iter #4743168:  Learning rate = 0.000734:   Batch Loss = 0.341115, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4229075312614441, Accuracy = 0.9340984225273132\n",
            "Iter #4747264:  Learning rate = 0.000734:   Batch Loss = 0.336392, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4424913227558136, Accuracy = 0.9210572242736816\n",
            "Iter #4751360:  Learning rate = 0.000734:   Batch Loss = 0.368537, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4448782205581665, Accuracy = 0.9179273247718811\n",
            "Iter #4755456:  Learning rate = 0.000734:   Batch Loss = 0.350884, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4600483477115631, Accuracy = 0.9245348572731018\n",
            "Iter #4759552:  Learning rate = 0.000734:   Batch Loss = 0.347265, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4783044457435608, Accuracy = 0.9092331528663635\n",
            "Iter #4763648:  Learning rate = 0.000734:   Batch Loss = 0.366211, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44042569398880005, Accuracy = 0.9156668186187744\n",
            "Iter #4767744:  Learning rate = 0.000734:   Batch Loss = 0.327871, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46645236015319824, Accuracy = 0.9141018986701965\n",
            "Iter #4771840:  Learning rate = 0.000734:   Batch Loss = 0.382522, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47280722856521606, Accuracy = 0.910276472568512\n",
            "Iter #4775936:  Learning rate = 0.000734:   Batch Loss = 0.360588, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4692651033401489, Accuracy = 0.9127108454704285\n",
            "Iter #4780032:  Learning rate = 0.000734:   Batch Loss = 0.337505, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.47244682908058167, Accuracy = 0.9146235585212708\n",
            "Iter #4784128:  Learning rate = 0.000734:   Batch Loss = 0.327977, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4581935405731201, Accuracy = 0.9179273247718811\n",
            "Iter #4788224:  Learning rate = 0.000734:   Batch Loss = 0.375361, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44611918926239014, Accuracy = 0.9207094311714172\n",
            "Iter #4792320:  Learning rate = 0.000734:   Batch Loss = 0.391991, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43946927785873413, Accuracy = 0.9189705848693848\n",
            "Iter #4796416:  Learning rate = 0.000734:   Batch Loss = 0.365652, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4449082911014557, Accuracy = 0.9153190851211548\n",
            "Iter #4800512:  Learning rate = 0.000705:   Batch Loss = 0.327403, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4270634055137634, Accuracy = 0.9234915375709534\n",
            "Iter #4804608:  Learning rate = 0.000705:   Batch Loss = 0.359839, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4534400403499603, Accuracy = 0.9264475703239441\n",
            "Iter #4808704:  Learning rate = 0.000705:   Batch Loss = 0.343535, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45189356803894043, Accuracy = 0.923665463924408\n",
            "Iter #4812800:  Learning rate = 0.000705:   Batch Loss = 0.324462, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45603346824645996, Accuracy = 0.9212310910224915\n",
            "Iter #4816896:  Learning rate = 0.000705:   Batch Loss = 0.347253, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4470090866088867, Accuracy = 0.9287080764770508\n",
            "Iter #4820992:  Learning rate = 0.000705:   Batch Loss = 0.330502, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4477425217628479, Accuracy = 0.9219266176223755\n",
            "Iter #4825088:  Learning rate = 0.000705:   Batch Loss = 0.322742, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4264504909515381, Accuracy = 0.932881236076355\n",
            "Iter #4829184:  Learning rate = 0.000705:   Batch Loss = 0.312671, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4440983533859253, Accuracy = 0.9307946562767029\n",
            "Iter #4833280:  Learning rate = 0.000705:   Batch Loss = 0.325231, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4383043050765991, Accuracy = 0.9238393306732178\n",
            "Iter #4837376:  Learning rate = 0.000705:   Batch Loss = 0.347326, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44800475239753723, Accuracy = 0.9271430969238281\n",
            "Iter #4841472:  Learning rate = 0.000705:   Batch Loss = 0.338846, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4446829557418823, Accuracy = 0.9181011915206909\n",
            "Iter #4845568:  Learning rate = 0.000705:   Batch Loss = 0.369378, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44826436042785645, Accuracy = 0.9170579314231873\n",
            "Iter #4849664:  Learning rate = 0.000705:   Batch Loss = 0.351145, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44978421926498413, Accuracy = 0.915840744972229\n",
            "Iter #4853760:  Learning rate = 0.000705:   Batch Loss = 0.326369, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4418211877346039, Accuracy = 0.9262737035751343\n",
            "Iter #4857856:  Learning rate = 0.000705:   Batch Loss = 0.338756, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4569248557090759, Accuracy = 0.9229699373245239\n",
            "Iter #4861952:  Learning rate = 0.000705:   Batch Loss = 0.339083, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42961299419403076, Accuracy = 0.9248826503753662\n",
            "Iter #4866048:  Learning rate = 0.000705:   Batch Loss = 0.354309, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.447226345539093, Accuracy = 0.9260998368263245\n",
            "Iter #4870144:  Learning rate = 0.000705:   Batch Loss = 0.345341, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4587693512439728, Accuracy = 0.9288819432258606\n",
            "Iter #4874240:  Learning rate = 0.000705:   Batch Loss = 0.377138, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43755555152893066, Accuracy = 0.9233176708221436\n",
            "Iter #4878336:  Learning rate = 0.000705:   Batch Loss = 0.340054, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4587598145008087, Accuracy = 0.9221004843711853\n",
            "Iter #4882432:  Learning rate = 0.000705:   Batch Loss = 0.354719, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4532802700996399, Accuracy = 0.923665463924408\n",
            "Iter #4886528:  Learning rate = 0.000705:   Batch Loss = 0.333039, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44067370891571045, Accuracy = 0.9281864166259766\n",
            "Iter #4890624:  Learning rate = 0.000705:   Batch Loss = 0.332838, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44283562898635864, Accuracy = 0.9283602833747864\n",
            "Iter #4894720:  Learning rate = 0.000705:   Batch Loss = 0.347351, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5391889810562134, Accuracy = 0.901756227016449\n",
            "Iter #4898816:  Learning rate = 0.000705:   Batch Loss = 0.365417, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.431268572807312, Accuracy = 0.9229699373245239\n",
            "Iter #4902912:  Learning rate = 0.000676:   Batch Loss = 0.326345, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46729665994644165, Accuracy = 0.9114936590194702\n",
            "Iter #4907008:  Learning rate = 0.000676:   Batch Loss = 0.379153, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4753701090812683, Accuracy = 0.9154929518699646\n",
            "Iter #4911104:  Learning rate = 0.000676:   Batch Loss = 0.336289, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4306882619857788, Accuracy = 0.9245348572731018\n",
            "Iter #4915200:  Learning rate = 0.000676:   Batch Loss = 0.348629, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44267767667770386, Accuracy = 0.9198400378227234\n",
            "Iter #4919296:  Learning rate = 0.000676:   Batch Loss = 0.339655, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45756399631500244, Accuracy = 0.9161884784698486\n",
            "Iter #4923392:  Learning rate = 0.000676:   Batch Loss = 0.339464, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4608552157878876, Accuracy = 0.9167101383209229\n",
            "Iter #4927488:  Learning rate = 0.000676:   Batch Loss = 0.357081, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4374341070652008, Accuracy = 0.9221004843711853\n",
            "Iter #4931584:  Learning rate = 0.000676:   Batch Loss = 0.353789, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4294302463531494, Accuracy = 0.9300991296768188\n",
            "Iter #4935680:  Learning rate = 0.000676:   Batch Loss = 0.342364, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41944611072540283, Accuracy = 0.9271430969238281\n",
            "Iter #4939776:  Learning rate = 0.000676:   Batch Loss = 0.304021, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4267400801181793, Accuracy = 0.9247087240219116\n",
            "Iter #4943872:  Learning rate = 0.000676:   Batch Loss = 0.329235, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44113877415657043, Accuracy = 0.924360990524292\n",
            "Iter #4947968:  Learning rate = 0.000676:   Batch Loss = 0.333366, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43300914764404297, Accuracy = 0.9269692301750183\n",
            "Iter #4952064:  Learning rate = 0.000676:   Batch Loss = 0.324098, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4283956289291382, Accuracy = 0.9273169636726379\n",
            "Iter #4956160:  Learning rate = 0.000676:   Batch Loss = 0.334500, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4660264849662781, Accuracy = 0.920883297920227\n",
            "Iter #4960256:  Learning rate = 0.000676:   Batch Loss = 0.326278, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43070298433303833, Accuracy = 0.9260998368263245\n",
            "Iter #4964352:  Learning rate = 0.000676:   Batch Loss = 0.302718, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4326404333114624, Accuracy = 0.9231438040733337\n",
            "Iter #4968448:  Learning rate = 0.000676:   Batch Loss = 0.329248, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4428195655345917, Accuracy = 0.9269692301750183\n",
            "Iter #4972544:  Learning rate = 0.000676:   Batch Loss = 0.344386, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4350019693374634, Accuracy = 0.9285341501235962\n",
            "Iter #4976640:  Learning rate = 0.000676:   Batch Loss = 0.323498, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43375974893569946, Accuracy = 0.9234915375709534\n",
            "Iter #4980736:  Learning rate = 0.000676:   Batch Loss = 0.340176, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4479895830154419, Accuracy = 0.9212310910224915\n",
            "Iter #4984832:  Learning rate = 0.000676:   Batch Loss = 0.362396, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.437788724899292, Accuracy = 0.9311423897743225\n",
            "Iter #4988928:  Learning rate = 0.000676:   Batch Loss = 0.352079, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46256858110427856, Accuracy = 0.9175795316696167\n",
            "Iter #4993024:  Learning rate = 0.000676:   Batch Loss = 0.326377, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45642104744911194, Accuracy = 0.9212310910224915\n",
            "Iter #4997120:  Learning rate = 0.000676:   Batch Loss = 0.363977, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4376847743988037, Accuracy = 0.924360990524292\n",
            "Iter #5001216:  Learning rate = 0.000649:   Batch Loss = 0.344032, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.434572696685791, Accuracy = 0.9179273247718811\n",
            "Iter #5005312:  Learning rate = 0.000649:   Batch Loss = 0.329935, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45721256732940674, Accuracy = 0.915840744972229\n",
            "Iter #5009408:  Learning rate = 0.000649:   Batch Loss = 0.323855, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44867074489593506, Accuracy = 0.9302729964256287\n",
            "Iter #5013504:  Learning rate = 0.000649:   Batch Loss = 0.348191, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43515825271606445, Accuracy = 0.920187771320343\n",
            "Iter #5017600:  Learning rate = 0.000649:   Batch Loss = 0.347021, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4410058259963989, Accuracy = 0.9311423897743225\n",
            "Iter #5021696:  Learning rate = 0.000649:   Batch Loss = 0.326461, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4270235300064087, Accuracy = 0.9299252033233643\n",
            "Iter #5025792:  Learning rate = 0.000649:   Batch Loss = 0.376820, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42808401584625244, Accuracy = 0.9314901828765869\n",
            "Iter #5029888:  Learning rate = 0.000649:   Batch Loss = 0.348786, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43206921219825745, Accuracy = 0.9233176708221436\n",
            "Iter #5033984:  Learning rate = 0.000649:   Batch Loss = 0.321982, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44682997465133667, Accuracy = 0.9273169636726379\n",
            "Iter #5038080:  Learning rate = 0.000649:   Batch Loss = 0.332849, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4391179382801056, Accuracy = 0.9295774698257446\n",
            "Iter #5042176:  Learning rate = 0.000649:   Batch Loss = 0.321074, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4259921610355377, Accuracy = 0.9222744107246399\n",
            "Iter #5046272:  Learning rate = 0.000649:   Batch Loss = 0.329209, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4569714069366455, Accuracy = 0.923665463924408\n",
            "Iter #5050368:  Learning rate = 0.000649:   Batch Loss = 0.333074, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45883071422576904, Accuracy = 0.9212310910224915\n",
            "Iter #5054464:  Learning rate = 0.000649:   Batch Loss = 0.361944, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4389031231403351, Accuracy = 0.9168840050697327\n",
            "Iter #5058560:  Learning rate = 0.000649:   Batch Loss = 0.361658, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.49873974919319153, Accuracy = 0.9097548127174377\n",
            "Iter #5062656:  Learning rate = 0.000649:   Batch Loss = 0.338304, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4625850319862366, Accuracy = 0.9146235585212708\n",
            "Iter #5066752:  Learning rate = 0.000649:   Batch Loss = 0.376576, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4262104630470276, Accuracy = 0.9227960109710693\n",
            "Iter #5070848:  Learning rate = 0.000649:   Batch Loss = 0.315640, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43065112829208374, Accuracy = 0.9260998368263245\n",
            "Iter #5074944:  Learning rate = 0.000649:   Batch Loss = 0.383049, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4330143928527832, Accuracy = 0.9245348572731018\n",
            "Iter #5079040:  Learning rate = 0.000649:   Batch Loss = 0.337506, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43310511112213135, Accuracy = 0.9231438040733337\n",
            "Iter #5083136:  Learning rate = 0.000649:   Batch Loss = 0.366030, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45959991216659546, Accuracy = 0.9203616976737976\n",
            "Iter #5087232:  Learning rate = 0.000649:   Batch Loss = 0.327886, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43665534257888794, Accuracy = 0.9240131974220276\n",
            "Iter #5091328:  Learning rate = 0.000649:   Batch Loss = 0.331117, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44710707664489746, Accuracy = 0.9168840050697327\n",
            "Iter #5095424:  Learning rate = 0.000649:   Batch Loss = 0.330359, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4298321008682251, Accuracy = 0.9227960109710693\n",
            "Iter #5099520:  Learning rate = 0.000649:   Batch Loss = 0.344107, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4393253028392792, Accuracy = 0.9271430969238281\n",
            "Iter #5103616:  Learning rate = 0.000623:   Batch Loss = 0.348984, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43070146441459656, Accuracy = 0.9238393306732178\n",
            "Iter #5107712:  Learning rate = 0.000623:   Batch Loss = 0.353400, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4376097023487091, Accuracy = 0.9240131974220276\n",
            "Iter #5111808:  Learning rate = 0.000623:   Batch Loss = 0.326187, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43147802352905273, Accuracy = 0.9276647567749023\n",
            "Iter #5115904:  Learning rate = 0.000623:   Batch Loss = 0.313671, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43693822622299194, Accuracy = 0.9273169636726379\n",
            "Iter #5120000:  Learning rate = 0.000623:   Batch Loss = 0.303288, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42639610171318054, Accuracy = 0.9316640496253967\n",
            "Iter #5124096:  Learning rate = 0.000623:   Batch Loss = 0.352727, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4371631145477295, Accuracy = 0.9300991296768188\n",
            "Iter #5128192:  Learning rate = 0.000623:   Batch Loss = 0.353777, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43847694993019104, Accuracy = 0.9193183779716492\n",
            "Iter #5132288:  Learning rate = 0.000623:   Batch Loss = 0.341911, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45227909088134766, Accuracy = 0.920883297920227\n",
            "Iter #5136384:  Learning rate = 0.000623:   Batch Loss = 0.343438, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4651186168193817, Accuracy = 0.9128847122192383\n",
            "Iter #5140480:  Learning rate = 0.000623:   Batch Loss = 0.340683, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42711150646209717, Accuracy = 0.9240131974220276\n",
            "Iter #5144576:  Learning rate = 0.000623:   Batch Loss = 0.330471, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.5275115966796875, Accuracy = 0.8991479873657227\n",
            "Iter #5148672:  Learning rate = 0.000623:   Batch Loss = 0.334548, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4460301995277405, Accuracy = 0.9247087240219116\n",
            "Iter #5152768:  Learning rate = 0.000623:   Batch Loss = 0.342656, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4541444480419159, Accuracy = 0.9186228513717651\n",
            "Iter #5156864:  Learning rate = 0.000623:   Batch Loss = 0.322380, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4408772587776184, Accuracy = 0.9200139045715332\n",
            "Iter #5160960:  Learning rate = 0.000623:   Batch Loss = 0.356527, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4309527575969696, Accuracy = 0.9221004843711853\n",
            "Iter #5165056:  Learning rate = 0.000623:   Batch Loss = 0.325155, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4589689075946808, Accuracy = 0.9248826503753662\n",
            "Iter #5169152:  Learning rate = 0.000623:   Batch Loss = 0.356447, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44170093536376953, Accuracy = 0.9262737035751343\n",
            "Iter #5173248:  Learning rate = 0.000623:   Batch Loss = 0.340935, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42043739557266235, Accuracy = 0.9262737035751343\n",
            "Iter #5177344:  Learning rate = 0.000623:   Batch Loss = 0.356569, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46910560131073, Accuracy = 0.9177534580230713\n",
            "Iter #5181440:  Learning rate = 0.000623:   Batch Loss = 0.329739, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4564777910709381, Accuracy = 0.9229699373245239\n",
            "Iter #5185536:  Learning rate = 0.000623:   Batch Loss = 0.364923, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4593560993671417, Accuracy = 0.9191445112228394\n",
            "Iter #5189632:  Learning rate = 0.000623:   Batch Loss = 0.388297, Accuracy = 0.93359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43873193860054016, Accuracy = 0.9227960109710693\n",
            "Iter #5193728:  Learning rate = 0.000623:   Batch Loss = 0.356668, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43162432312965393, Accuracy = 0.9231438040733337\n",
            "Iter #5197824:  Learning rate = 0.000623:   Batch Loss = 0.324641, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4252268075942993, Accuracy = 0.9233176708221436\n",
            "Iter #5201920:  Learning rate = 0.000599:   Batch Loss = 0.375722, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4514491558074951, Accuracy = 0.9175795316696167\n",
            "Iter #5206016:  Learning rate = 0.000599:   Batch Loss = 0.364483, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43941324949264526, Accuracy = 0.9241871237754822\n",
            "Iter #5210112:  Learning rate = 0.000599:   Batch Loss = 0.342517, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4316920340061188, Accuracy = 0.9205355644226074\n",
            "Iter #5214208:  Learning rate = 0.000599:   Batch Loss = 0.360171, Accuracy = 0.943359375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4486894905567169, Accuracy = 0.9267953634262085\n",
            "Iter #5218304:  Learning rate = 0.000599:   Batch Loss = 0.349306, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4177965819835663, Accuracy = 0.9304468631744385\n",
            "Iter #5222400:  Learning rate = 0.000599:   Batch Loss = 0.334530, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45067328214645386, Accuracy = 0.9179273247718811\n",
            "Iter #5226496:  Learning rate = 0.000599:   Batch Loss = 0.353543, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44009166955947876, Accuracy = 0.9170579314231873\n",
            "Iter #5230592:  Learning rate = 0.000599:   Batch Loss = 0.315481, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4453660249710083, Accuracy = 0.9229699373245239\n",
            "Iter #5234688:  Learning rate = 0.000599:   Batch Loss = 0.317625, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4214556813240051, Accuracy = 0.9274908900260925\n",
            "Iter #5238784:  Learning rate = 0.000599:   Batch Loss = 0.329234, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45023229718208313, Accuracy = 0.9170579314231873\n",
            "Iter #5242880:  Learning rate = 0.000599:   Batch Loss = 0.360439, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4404952824115753, Accuracy = 0.925056517124176\n",
            "Iter #5246976:  Learning rate = 0.000599:   Batch Loss = 0.326677, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4333110749721527, Accuracy = 0.9227960109710693\n",
            "Iter #5251072:  Learning rate = 0.000599:   Batch Loss = 0.337948, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.436675488948822, Accuracy = 0.9299252033233643\n",
            "Iter #5255168:  Learning rate = 0.000599:   Batch Loss = 0.349988, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4335765242576599, Accuracy = 0.9281864166259766\n",
            "Iter #5259264:  Learning rate = 0.000599:   Batch Loss = 0.320960, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41868722438812256, Accuracy = 0.9267953634262085\n",
            "Iter #5263360:  Learning rate = 0.000599:   Batch Loss = 0.308912, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4308620095252991, Accuracy = 0.9229699373245239\n",
            "Iter #5267456:  Learning rate = 0.000599:   Batch Loss = 0.328446, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4374431371688843, Accuracy = 0.9245348572731018\n",
            "Iter #5271552:  Learning rate = 0.000599:   Batch Loss = 0.341056, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4418144226074219, Accuracy = 0.9271430969238281\n",
            "Iter #5275648:  Learning rate = 0.000599:   Batch Loss = 0.337109, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44837117195129395, Accuracy = 0.918796718120575\n",
            "Iter #5279744:  Learning rate = 0.000599:   Batch Loss = 0.350853, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4462904632091522, Accuracy = 0.9227960109710693\n",
            "Iter #5283840:  Learning rate = 0.000599:   Batch Loss = 0.356398, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4397503733634949, Accuracy = 0.9198400378227234\n",
            "Iter #5287936:  Learning rate = 0.000599:   Batch Loss = 0.338214, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43156731128692627, Accuracy = 0.9215788841247559\n",
            "Iter #5292032:  Learning rate = 0.000599:   Batch Loss = 0.313395, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4458844065666199, Accuracy = 0.9215788841247559\n",
            "Iter #5296128:  Learning rate = 0.000599:   Batch Loss = 0.350224, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4329212009906769, Accuracy = 0.9247087240219116\n",
            "Iter #5300224:  Learning rate = 0.000575:   Batch Loss = 0.339581, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44606778025627136, Accuracy = 0.9177534580230713\n",
            "Iter #5304320:  Learning rate = 0.000575:   Batch Loss = 0.314617, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43786293268203735, Accuracy = 0.9273169636726379\n",
            "Iter #5308416:  Learning rate = 0.000575:   Batch Loss = 0.360282, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46542680263519287, Accuracy = 0.9248826503753662\n",
            "Iter #5312512:  Learning rate = 0.000575:   Batch Loss = 0.335041, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4361175298690796, Accuracy = 0.9231438040733337\n",
            "Iter #5316608:  Learning rate = 0.000575:   Batch Loss = 0.343842, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4438915252685547, Accuracy = 0.9210572242736816\n",
            "Iter #5320704:  Learning rate = 0.000575:   Batch Loss = 0.344068, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4304422438144684, Accuracy = 0.9217527508735657\n",
            "Iter #5324800:  Learning rate = 0.000575:   Batch Loss = 0.328845, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.430799663066864, Accuracy = 0.9274908900260925\n",
            "Iter #5328896:  Learning rate = 0.000575:   Batch Loss = 0.340735, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4345585107803345, Accuracy = 0.9221004843711853\n",
            "Iter #5332992:  Learning rate = 0.000575:   Batch Loss = 0.316629, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4354006350040436, Accuracy = 0.9259259104728699\n",
            "Iter #5337088:  Learning rate = 0.000575:   Batch Loss = 0.351040, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4370826482772827, Accuracy = 0.9224482774734497\n",
            "Iter #5341184:  Learning rate = 0.000575:   Batch Loss = 0.343413, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43341296911239624, Accuracy = 0.9241871237754822\n",
            "Iter #5345280:  Learning rate = 0.000575:   Batch Loss = 0.316230, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4419150948524475, Accuracy = 0.9210572242736816\n",
            "Iter #5349376:  Learning rate = 0.000575:   Batch Loss = 0.321196, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44906583428382874, Accuracy = 0.9214049577713013\n",
            "Iter #5353472:  Learning rate = 0.000575:   Batch Loss = 0.328846, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43209296464920044, Accuracy = 0.9254042506217957\n",
            "Iter #5357568:  Learning rate = 0.000575:   Batch Loss = 0.346330, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44651612639427185, Accuracy = 0.9181011915206909\n",
            "Iter #5361664:  Learning rate = 0.000575:   Batch Loss = 0.341219, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4388390779495239, Accuracy = 0.9193183779716492\n",
            "Iter #5365760:  Learning rate = 0.000575:   Batch Loss = 0.332143, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4422876834869385, Accuracy = 0.9233176708221436\n",
            "Iter #5369856:  Learning rate = 0.000575:   Batch Loss = 0.345806, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4530302584171295, Accuracy = 0.9212310910224915\n",
            "Iter #5373952:  Learning rate = 0.000575:   Batch Loss = 0.348352, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44540655612945557, Accuracy = 0.9217527508735657\n",
            "Iter #5378048:  Learning rate = 0.000575:   Batch Loss = 0.329778, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45280683040618896, Accuracy = 0.9193183779716492\n",
            "Iter #5382144:  Learning rate = 0.000575:   Batch Loss = 0.342029, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44920945167541504, Accuracy = 0.9170579314231873\n",
            "Iter #5386240:  Learning rate = 0.000575:   Batch Loss = 0.355166, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4313514232635498, Accuracy = 0.9283602833747864\n",
            "Iter #5390336:  Learning rate = 0.000575:   Batch Loss = 0.311803, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4422643184661865, Accuracy = 0.9229699373245239\n",
            "Iter #5394432:  Learning rate = 0.000575:   Batch Loss = 0.326240, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45100533962249756, Accuracy = 0.9182750582695007\n",
            "Iter #5398528:  Learning rate = 0.000575:   Batch Loss = 0.306801, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4427906274795532, Accuracy = 0.9207094311714172\n",
            "Iter #5402624:  Learning rate = 0.000552:   Batch Loss = 0.319809, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44187983870506287, Accuracy = 0.9276647567749023\n",
            "Iter #5406720:  Learning rate = 0.000552:   Batch Loss = 0.310332, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4344252347946167, Accuracy = 0.9266214370727539\n",
            "Iter #5410816:  Learning rate = 0.000552:   Batch Loss = 0.315168, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43907731771469116, Accuracy = 0.925056517124176\n",
            "Iter #5414912:  Learning rate = 0.000552:   Batch Loss = 0.337168, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4509679079055786, Accuracy = 0.9181011915206909\n",
            "Iter #5419008:  Learning rate = 0.000552:   Batch Loss = 0.341691, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43661975860595703, Accuracy = 0.923665463924408\n",
            "Iter #5423104:  Learning rate = 0.000552:   Batch Loss = 0.334036, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4330793619155884, Accuracy = 0.9226221442222595\n",
            "Iter #5427200:  Learning rate = 0.000552:   Batch Loss = 0.331085, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4453124701976776, Accuracy = 0.9240131974220276\n",
            "Iter #5431296:  Learning rate = 0.000552:   Batch Loss = 0.369612, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43263837695121765, Accuracy = 0.9269692301750183\n",
            "Iter #5435392:  Learning rate = 0.000552:   Batch Loss = 0.320416, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4429389238357544, Accuracy = 0.9241871237754822\n",
            "Iter #5439488:  Learning rate = 0.000552:   Batch Loss = 0.339963, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41782379150390625, Accuracy = 0.9252303838729858\n",
            "Iter #5443584:  Learning rate = 0.000552:   Batch Loss = 0.333776, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45472830533981323, Accuracy = 0.9191445112228394\n",
            "Iter #5447680:  Learning rate = 0.000552:   Batch Loss = 0.315550, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4355959892272949, Accuracy = 0.9186228513717651\n",
            "Iter #5451776:  Learning rate = 0.000552:   Batch Loss = 0.328823, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4406326711177826, Accuracy = 0.9266214370727539\n",
            "Iter #5455872:  Learning rate = 0.000552:   Batch Loss = 0.335528, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4529110789299011, Accuracy = 0.9210572242736816\n",
            "Iter #5459968:  Learning rate = 0.000552:   Batch Loss = 0.337470, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4195109009742737, Accuracy = 0.9334028959274292\n",
            "Iter #5464064:  Learning rate = 0.000552:   Batch Loss = 0.327286, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4381980299949646, Accuracy = 0.9257520437240601\n",
            "Iter #5468160:  Learning rate = 0.000552:   Batch Loss = 0.363495, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4494065046310425, Accuracy = 0.9161884784698486\n",
            "Iter #5472256:  Learning rate = 0.000552:   Batch Loss = 0.338624, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4273195266723633, Accuracy = 0.9245348572731018\n",
            "Iter #5476352:  Learning rate = 0.000552:   Batch Loss = 0.317350, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4502260088920593, Accuracy = 0.9193183779716492\n",
            "Iter #5480448:  Learning rate = 0.000552:   Batch Loss = 0.348674, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4331868290901184, Accuracy = 0.9247087240219116\n",
            "Iter #5484544:  Learning rate = 0.000552:   Batch Loss = 0.334136, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4352908730506897, Accuracy = 0.918796718120575\n",
            "Iter #5488640:  Learning rate = 0.000552:   Batch Loss = 0.368300, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44533997774124146, Accuracy = 0.9154929518699646\n",
            "Iter #5492736:  Learning rate = 0.000552:   Batch Loss = 0.340488, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4283158481121063, Accuracy = 0.9247087240219116\n",
            "Iter #5496832:  Learning rate = 0.000552:   Batch Loss = 0.330485, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43279144167900085, Accuracy = 0.9222744107246399\n",
            "Iter #5500928:  Learning rate = 0.000530:   Batch Loss = 0.313265, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4476676881313324, Accuracy = 0.9241871237754822\n",
            "Iter #5505024:  Learning rate = 0.000530:   Batch Loss = 0.326740, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44346165657043457, Accuracy = 0.9219266176223755\n",
            "Iter #5509120:  Learning rate = 0.000530:   Batch Loss = 0.351780, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42254316806793213, Accuracy = 0.9262737035751343\n",
            "Iter #5513216:  Learning rate = 0.000530:   Batch Loss = 0.339553, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4238053858280182, Accuracy = 0.9290558099746704\n",
            "Iter #5517312:  Learning rate = 0.000530:   Batch Loss = 0.311451, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44092997908592224, Accuracy = 0.9269692301750183\n",
            "Iter #5521408:  Learning rate = 0.000530:   Batch Loss = 0.315799, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.415050208568573, Accuracy = 0.9318379163742065\n",
            "Iter #5525504:  Learning rate = 0.000530:   Batch Loss = 0.311999, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42685961723327637, Accuracy = 0.9262737035751343\n",
            "Iter #5529600:  Learning rate = 0.000530:   Batch Loss = 0.329645, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4309837222099304, Accuracy = 0.9274908900260925\n",
            "Iter #5533696:  Learning rate = 0.000530:   Batch Loss = 0.320410, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4473247528076172, Accuracy = 0.9241871237754822\n",
            "Iter #5537792:  Learning rate = 0.000530:   Batch Loss = 0.333147, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42268240451812744, Accuracy = 0.9325334429740906\n",
            "Iter #5541888:  Learning rate = 0.000530:   Batch Loss = 0.343273, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4623957872390747, Accuracy = 0.9193183779716492\n",
            "Iter #5545984:  Learning rate = 0.000530:   Batch Loss = 0.327602, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4604942202568054, Accuracy = 0.919492244720459\n",
            "Iter #5550080:  Learning rate = 0.000530:   Batch Loss = 0.321919, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4233127236366272, Accuracy = 0.9299252033233643\n",
            "Iter #5554176:  Learning rate = 0.000530:   Batch Loss = 0.325097, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44633162021636963, Accuracy = 0.9154929518699646\n",
            "Iter #5558272:  Learning rate = 0.000530:   Batch Loss = 0.346602, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4204956293106079, Accuracy = 0.9259259104728699\n",
            "Iter #5562368:  Learning rate = 0.000530:   Batch Loss = 0.309427, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41815921664237976, Accuracy = 0.9276647567749023\n",
            "Iter #5566464:  Learning rate = 0.000530:   Batch Loss = 0.314838, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44521859288215637, Accuracy = 0.918796718120575\n",
            "Iter #5570560:  Learning rate = 0.000530:   Batch Loss = 0.320729, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44687867164611816, Accuracy = 0.9226221442222595\n",
            "Iter #5574656:  Learning rate = 0.000530:   Batch Loss = 0.310354, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4392862319946289, Accuracy = 0.9285341501235962\n",
            "Iter #5578752:  Learning rate = 0.000530:   Batch Loss = 0.305379, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4436589777469635, Accuracy = 0.9198400378227234\n",
            "Iter #5582848:  Learning rate = 0.000530:   Batch Loss = 0.299567, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42757731676101685, Accuracy = 0.9278386235237122\n",
            "Iter #5586944:  Learning rate = 0.000530:   Batch Loss = 0.324669, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4172942638397217, Accuracy = 0.923665463924408\n",
            "Iter #5591040:  Learning rate = 0.000530:   Batch Loss = 0.353823, Accuracy = 0.94140625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4288976192474365, Accuracy = 0.9257520437240601\n",
            "Iter #5595136:  Learning rate = 0.000530:   Batch Loss = 0.335532, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4310409724712372, Accuracy = 0.9212310910224915\n",
            "Iter #5599232:  Learning rate = 0.000530:   Batch Loss = 0.312340, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43174201250076294, Accuracy = 0.9247087240219116\n",
            "Iter #5603328:  Learning rate = 0.000508:   Batch Loss = 0.323853, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4387405812740326, Accuracy = 0.9248826503753662\n",
            "Iter #5607424:  Learning rate = 0.000508:   Batch Loss = 0.333972, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4378204345703125, Accuracy = 0.9215788841247559\n",
            "Iter #5611520:  Learning rate = 0.000508:   Batch Loss = 0.336142, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4410678744316101, Accuracy = 0.9307946562767029\n",
            "Iter #5615616:  Learning rate = 0.000508:   Batch Loss = 0.341577, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4500468969345093, Accuracy = 0.9240131974220276\n",
            "Iter #5619712:  Learning rate = 0.000508:   Batch Loss = 0.323481, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4263451099395752, Accuracy = 0.9248826503753662\n",
            "Iter #5623808:  Learning rate = 0.000508:   Batch Loss = 0.341002, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43963003158569336, Accuracy = 0.9234915375709534\n",
            "Iter #5627904:  Learning rate = 0.000508:   Batch Loss = 0.341856, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41864460706710815, Accuracy = 0.9254042506217957\n",
            "Iter #5632000:  Learning rate = 0.000508:   Batch Loss = 0.334392, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42974191904067993, Accuracy = 0.923665463924408\n",
            "Iter #5636096:  Learning rate = 0.000508:   Batch Loss = 0.307512, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42489200830459595, Accuracy = 0.933576762676239\n",
            "Iter #5640192:  Learning rate = 0.000508:   Batch Loss = 0.328299, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4185318946838379, Accuracy = 0.9297513365745544\n",
            "Iter #5644288:  Learning rate = 0.000508:   Batch Loss = 0.315075, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4339759945869446, Accuracy = 0.9262737035751343\n",
            "Iter #5648384:  Learning rate = 0.000508:   Batch Loss = 0.346639, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4516962170600891, Accuracy = 0.9181011915206909\n",
            "Iter #5652480:  Learning rate = 0.000508:   Batch Loss = 0.339915, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42624279856681824, Accuracy = 0.9306207895278931\n",
            "Iter #5656576:  Learning rate = 0.000508:   Batch Loss = 0.332954, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4241185784339905, Accuracy = 0.9294036030769348\n",
            "Iter #5660672:  Learning rate = 0.000508:   Batch Loss = 0.323836, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4249035120010376, Accuracy = 0.9304468631744385\n",
            "Iter #5664768:  Learning rate = 0.000508:   Batch Loss = 0.317858, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4199150502681732, Accuracy = 0.9302729964256287\n",
            "Iter #5668864:  Learning rate = 0.000508:   Batch Loss = 0.336084, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4402819275856018, Accuracy = 0.9214049577713013\n",
            "Iter #5672960:  Learning rate = 0.000508:   Batch Loss = 0.333619, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4308718740940094, Accuracy = 0.9248826503753662\n",
            "Iter #5677056:  Learning rate = 0.000508:   Batch Loss = 0.329446, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4439944624900818, Accuracy = 0.9278386235237122\n",
            "Iter #5681152:  Learning rate = 0.000508:   Batch Loss = 0.324946, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4503537118434906, Accuracy = 0.9245348572731018\n",
            "Iter #5685248:  Learning rate = 0.000508:   Batch Loss = 0.334295, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4307712912559509, Accuracy = 0.9255781769752502\n",
            "Iter #5689344:  Learning rate = 0.000508:   Batch Loss = 0.336741, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44945472478866577, Accuracy = 0.923665463924408\n",
            "Iter #5693440:  Learning rate = 0.000508:   Batch Loss = 0.318837, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43069708347320557, Accuracy = 0.9231438040733337\n",
            "Iter #5697536:  Learning rate = 0.000508:   Batch Loss = 0.315865, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43359386920928955, Accuracy = 0.9231438040733337\n",
            "Iter #5701632:  Learning rate = 0.000488:   Batch Loss = 0.344618, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.46068593859672546, Accuracy = 0.9154929518699646\n",
            "Iter #5705728:  Learning rate = 0.000488:   Batch Loss = 0.346796, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42444926500320435, Accuracy = 0.9311423897743225\n",
            "Iter #5709824:  Learning rate = 0.000488:   Batch Loss = 0.325086, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42957550287246704, Accuracy = 0.9307946562767029\n",
            "Iter #5713920:  Learning rate = 0.000488:   Batch Loss = 0.302510, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42523717880249023, Accuracy = 0.923665463924408\n",
            "Iter #5718016:  Learning rate = 0.000488:   Batch Loss = 0.330870, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.426438570022583, Accuracy = 0.9226221442222595\n",
            "Iter #5722112:  Learning rate = 0.000488:   Batch Loss = 0.339931, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42922502756118774, Accuracy = 0.9241871237754822\n",
            "Iter #5726208:  Learning rate = 0.000488:   Batch Loss = 0.313832, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43408069014549255, Accuracy = 0.9229699373245239\n",
            "Iter #5730304:  Learning rate = 0.000488:   Batch Loss = 0.312127, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41927748918533325, Accuracy = 0.9276647567749023\n",
            "Iter #5734400:  Learning rate = 0.000488:   Batch Loss = 0.329842, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42437613010406494, Accuracy = 0.9267953634262085\n",
            "Iter #5738496:  Learning rate = 0.000488:   Batch Loss = 0.317402, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4220162034034729, Accuracy = 0.9274908900260925\n",
            "Iter #5742592:  Learning rate = 0.000488:   Batch Loss = 0.310772, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42941561341285706, Accuracy = 0.9231438040733337\n",
            "Iter #5746688:  Learning rate = 0.000488:   Batch Loss = 0.326363, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42982667684555054, Accuracy = 0.9300991296768188\n",
            "Iter #5750784:  Learning rate = 0.000488:   Batch Loss = 0.349197, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4288901686668396, Accuracy = 0.9247087240219116\n",
            "Iter #5754880:  Learning rate = 0.000488:   Batch Loss = 0.310516, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43322885036468506, Accuracy = 0.9276647567749023\n",
            "Iter #5758976:  Learning rate = 0.000488:   Batch Loss = 0.310812, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4199666380882263, Accuracy = 0.9306207895278931\n",
            "Iter #5763072:  Learning rate = 0.000488:   Batch Loss = 0.316268, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42839550971984863, Accuracy = 0.9269692301750183\n",
            "Iter #5767168:  Learning rate = 0.000488:   Batch Loss = 0.284787, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4293922185897827, Accuracy = 0.9295774698257446\n",
            "Iter #5771264:  Learning rate = 0.000488:   Batch Loss = 0.332065, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4227316379547119, Accuracy = 0.9266214370727539\n",
            "Iter #5775360:  Learning rate = 0.000488:   Batch Loss = 0.302224, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4288199543952942, Accuracy = 0.9224482774734497\n",
            "Iter #5779456:  Learning rate = 0.000488:   Batch Loss = 0.321806, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43215692043304443, Accuracy = 0.923665463924408\n",
            "Iter #5783552:  Learning rate = 0.000488:   Batch Loss = 0.321192, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4536477327346802, Accuracy = 0.9170579314231873\n",
            "Iter #5787648:  Learning rate = 0.000488:   Batch Loss = 0.335184, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.447742760181427, Accuracy = 0.9219266176223755\n",
            "Iter #5791744:  Learning rate = 0.000488:   Batch Loss = 0.335883, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4237673282623291, Accuracy = 0.9302729964256287\n",
            "Iter #5795840:  Learning rate = 0.000488:   Batch Loss = 0.308836, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4096769094467163, Accuracy = 0.9295774698257446\n",
            "Iter #5799936:  Learning rate = 0.000488:   Batch Loss = 0.329400, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43279242515563965, Accuracy = 0.9267953634262085\n",
            "Iter #5804032:  Learning rate = 0.000468:   Batch Loss = 0.322671, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41411292552948, Accuracy = 0.9260998368263245\n",
            "Iter #5808128:  Learning rate = 0.000468:   Batch Loss = 0.308807, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42099472880363464, Accuracy = 0.9255781769752502\n",
            "Iter #5812224:  Learning rate = 0.000468:   Batch Loss = 0.320176, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42941176891326904, Accuracy = 0.924360990524292\n",
            "Iter #5816320:  Learning rate = 0.000468:   Batch Loss = 0.298942, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44047811627388, Accuracy = 0.9245348572731018\n",
            "Iter #5820416:  Learning rate = 0.000468:   Batch Loss = 0.331850, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4277295768260956, Accuracy = 0.9271430969238281\n",
            "Iter #5824512:  Learning rate = 0.000468:   Batch Loss = 0.353238, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4652292728424072, Accuracy = 0.9196661710739136\n",
            "Iter #5828608:  Learning rate = 0.000468:   Batch Loss = 0.302408, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4185774326324463, Accuracy = 0.9229699373245239\n",
            "Iter #5832704:  Learning rate = 0.000468:   Batch Loss = 0.310847, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4208317697048187, Accuracy = 0.923665463924408\n",
            "Iter #5836800:  Learning rate = 0.000468:   Batch Loss = 0.312277, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4250064790248871, Accuracy = 0.9259259104728699\n",
            "Iter #5840896:  Learning rate = 0.000468:   Batch Loss = 0.320162, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4125455617904663, Accuracy = 0.9314901828765869\n",
            "Iter #5844992:  Learning rate = 0.000468:   Batch Loss = 0.319057, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4324723482131958, Accuracy = 0.9233176708221436\n",
            "Iter #5849088:  Learning rate = 0.000468:   Batch Loss = 0.307616, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42726758122444153, Accuracy = 0.9269692301750183\n",
            "Iter #5853184:  Learning rate = 0.000468:   Batch Loss = 0.344766, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44178104400634766, Accuracy = 0.9255781769752502\n",
            "Iter #5857280:  Learning rate = 0.000468:   Batch Loss = 0.347057, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43743839859962463, Accuracy = 0.9196661710739136\n",
            "Iter #5861376:  Learning rate = 0.000468:   Batch Loss = 0.347802, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4167928695678711, Accuracy = 0.9346200823783875\n",
            "Iter #5865472:  Learning rate = 0.000468:   Batch Loss = 0.351379, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44077610969543457, Accuracy = 0.9227960109710693\n",
            "Iter #5869568:  Learning rate = 0.000468:   Batch Loss = 0.354515, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4248194992542267, Accuracy = 0.924360990524292\n",
            "Iter #5873664:  Learning rate = 0.000468:   Batch Loss = 0.343015, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4272184371948242, Accuracy = 0.9231438040733337\n",
            "Iter #5877760:  Learning rate = 0.000468:   Batch Loss = 0.290635, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4146769046783447, Accuracy = 0.9283602833747864\n",
            "Iter #5881856:  Learning rate = 0.000468:   Batch Loss = 0.317878, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4220174551010132, Accuracy = 0.9309685230255127\n",
            "Iter #5885952:  Learning rate = 0.000468:   Batch Loss = 0.317528, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4176000952720642, Accuracy = 0.9281864166259766\n",
            "Iter #5890048:  Learning rate = 0.000468:   Batch Loss = 0.319984, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4250285029411316, Accuracy = 0.9269692301750183\n",
            "Iter #5894144:  Learning rate = 0.000468:   Batch Loss = 0.332510, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4293762147426605, Accuracy = 0.9285341501235962\n",
            "Iter #5898240:  Learning rate = 0.000468:   Batch Loss = 0.309238, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.420335590839386, Accuracy = 0.9288819432258606\n",
            "Iter #5902336:  Learning rate = 0.000450:   Batch Loss = 0.333628, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4175616204738617, Accuracy = 0.9297513365745544\n",
            "Iter #5906432:  Learning rate = 0.000450:   Batch Loss = 0.332191, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4222000241279602, Accuracy = 0.9221004843711853\n",
            "Iter #5910528:  Learning rate = 0.000450:   Batch Loss = 0.298935, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4296617805957794, Accuracy = 0.9274908900260925\n",
            "Iter #5914624:  Learning rate = 0.000450:   Batch Loss = 0.315140, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4138524532318115, Accuracy = 0.9320118427276611\n",
            "Iter #5918720:  Learning rate = 0.000450:   Batch Loss = 0.332039, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43133026361465454, Accuracy = 0.925056517124176\n",
            "Iter #5922816:  Learning rate = 0.000450:   Batch Loss = 0.333886, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4457622766494751, Accuracy = 0.9196661710739136\n",
            "Iter #5926912:  Learning rate = 0.000450:   Batch Loss = 0.333622, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4272327423095703, Accuracy = 0.9252303838729858\n",
            "Iter #5931008:  Learning rate = 0.000450:   Batch Loss = 0.299069, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42044588923454285, Accuracy = 0.9299252033233643\n",
            "Iter #5935104:  Learning rate = 0.000450:   Batch Loss = 0.299840, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4380268454551697, Accuracy = 0.9306207895278931\n",
            "Iter #5939200:  Learning rate = 0.000450:   Batch Loss = 0.309054, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41210949420928955, Accuracy = 0.9327073693275452\n",
            "Iter #5943296:  Learning rate = 0.000450:   Batch Loss = 0.305405, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4105636477470398, Accuracy = 0.9281864166259766\n",
            "Iter #5947392:  Learning rate = 0.000450:   Batch Loss = 0.328544, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4242594540119171, Accuracy = 0.9349678158760071\n",
            "Iter #5951488:  Learning rate = 0.000450:   Batch Loss = 0.307820, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43640246987342834, Accuracy = 0.9287080764770508\n",
            "Iter #5955584:  Learning rate = 0.000450:   Batch Loss = 0.311386, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42880678176879883, Accuracy = 0.925056517124176\n",
            "Iter #5959680:  Learning rate = 0.000450:   Batch Loss = 0.322739, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4236617088317871, Accuracy = 0.9297513365745544\n",
            "Iter #5963776:  Learning rate = 0.000450:   Batch Loss = 0.304158, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43494105339050293, Accuracy = 0.924360990524292\n",
            "Iter #5967872:  Learning rate = 0.000450:   Batch Loss = 0.328969, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4411473870277405, Accuracy = 0.9262737035751343\n",
            "Iter #5971968:  Learning rate = 0.000450:   Batch Loss = 0.321079, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4175291061401367, Accuracy = 0.9295774698257446\n",
            "Iter #5976064:  Learning rate = 0.000450:   Batch Loss = 0.333446, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42317163944244385, Accuracy = 0.9281864166259766\n",
            "Iter #5980160:  Learning rate = 0.000450:   Batch Loss = 0.308675, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.424135684967041, Accuracy = 0.9281864166259766\n",
            "Iter #5984256:  Learning rate = 0.000450:   Batch Loss = 0.332632, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4336509108543396, Accuracy = 0.9238393306732178\n",
            "Iter #5988352:  Learning rate = 0.000450:   Batch Loss = 0.361353, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4273850917816162, Accuracy = 0.9252303838729858\n",
            "Iter #5992448:  Learning rate = 0.000450:   Batch Loss = 0.324973, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41547060012817383, Accuracy = 0.9306207895278931\n",
            "Iter #5996544:  Learning rate = 0.000450:   Batch Loss = 0.294148, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4303486943244934, Accuracy = 0.9229699373245239\n",
            "Iter #6000640:  Learning rate = 0.000432:   Batch Loss = 0.293681, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4109840393066406, Accuracy = 0.9281864166259766\n",
            "Iter #6004736:  Learning rate = 0.000432:   Batch Loss = 0.297888, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.40838849544525146, Accuracy = 0.9285341501235962\n",
            "Iter #6008832:  Learning rate = 0.000432:   Batch Loss = 0.299542, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4372285306453705, Accuracy = 0.9314901828765869\n",
            "Iter #6012928:  Learning rate = 0.000432:   Batch Loss = 0.332914, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41845548152923584, Accuracy = 0.9340984225273132\n",
            "Iter #6017024:  Learning rate = 0.000432:   Batch Loss = 0.325539, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41499683260917664, Accuracy = 0.9285341501235962\n",
            "Iter #6021120:  Learning rate = 0.000432:   Batch Loss = 0.312547, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4320511817932129, Accuracy = 0.9295774698257446\n",
            "Iter #6025216:  Learning rate = 0.000432:   Batch Loss = 0.327635, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43166959285736084, Accuracy = 0.9316640496253967\n",
            "Iter #6029312:  Learning rate = 0.000432:   Batch Loss = 0.341825, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.413669228553772, Accuracy = 0.9273169636726379\n",
            "Iter #6033408:  Learning rate = 0.000432:   Batch Loss = 0.335362, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.422415167093277, Accuracy = 0.9260998368263245\n",
            "Iter #6037504:  Learning rate = 0.000432:   Batch Loss = 0.323713, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42262065410614014, Accuracy = 0.9297513365745544\n",
            "Iter #6041600:  Learning rate = 0.000432:   Batch Loss = 0.308895, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42906486988067627, Accuracy = 0.9264475703239441\n",
            "Iter #6045696:  Learning rate = 0.000432:   Batch Loss = 0.315793, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4172022342681885, Accuracy = 0.9370543956756592\n",
            "Iter #6049792:  Learning rate = 0.000432:   Batch Loss = 0.327958, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41950511932373047, Accuracy = 0.9266214370727539\n",
            "Iter #6053888:  Learning rate = 0.000432:   Batch Loss = 0.324649, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.425545871257782, Accuracy = 0.9264475703239441\n",
            "Iter #6057984:  Learning rate = 0.000432:   Batch Loss = 0.329740, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42051881551742554, Accuracy = 0.9334028959274292\n",
            "Iter #6062080:  Learning rate = 0.000432:   Batch Loss = 0.294977, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42362579703330994, Accuracy = 0.9262737035751343\n",
            "Iter #6066176:  Learning rate = 0.000432:   Batch Loss = 0.320836, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.410804808139801, Accuracy = 0.9325334429740906\n",
            "Iter #6070272:  Learning rate = 0.000432:   Batch Loss = 0.305443, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4186778664588928, Accuracy = 0.9274908900260925\n",
            "Iter #6074368:  Learning rate = 0.000432:   Batch Loss = 0.335392, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42517369985580444, Accuracy = 0.9292296767234802\n",
            "Iter #6078464:  Learning rate = 0.000432:   Batch Loss = 0.320887, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4087996482849121, Accuracy = 0.9313163161277771\n",
            "Iter #6082560:  Learning rate = 0.000432:   Batch Loss = 0.311327, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.440391480922699, Accuracy = 0.9248826503753662\n",
            "Iter #6086656:  Learning rate = 0.000432:   Batch Loss = 0.337317, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4157331883907318, Accuracy = 0.9365327954292297\n",
            "Iter #6090752:  Learning rate = 0.000432:   Batch Loss = 0.319565, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42473024129867554, Accuracy = 0.9274908900260925\n",
            "Iter #6094848:  Learning rate = 0.000432:   Batch Loss = 0.314973, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4151199162006378, Accuracy = 0.9351417422294617\n",
            "Iter #6098944:  Learning rate = 0.000432:   Batch Loss = 0.309736, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41336071491241455, Accuracy = 0.9290558099746704\n",
            "Iter #6103040:  Learning rate = 0.000414:   Batch Loss = 0.295887, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41263800859451294, Accuracy = 0.933576762676239\n",
            "Iter #6107136:  Learning rate = 0.000414:   Batch Loss = 0.291509, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41528230905532837, Accuracy = 0.9294036030769348\n",
            "Iter #6111232:  Learning rate = 0.000414:   Batch Loss = 0.293149, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4122350215911865, Accuracy = 0.9273169636726379\n",
            "Iter #6115328:  Learning rate = 0.000414:   Batch Loss = 0.321701, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42206162214279175, Accuracy = 0.9295774698257446\n",
            "Iter #6119424:  Learning rate = 0.000414:   Batch Loss = 0.300456, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4147467315196991, Accuracy = 0.9340984225273132\n",
            "Iter #6123520:  Learning rate = 0.000414:   Batch Loss = 0.342790, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4029581546783447, Accuracy = 0.9334028959274292\n",
            "Iter #6127616:  Learning rate = 0.000414:   Batch Loss = 0.346694, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4222625494003296, Accuracy = 0.932881236076355\n",
            "Iter #6131712:  Learning rate = 0.000414:   Batch Loss = 0.311257, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4292938709259033, Accuracy = 0.9325334429740906\n",
            "Iter #6135808:  Learning rate = 0.000414:   Batch Loss = 0.356406, Accuracy = 0.9453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41179001331329346, Accuracy = 0.9311423897743225\n",
            "Iter #6139904:  Learning rate = 0.000414:   Batch Loss = 0.309767, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4186124801635742, Accuracy = 0.9300991296768188\n",
            "Iter #6144000:  Learning rate = 0.000414:   Batch Loss = 0.313921, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4201878309249878, Accuracy = 0.9294036030769348\n",
            "Iter #6148096:  Learning rate = 0.000414:   Batch Loss = 0.329337, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4149506092071533, Accuracy = 0.9304468631744385\n",
            "Iter #6152192:  Learning rate = 0.000414:   Batch Loss = 0.336114, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4262632727622986, Accuracy = 0.9227960109710693\n",
            "Iter #6156288:  Learning rate = 0.000414:   Batch Loss = 0.281449, Accuracy = 0.982421875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4331241250038147, Accuracy = 0.9252303838729858\n",
            "Iter #6160384:  Learning rate = 0.000414:   Batch Loss = 0.340710, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4279296398162842, Accuracy = 0.9294036030769348\n",
            "Iter #6164480:  Learning rate = 0.000414:   Batch Loss = 0.304834, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42485278844833374, Accuracy = 0.9266214370727539\n",
            "Iter #6168576:  Learning rate = 0.000414:   Batch Loss = 0.307400, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4136529564857483, Accuracy = 0.9320118427276611\n",
            "Iter #6172672:  Learning rate = 0.000414:   Batch Loss = 0.330156, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4222283959388733, Accuracy = 0.9281864166259766\n",
            "Iter #6176768:  Learning rate = 0.000414:   Batch Loss = 0.350813, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.48120278120040894, Accuracy = 0.9036689400672913\n",
            "Iter #6180864:  Learning rate = 0.000414:   Batch Loss = 0.336077, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4508224129676819, Accuracy = 0.9092331528663635\n",
            "Iter #6184960:  Learning rate = 0.000414:   Batch Loss = 0.330631, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.45390596985816956, Accuracy = 0.9130585789680481\n",
            "Iter #6189056:  Learning rate = 0.000414:   Batch Loss = 0.337158, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42018961906433105, Accuracy = 0.9217527508735657\n",
            "Iter #6193152:  Learning rate = 0.000414:   Batch Loss = 0.378700, Accuracy = 0.939453125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4599156677722931, Accuracy = 0.9174056649208069\n",
            "Iter #6197248:  Learning rate = 0.000414:   Batch Loss = 0.317960, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4550725817680359, Accuracy = 0.9217527508735657\n",
            "Iter #6201344:  Learning rate = 0.000398:   Batch Loss = 0.373332, Accuracy = 0.935546875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4194892644882202, Accuracy = 0.9255781769752502\n",
            "Iter #6205440:  Learning rate = 0.000398:   Batch Loss = 0.336489, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4186388850212097, Accuracy = 0.9273169636726379\n",
            "Iter #6209536:  Learning rate = 0.000398:   Batch Loss = 0.307068, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4337468445301056, Accuracy = 0.9255781769752502\n",
            "Iter #6213632:  Learning rate = 0.000398:   Batch Loss = 0.328298, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4119226932525635, Accuracy = 0.9292296767234802\n",
            "Iter #6217728:  Learning rate = 0.000398:   Batch Loss = 0.299648, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42271068692207336, Accuracy = 0.9267953634262085\n",
            "Iter #6221824:  Learning rate = 0.000398:   Batch Loss = 0.286392, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42161649465560913, Accuracy = 0.9274908900260925\n",
            "Iter #6225920:  Learning rate = 0.000398:   Batch Loss = 0.312859, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41941261291503906, Accuracy = 0.9278386235237122\n",
            "Iter #6230016:  Learning rate = 0.000398:   Batch Loss = 0.312853, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41913583874702454, Accuracy = 0.925056517124176\n",
            "Iter #6234112:  Learning rate = 0.000398:   Batch Loss = 0.312984, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4244447648525238, Accuracy = 0.9254042506217957\n",
            "Iter #6238208:  Learning rate = 0.000398:   Batch Loss = 0.332285, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4108579754829407, Accuracy = 0.9354894757270813\n",
            "Iter #6242304:  Learning rate = 0.000398:   Batch Loss = 0.323600, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42163312435150146, Accuracy = 0.9292296767234802\n",
            "Iter #6246400:  Learning rate = 0.000398:   Batch Loss = 0.327310, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43353384733200073, Accuracy = 0.9262737035751343\n",
            "Iter #6250496:  Learning rate = 0.000398:   Batch Loss = 0.314361, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4168360233306885, Accuracy = 0.9269692301750183\n",
            "Iter #6254592:  Learning rate = 0.000398:   Batch Loss = 0.318732, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43757808208465576, Accuracy = 0.9300991296768188\n",
            "Iter #6258688:  Learning rate = 0.000398:   Batch Loss = 0.328674, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4280146360397339, Accuracy = 0.9283602833747864\n",
            "Iter #6262784:  Learning rate = 0.000398:   Batch Loss = 0.301014, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4083024263381958, Accuracy = 0.9349678158760071\n",
            "Iter #6266880:  Learning rate = 0.000398:   Batch Loss = 0.302548, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41585052013397217, Accuracy = 0.9325334429740906\n",
            "Iter #6270976:  Learning rate = 0.000398:   Batch Loss = 0.301123, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41661226749420166, Accuracy = 0.9311423897743225\n",
            "Iter #6275072:  Learning rate = 0.000398:   Batch Loss = 0.327830, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.414162278175354, Accuracy = 0.9337506294250488\n",
            "Iter #6279168:  Learning rate = 0.000398:   Batch Loss = 0.309948, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4181099534034729, Accuracy = 0.9290558099746704\n",
            "Iter #6283264:  Learning rate = 0.000398:   Batch Loss = 0.302165, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42428886890411377, Accuracy = 0.9300991296768188\n",
            "Iter #6287360:  Learning rate = 0.000398:   Batch Loss = 0.324690, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4524185359477997, Accuracy = 0.9227960109710693\n",
            "Iter #6291456:  Learning rate = 0.000398:   Batch Loss = 0.318354, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4280163049697876, Accuracy = 0.924360990524292\n",
            "Iter #6295552:  Learning rate = 0.000398:   Batch Loss = 0.285187, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42211511731147766, Accuracy = 0.9269692301750183\n",
            "Iter #6299648:  Learning rate = 0.000398:   Batch Loss = 0.339998, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42383894324302673, Accuracy = 0.9259259104728699\n",
            "Iter #6303744:  Learning rate = 0.000382:   Batch Loss = 0.321921, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4338185489177704, Accuracy = 0.9271430969238281\n",
            "Iter #6307840:  Learning rate = 0.000382:   Batch Loss = 0.334221, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41623634099960327, Accuracy = 0.9292296767234802\n",
            "Iter #6311936:  Learning rate = 0.000382:   Batch Loss = 0.337333, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4377053678035736, Accuracy = 0.925056517124176\n",
            "Iter #6316032:  Learning rate = 0.000382:   Batch Loss = 0.283697, Accuracy = 0.98828125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.427163302898407, Accuracy = 0.9231438040733337\n",
            "Iter #6320128:  Learning rate = 0.000382:   Batch Loss = 0.312604, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41909468173980713, Accuracy = 0.9299252033233643\n",
            "Iter #6324224:  Learning rate = 0.000382:   Batch Loss = 0.302232, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41916871070861816, Accuracy = 0.9257520437240601\n",
            "Iter #6328320:  Learning rate = 0.000382:   Batch Loss = 0.319789, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.40820157527923584, Accuracy = 0.9294036030769348\n",
            "Iter #6332416:  Learning rate = 0.000382:   Batch Loss = 0.363552, Accuracy = 0.94921875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4357506334781647, Accuracy = 0.920187771320343\n",
            "Iter #6336512:  Learning rate = 0.000382:   Batch Loss = 0.325348, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4320641756057739, Accuracy = 0.9273169636726379\n",
            "Iter #6340608:  Learning rate = 0.000382:   Batch Loss = 0.309784, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4196639060974121, Accuracy = 0.9285341501235962\n",
            "Iter #6344704:  Learning rate = 0.000382:   Batch Loss = 0.307854, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4169793725013733, Accuracy = 0.934272289276123\n",
            "Iter #6348800:  Learning rate = 0.000382:   Batch Loss = 0.317134, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41631120443344116, Accuracy = 0.9300991296768188\n",
            "Iter #6352896:  Learning rate = 0.000382:   Batch Loss = 0.317641, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4327625036239624, Accuracy = 0.9231438040733337\n",
            "Iter #6356992:  Learning rate = 0.000382:   Batch Loss = 0.305741, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43459030985832214, Accuracy = 0.9267953634262085\n",
            "Iter #6361088:  Learning rate = 0.000382:   Batch Loss = 0.311377, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4207336902618408, Accuracy = 0.9281864166259766\n",
            "Iter #6365184:  Learning rate = 0.000382:   Batch Loss = 0.321739, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41288328170776367, Accuracy = 0.9344461560249329\n",
            "Iter #6369280:  Learning rate = 0.000382:   Batch Loss = 0.316653, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4327523708343506, Accuracy = 0.925056517124176\n",
            "Iter #6373376:  Learning rate = 0.000382:   Batch Loss = 0.312820, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44736775755882263, Accuracy = 0.919492244720459\n",
            "Iter #6377472:  Learning rate = 0.000382:   Batch Loss = 0.307376, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41973423957824707, Accuracy = 0.9247087240219116\n",
            "Iter #6381568:  Learning rate = 0.000382:   Batch Loss = 0.329047, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4366503357887268, Accuracy = 0.920187771320343\n",
            "Iter #6385664:  Learning rate = 0.000382:   Batch Loss = 0.318920, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4289800524711609, Accuracy = 0.9222744107246399\n",
            "Iter #6389760:  Learning rate = 0.000382:   Batch Loss = 0.307636, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4104936122894287, Accuracy = 0.9285341501235962\n",
            "Iter #6393856:  Learning rate = 0.000382:   Batch Loss = 0.300641, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42510586977005005, Accuracy = 0.9245348572731018\n",
            "Iter #6397952:  Learning rate = 0.000382:   Batch Loss = 0.323150, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41976046562194824, Accuracy = 0.9247087240219116\n",
            "Iter #6402048:  Learning rate = 0.000367:   Batch Loss = 0.311217, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4162698984146118, Accuracy = 0.9273169636726379\n",
            "Iter #6406144:  Learning rate = 0.000367:   Batch Loss = 0.300804, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4172496795654297, Accuracy = 0.9311423897743225\n",
            "Iter #6410240:  Learning rate = 0.000367:   Batch Loss = 0.311629, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4153345227241516, Accuracy = 0.9292296767234802\n",
            "Iter #6414336:  Learning rate = 0.000367:   Batch Loss = 0.322099, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.40863537788391113, Accuracy = 0.9318379163742065\n",
            "Iter #6418432:  Learning rate = 0.000367:   Batch Loss = 0.282409, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41204625368118286, Accuracy = 0.9273169636726379\n",
            "Iter #6422528:  Learning rate = 0.000367:   Batch Loss = 0.296674, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4208841919898987, Accuracy = 0.9238393306732178\n",
            "Iter #6426624:  Learning rate = 0.000367:   Batch Loss = 0.297729, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42785921692848206, Accuracy = 0.9281864166259766\n",
            "Iter #6430720:  Learning rate = 0.000367:   Batch Loss = 0.311315, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42094147205352783, Accuracy = 0.9267953634262085\n",
            "Iter #6434816:  Learning rate = 0.000367:   Batch Loss = 0.304744, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43419256806373596, Accuracy = 0.924360990524292\n",
            "Iter #6438912:  Learning rate = 0.000367:   Batch Loss = 0.317322, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4286937415599823, Accuracy = 0.9259259104728699\n",
            "Iter #6443008:  Learning rate = 0.000367:   Batch Loss = 0.296108, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44034719467163086, Accuracy = 0.9247087240219116\n",
            "Iter #6447104:  Learning rate = 0.000367:   Batch Loss = 0.302677, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4222066402435303, Accuracy = 0.9259259104728699\n",
            "Iter #6451200:  Learning rate = 0.000367:   Batch Loss = 0.318884, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4237228333950043, Accuracy = 0.9252303838729858\n",
            "Iter #6455296:  Learning rate = 0.000367:   Batch Loss = 0.301159, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4181111454963684, Accuracy = 0.932185709476471\n",
            "Iter #6459392:  Learning rate = 0.000367:   Batch Loss = 0.302239, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4027444124221802, Accuracy = 0.9339245557785034\n",
            "Iter #6463488:  Learning rate = 0.000367:   Batch Loss = 0.276595, Accuracy = 0.984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4152275621891022, Accuracy = 0.9344461560249329\n",
            "Iter #6467584:  Learning rate = 0.000367:   Batch Loss = 0.334910, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4316257834434509, Accuracy = 0.9245348572731018\n",
            "Iter #6471680:  Learning rate = 0.000367:   Batch Loss = 0.308883, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4120512008666992, Accuracy = 0.9334028959274292\n",
            "Iter #6475776:  Learning rate = 0.000367:   Batch Loss = 0.306150, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4349755644798279, Accuracy = 0.9219266176223755\n",
            "Iter #6479872:  Learning rate = 0.000367:   Batch Loss = 0.283017, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4320048987865448, Accuracy = 0.9297513365745544\n",
            "Iter #6483968:  Learning rate = 0.000367:   Batch Loss = 0.301601, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42905259132385254, Accuracy = 0.9295774698257446\n",
            "Iter #6488064:  Learning rate = 0.000367:   Batch Loss = 0.279730, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4122984707355499, Accuracy = 0.9269692301750183\n",
            "Iter #6492160:  Learning rate = 0.000367:   Batch Loss = 0.297027, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4251973032951355, Accuracy = 0.9288819432258606\n",
            "Iter #6496256:  Learning rate = 0.000367:   Batch Loss = 0.292391, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41911691427230835, Accuracy = 0.9358372688293457\n",
            "Iter #6500352:  Learning rate = 0.000352:   Batch Loss = 0.296041, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42208540439605713, Accuracy = 0.928012490272522\n",
            "Iter #6504448:  Learning rate = 0.000352:   Batch Loss = 0.326691, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41474449634552, Accuracy = 0.9257520437240601\n",
            "Iter #6508544:  Learning rate = 0.000352:   Batch Loss = 0.300177, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4119124710559845, Accuracy = 0.9320118427276611\n",
            "Iter #6512640:  Learning rate = 0.000352:   Batch Loss = 0.298331, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4288383722305298, Accuracy = 0.925056517124176\n",
            "Iter #6516736:  Learning rate = 0.000352:   Batch Loss = 0.304396, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4174920320510864, Accuracy = 0.9273169636726379\n",
            "Iter #6520832:  Learning rate = 0.000352:   Batch Loss = 0.287205, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4289481043815613, Accuracy = 0.9273169636726379\n",
            "Iter #6524928:  Learning rate = 0.000352:   Batch Loss = 0.302816, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4103859066963196, Accuracy = 0.9274908900260925\n",
            "Iter #6529024:  Learning rate = 0.000352:   Batch Loss = 0.304441, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4114760458469391, Accuracy = 0.9337506294250488\n",
            "Iter #6533120:  Learning rate = 0.000352:   Batch Loss = 0.304861, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42198726534843445, Accuracy = 0.932881236076355\n",
            "Iter #6537216:  Learning rate = 0.000352:   Batch Loss = 0.336009, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44356805086135864, Accuracy = 0.9191445112228394\n",
            "Iter #6541312:  Learning rate = 0.000352:   Batch Loss = 0.325866, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.40982896089553833, Accuracy = 0.9347939491271973\n",
            "Iter #6545408:  Learning rate = 0.000352:   Batch Loss = 0.334351, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.40784934163093567, Accuracy = 0.9360111355781555\n",
            "Iter #6549504:  Learning rate = 0.000352:   Batch Loss = 0.308848, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.410230815410614, Accuracy = 0.9351417422294617\n",
            "Iter #6553600:  Learning rate = 0.000352:   Batch Loss = 0.313254, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4293045401573181, Accuracy = 0.9224482774734497\n",
            "Iter #6557696:  Learning rate = 0.000352:   Batch Loss = 0.358944, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42694175243377686, Accuracy = 0.925056517124176\n",
            "Iter #6561792:  Learning rate = 0.000352:   Batch Loss = 0.294380, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42298007011413574, Accuracy = 0.9260998368263245\n",
            "Iter #6565888:  Learning rate = 0.000352:   Batch Loss = 0.327552, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4072516858577728, Accuracy = 0.928012490272522\n",
            "Iter #6569984:  Learning rate = 0.000352:   Batch Loss = 0.319381, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41874802112579346, Accuracy = 0.9273169636726379\n",
            "Iter #6574080:  Learning rate = 0.000352:   Batch Loss = 0.302930, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4138965606689453, Accuracy = 0.928012490272522\n",
            "Iter #6578176:  Learning rate = 0.000352:   Batch Loss = 0.303581, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43576154112815857, Accuracy = 0.9257520437240601\n",
            "Iter #6582272:  Learning rate = 0.000352:   Batch Loss = 0.304359, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4138297438621521, Accuracy = 0.9354894757270813\n",
            "Iter #6586368:  Learning rate = 0.000352:   Batch Loss = 0.351513, Accuracy = 0.951171875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4038986563682556, Accuracy = 0.9325334429740906\n",
            "Iter #6590464:  Learning rate = 0.000352:   Batch Loss = 0.291365, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41271594166755676, Accuracy = 0.9304468631744385\n",
            "Iter #6594560:  Learning rate = 0.000352:   Batch Loss = 0.306767, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4257654547691345, Accuracy = 0.9309685230255127\n",
            "Iter #6598656:  Learning rate = 0.000352:   Batch Loss = 0.345077, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4406546354293823, Accuracy = 0.9212310910224915\n",
            "Iter #6602752:  Learning rate = 0.000338:   Batch Loss = 0.330406, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42465633153915405, Accuracy = 0.9281864166259766\n",
            "Iter #6606848:  Learning rate = 0.000338:   Batch Loss = 0.327912, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41567879915237427, Accuracy = 0.9294036030769348\n",
            "Iter #6610944:  Learning rate = 0.000338:   Batch Loss = 0.324139, Accuracy = 0.9609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4258025288581848, Accuracy = 0.924360990524292\n",
            "Iter #6615040:  Learning rate = 0.000338:   Batch Loss = 0.309956, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42513686418533325, Accuracy = 0.9262737035751343\n",
            "Iter #6619136:  Learning rate = 0.000338:   Batch Loss = 0.320377, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42740947008132935, Accuracy = 0.9271430969238281\n",
            "Iter #6623232:  Learning rate = 0.000338:   Batch Loss = 0.308952, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4203340709209442, Accuracy = 0.9274908900260925\n",
            "Iter #6627328:  Learning rate = 0.000338:   Batch Loss = 0.292246, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4235420227050781, Accuracy = 0.9288819432258606\n",
            "Iter #6631424:  Learning rate = 0.000338:   Batch Loss = 0.305132, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43293365836143494, Accuracy = 0.9288819432258606\n",
            "Iter #6635520:  Learning rate = 0.000338:   Batch Loss = 0.312944, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4229544997215271, Accuracy = 0.9248826503753662\n",
            "Iter #6639616:  Learning rate = 0.000338:   Batch Loss = 0.304846, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42348748445510864, Accuracy = 0.9269692301750183\n",
            "Iter #6643712:  Learning rate = 0.000338:   Batch Loss = 0.288399, Accuracy = 0.978515625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4159719944000244, Accuracy = 0.9297513365745544\n",
            "Iter #6647808:  Learning rate = 0.000338:   Batch Loss = 0.308856, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4248824417591095, Accuracy = 0.9297513365745544\n",
            "Iter #6651904:  Learning rate = 0.000338:   Batch Loss = 0.325799, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4283389449119568, Accuracy = 0.9292296767234802\n",
            "Iter #6656000:  Learning rate = 0.000338:   Batch Loss = 0.320699, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4222887456417084, Accuracy = 0.9262737035751343\n",
            "Iter #6660096:  Learning rate = 0.000338:   Batch Loss = 0.312016, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.43021517992019653, Accuracy = 0.9294036030769348\n",
            "Iter #6664192:  Learning rate = 0.000338:   Batch Loss = 0.280449, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4173482060432434, Accuracy = 0.9264475703239441\n",
            "Iter #6668288:  Learning rate = 0.000338:   Batch Loss = 0.308412, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4071931838989258, Accuracy = 0.9386193752288818\n",
            "Iter #6672384:  Learning rate = 0.000338:   Batch Loss = 0.307083, Accuracy = 0.98046875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4134315848350525, Accuracy = 0.9354894757270813\n",
            "Iter #6676480:  Learning rate = 0.000338:   Batch Loss = 0.320357, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4193151891231537, Accuracy = 0.9273169636726379\n",
            "Iter #6680576:  Learning rate = 0.000338:   Batch Loss = 0.311380, Accuracy = 0.966796875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41625797748565674, Accuracy = 0.9295774698257446\n",
            "Iter #6684672:  Learning rate = 0.000338:   Batch Loss = 0.332297, Accuracy = 0.953125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42731794714927673, Accuracy = 0.9267953634262085\n",
            "Iter #6688768:  Learning rate = 0.000338:   Batch Loss = 0.293202, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4372490644454956, Accuracy = 0.9227960109710693\n",
            "Iter #6692864:  Learning rate = 0.000338:   Batch Loss = 0.296656, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42883262038230896, Accuracy = 0.9320118427276611\n",
            "Iter #6696960:  Learning rate = 0.000338:   Batch Loss = 0.313291, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4199753999710083, Accuracy = 0.928012490272522\n",
            "Iter #6701056:  Learning rate = 0.000324:   Batch Loss = 0.283765, Accuracy = 0.9765625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41522645950317383, Accuracy = 0.9271430969238281\n",
            "Iter #6705152:  Learning rate = 0.000324:   Batch Loss = 0.286541, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4105105996131897, Accuracy = 0.9278386235237122\n",
            "Iter #6709248:  Learning rate = 0.000324:   Batch Loss = 0.298668, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4124071002006531, Accuracy = 0.9288819432258606\n",
            "Iter #6713344:  Learning rate = 0.000324:   Batch Loss = 0.320412, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41006988286972046, Accuracy = 0.9334028959274292\n",
            "Iter #6717440:  Learning rate = 0.000324:   Batch Loss = 0.299518, Accuracy = 0.974609375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4127710163593292, Accuracy = 0.9318379163742065\n",
            "Iter #6721536:  Learning rate = 0.000324:   Batch Loss = 0.292700, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4141470193862915, Accuracy = 0.9287080764770508\n",
            "Iter #6725632:  Learning rate = 0.000324:   Batch Loss = 0.311184, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41840407252311707, Accuracy = 0.9278386235237122\n",
            "Iter #6729728:  Learning rate = 0.000324:   Batch Loss = 0.295482, Accuracy = 0.96875\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41845858097076416, Accuracy = 0.9287080764770508\n",
            "Iter #6733824:  Learning rate = 0.000324:   Batch Loss = 0.310918, Accuracy = 0.96484375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.40373802185058594, Accuracy = 0.9281864166259766\n",
            "Iter #6737920:  Learning rate = 0.000324:   Batch Loss = 0.311640, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41022491455078125, Accuracy = 0.9351417422294617\n",
            "Iter #6742016:  Learning rate = 0.000324:   Batch Loss = 0.295236, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.44182288646698, Accuracy = 0.9207094311714172\n",
            "Iter #6746112:  Learning rate = 0.000324:   Batch Loss = 0.346247, Accuracy = 0.947265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4282013773918152, Accuracy = 0.9260998368263245\n",
            "Iter #6750208:  Learning rate = 0.000324:   Batch Loss = 0.333646, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4189246892929077, Accuracy = 0.9241871237754822\n",
            "Iter #6754304:  Learning rate = 0.000324:   Batch Loss = 0.353606, Accuracy = 0.955078125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4351204037666321, Accuracy = 0.9210572242736816\n",
            "Iter #6758400:  Learning rate = 0.000324:   Batch Loss = 0.322155, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.42868584394454956, Accuracy = 0.9287080764770508\n",
            "Iter #6762496:  Learning rate = 0.000324:   Batch Loss = 0.316930, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41614440083503723, Accuracy = 0.9259259104728699\n",
            "Iter #6766592:  Learning rate = 0.000324:   Batch Loss = 0.319520, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41524460911750793, Accuracy = 0.9351417422294617\n",
            "Iter #6770688:  Learning rate = 0.000324:   Batch Loss = 0.314780, Accuracy = 0.958984375\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.41840219497680664, Accuracy = 0.9361850023269653\n",
            "Iter #6774784:  Learning rate = 0.000324:   Batch Loss = 0.289539, Accuracy = 0.97265625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.40505778789520264, Accuracy = 0.9307946562767029\n",
            "Iter #6778880:  Learning rate = 0.000324:   Batch Loss = 0.312325, Accuracy = 0.970703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4140104353427887, Accuracy = 0.9356633424758911\n",
            "Iter #6782976:  Learning rate = 0.000324:   Batch Loss = 0.300340, Accuracy = 0.95703125\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4120602309703827, Accuracy = 0.9311423897743225\n",
            "Iter #6787072:  Learning rate = 0.000324:   Batch Loss = 0.316943, Accuracy = 0.962890625\n",
            "PERFORMANCE ON TEST SET:             Batch Loss = 0.4108196198940277, Accuracy = 0.9332290291786194\n",
            "Optimization Finished!\n",
            "FINAL RESULT: Batch Loss = 0.4108196198940277, Accuracy = 0.9332290291786194\n",
            "TOTAL TIME:  804.3145775794983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6aaBMsM0PPqw"
      },
      "source": [
        "## Results:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WArQfJZ51laH",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "id": "PJhE4fTrPPqx",
        "outputId": "999ca580-f416-465b-99cb-e7cb001d0460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# (Inline plots: )\n",
        "%matplotlib inline\n",
        "\n",
        "font = {\n",
        "    'family' : 'Bitstream Vera Sans',\n",
        "    'weight' : 'bold',\n",
        "    'size'   : 18\n",
        "}\n",
        "matplotlib.rc('font', **font)\n",
        "\n",
        "width = 12\n",
        "height = 12\n",
        "plt.figure(figsize=(width, height))\n",
        "\n",
        "indep_train_axis = np.array(range(batch_size, (len(train_losses)+1)*batch_size, batch_size))\n",
        "#plt.plot(indep_train_axis, np.array(train_losses),     \"b--\", label=\"Train losses\")\n",
        "plt.plot(indep_train_axis, np.array(train_accuracies), \"g--\", label=\"Train accuracies\")\n",
        "\n",
        "indep_test_axis = np.append(\n",
        "    np.array(range(batch_size, len(test_losses)*display_iter, display_iter)[:-1]),\n",
        "    [training_iters]\n",
        ")\n",
        "#plt.plot(indep_test_axis, np.array(test_losses), \"b-\", linewidth=2.0, label=\"Test losses\")\n",
        "plt.plot(indep_test_axis, np.array(test_accuracies), \"b-\", linewidth=2.0, label=\"Test accuracies\")\n",
        "print(len(test_accuracies))\n",
        "print(len(train_accuracies))\n",
        "\n",
        "plt.title(\"Training session's Accuracy over Iterations\")\n",
        "plt.legend(loc='lower right', shadow=True)\n",
        "plt.ylabel('Training Accuracy')\n",
        "plt.xlabel('Training Iteration')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Results\n",
        "\n",
        "predictions = one_hot_predictions.argmax(1)\n",
        "\n",
        "print(\"Testing Accuracy: {}%\".format(100*accuracy))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n",
        "print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n",
        "print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"Created using test set of {} datapoints, normalised to % of each class in the test dataset\".format(len(y_test)))\n",
        "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
        "\n",
        "\n",
        "#print(confusion_matrix)\n",
        "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
        "\n",
        "\n",
        "# Plot Results:\n",
        "width = 12\n",
        "height = 12\n",
        "plt.figure(figsize=(width, height))\n",
        "plt.imshow(\n",
        "    normalised_confusion_matrix,\n",
        "    interpolation='nearest',\n",
        "    cmap=plt.cm.Blues\n",
        ")\n",
        "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(n_classes)\n",
        "plt.xticks(tick_marks, LABELS, rotation=90)\n",
        "plt.yticks(tick_marks, LABELS)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Bitstream Vera Sans'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1659\n",
            "13256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Bitstream Vera Sans'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['Bitstream Vera Sans'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAukAAALfCAYAAADc51y7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOydZ3gUVReA35tOLwJSRHqToiiogAgKIogI2LCLvWIFC34KiAI2LIiigqIoUqWIdAGpIk2U3nsLPUASUu73487szu7ObjYhIQHP+zz77Mxtc6efe+bcc5TWGkEQBEEQBEEQ8g4Rud0BQRAEQRAEQRB8ESFdEARBEARBEPIYIqQLgiAIgiAIQh5DhHRBEARBEARByGOIkC4IgiAIgiAIeQwR0gVBEARBEAQhjyFCunBOoZTSWfh1zoF+9LTa7plN7c2x2mueHe0JXpRSFa1juy2X+6GVUnPO0rYKKqVOWNtceja2KQhKqaHZ+VzMy8gzWzgbROV2BwQhk3zvklYVaALsB6a65G/K0R4JQt7jDqCAtXyFUqqO1npVbnZI+O9iKUq+A77XWnfO3d5kjCV4zwb+0Fo3z93eCP9lREgXzincHvDWC6AJsO4svgA+B0YAB7OpvQeA/MCObGpP8LIbqAWk5HZHziKdrf89QFlrvWtudUYQzkPkmS3kOCKkC0IW0FofJPsEdLTW8qDPIbTWKcC63O7H2UIpVRloCpwEHgKmAfcqpV7TWqfmaucE4TxBntnC2UBs0oXzGqfdoFKqpVJqulLqsJV2mVWmtlKqt1JqkVJqr1LqtFJqn1JqnFKqSZB2XW3SnelKqbJKqe+stpKUUmuUUs9m1M8Q/W+klJqqlDqqlDqllJqvlGoRYt8bKKV+s8onKKUWKqVuzaqNtlKqmVJqglJqm1IqWSl1SCm1Win1pVKqikv5gkqp7kqp5db2Tyml/lZKdVVKxbiUz6eU6qKUWqKUireO2R6l1FylVHeX8u2VUjOUUrus/hyw2u+vlCrpKBdyf5VS9ZRSPymldlvnfn8G536b1V5FpdRNSql51v4dt87P5Zk8rpcppYYrpTYppRKVUkeUUhss+95MtWXxIKCAX7TW04ENQGmgdQb9aKuUmmhdr6etYz9bKfXcmZQPdm078j3HM1i6UupO63o/ZqUVtcpcpZT6SCm1zDr/yUqpnUqpH5VSdc50f5V5Xmil1O0h2hlvlbk31Pb86lRSSn3tdy9NU0rd7FcuyuqfdrvHHOXWWGWu9kvP7D1o25R3Vkpdbu3bAaVUulKqQ7j759fmHIypC8CDyne+0FC/sjFKqWeVeVYdtZ4Ba5V5Phdyadv5vK1infe9Sqk0pdQLVplSSqkXrHO5zWrziDLPlQfcjgHG1AWgmV9/5zj3K9h1rZQqpJTqoZT61zrmCco8155TSkVnsB+ZeW8UVUr9Tym10tqnROv6n66Uetz1hAjnFlpr+cnvnP5hPuVrYI5L3hwrbxCQDqwAhgPzgHpWmcFW3irgN2A0sNKqlwrc5dJuTyu/Z5D0b4G9wFaMWcwcIM3K6x6in82DpH+AMddYarX3r5WeAlzr0l4rINkq84+1zwus9Y+s/22ZOMYPW3XSrHZ+to7Vaiv9Lr/y5THaa20dh9+ASZivDxrzEoxxlI+w0jRwxCo73ErbDyT5tf+2VfY0MMsqOxXYaKVf7ShbMdj+Arc6jtPfVjuLHPv6lEudbVZ+X6vMPGAUZu6DBk4A1V3qBVyj1nlKsfKWAiOBCZjrNA14LZP3gnL073orrbu1PiZEnW8c+7zIOg4zgX2APsPyc3C5tl2OZ8Ug6V9Y/wut7SwFilhlZlrH72/ruP0CrLfKn8L93gi7/0B7q9zMIH2/CPOMiAdiwzxHjYFjVrsbMPfSbKsdDfT1K9/fSu8VpL2GVv76M7kHrTpDrbzBmPtivdW/GUDbMPbNrt/TkfYaMN9K32SVsX+POsoVtc6xBg4B04HxGHM1jXk+F/fbXk8rbzjmubEDcw9NAh63ytxnldluneOfgbmO4z3Qr81HMc8SbV0Pzv6+5ig3B/dndim8z8V4YAzm2kxwHPe4IPsR9nsDM+dkreP8TrDqzLOOxbrMPDvklzd/ud4B+cnvTH+EJ6RroHOQ+s2ACi7pN2GEwMNAfr88+6HaM0i6BgYAkY682630BKBAkH42D5KejkMQxggaA6y8WX51ClgPbQ109ctr73g5bcvEMd6Kn/DryKsKVPLr259W+Q9xCC+YF7H9Anzb7xzYgqr/sYnEEjit9Tgg0TqOVV36cylQyrFe0W1/gTLAcSvvCb+8jtZxSsEazDnytll1EoFmjvRoYJyV922Yx3W2Vb6TS15Z4JJM3gvX4xVIlJV2EeZFn4yfkGPlv2zV2QHUdzn27c6wvOu17XI8KwZJPw20ClK3tfNcO9IftequtY9DVvpvrW/D3H/VXLZjDxbfC/P8xAE7rTrvOvuGEd5tQa6N3/Wsgc3++2Ll28+BN87kHrTyhuJ9fr3ltr0M9s+u7/9c7GylDw1Rd5RV5iegsN8xs9v9wa9OT0d/vwGiXdqtBTR0Sa+CuU8CnmtAc4K8UzK6rjFCucaYmRVypJfBDDQCrhey8N7AfDHTwK9AlF97sbgMUOV37v1yvQPyk9+Z/ghPSJ+axbZ/suq39Uu3H6o9g6Rvw0Wz5nhINwvSz+ZB0ke4tFXCykt2vpwcD++/g+zTSLuPmTgOJ4EjYZa9yT4fuAsVZaw+H8QrSN5h1fkkjPZLhto/l/IV3fYXI4RoYEaQekOt/MF+6dus9H4udRpYeVvD7JutcSuaTffCD1Z77/ilT7fSn/VLj8arWc3wpZ7Z8qGubZfjWTFI+qAsHgv7y1HtM+z/a1b5D/3SozBa3nSgcphtPWC1tQ6IcMnviYvmHu+XvaYu5yPe6sPFjvRM34N+1/wat/6FsX92/Z5+6Z0JIaQDtfF+WXB7bubHaLVTcAw0HcfrIFAwC/19zKr/gV96c/v4Zea6BipY5+K0//Xs124CDm06WXhvAN2stBeycn/I79z4iU268F9hfKhMpVQRpdS9Sqn3lVLfWLaZQwHbrrV6Jrc3W2ud7JK+3vovm8n2pvgnaDN59TAQgxHYba61/kcFaWt4JrcNRsNd1DoulyqlVIiybaz/Mdp6mzjRWu/FmKVcAFSzkm3zjoeVUk8qpUoFa1xrHY/Rgl5qna/Mnhsb+zi5ufUE8+kZjJbfjYBzQubPr+3D/Edl5hxEhlkvAMtm9zZr1X+f7PXOfukNMOdhk9Z6bhibyWz57CCje7eUUuoRZWzTBzvu3dJWEef1kZX+26YfnZVSsY709pjzPF1rvSXMtuxr7ketdbpLvn3NNfG7Fuzz529D3QZz78/WvhMZs3IPOpkYpH85hT1fYqLbc1NrfQpzr0RhzqE/M7XWJ4I1rpSKVkq1UUr1UkoNsmy+h2K01JD553swmmK+YszVWm/zz9Raz8F8lSwIXOFSPzPvDfvZ8YpS6h6lVJGsdlrIu4h3F+G/wvZgGUqpjpiXY9EQ9Qtncns7g6QnWP+xQfKz0l5xv/bKWf/B9jnosQjBUxh73wet3xGl1J+YT7o/aK2POMpWtv4HKKUGZNBuSWCD1nqTUup5zKf5L4EvlVKbMPaVY4HJfsLGfRj7y25AN6XUfow962RguPVSzwj7OG0Nkr/Fr5w/AedEa51gjV8CJuUF4TWgJtDW+p1QSv2FsZ39Xmu9J8x2AO7EaBwXaq03+uX9gjHt8feZfrH1v57wyGz57CDUvfs0Zo5FXIj6zns30/3XWh9USo3ECMh3AD9aWU9Z/1+G2xYZX3O7MFrYOIwAfcBK/wl4D7hDKdVFa51kpdtC+w9+7WT6HvRLy8oz4kyw+/uyUurlDMqWdEkLdY3UxNhrhxLEM/t8D0ZG5xfMc6US7s+VsN8bWuvZSqm+wCuY6yNdKbUW+AMYeRYH0UIOIkK68F8h0S1RKVUeo1mOw9iI/oz55HhKa62VUn2A1zHakcyQ3VqorLQXoEHLalta6zVKqbpAC4zWqylwI0Zj95ZSqpXWeplV3NYAziL4S8fmkGMbA5VSY4Gbre00xbgQfAj4XSnVWlsuBLXW85RS1aw+3GiV7Wj93lJKNdVa57SgccbnWGu9VynVCLgGcyyvxezL9cCbSqk7tNa/hdlcZ+u/glJqvtvmHOW6+qWF3eVMlg+HjL7oBrt3G2LiFaQCL2EmC+7SWida+cOBu/G9d7Pa/4EYgfhJzFePaphztMvabo6itd6vlJqOMWNpD4xUxsPNzRhTtLF+VbJ0DzpwPeY5iN3fvzDzCELhdl+H6u8YjIA+HjPQWQ8c11qnKaVaYRQNmX2+5xSZeqZorbsrpb4G2mGux2uAp4GnlVI/aK0fzIE+CmcREdKF/zptMQL6WK31/1zyq57l/mQHtvb14iD5FbPSqDb+xqdaPyyTlPcxmvXPgUZWUVsoGK61HpLJbezDmBcMtrZxFWbg1AJ4BPjKUfYUZqLmOKtsBYwXn9ZAP4yAFordGC12ZYz9sj+VHeVyDMusYK71QylVGDMwfA0zGS5D0xll3PNdY62WI7j2H3x9ptsmEuF+7s9seTCaYTCf+H1QSkVh7KOzwm0Y4eozrfXHLvlu925W+o/W+i+l1BKMGUpdzEBHAV9rrdMy0ZR9LVUOkn8R5itMEsaUzcn3GCH9Acy8kk4YzepIF1OPLN+DuYTd3+la6zezq1FLi14b4yHqdpdzld3P94zOrzMvW54rllnNAMxXEwXcgPnK+IBSarjWelp2bEfIHcQmXfivU9z6D9A2KaVKYB545xrzrP87g+RnJLyGhdb6AMa9H0A9R9ZU6z+ob+lMbGMxlsDutw23stuB3uGUtbA/Bwf4SrZ4yPr/I4y2sg2t9XHMcT0NlFEOn+8h6Gz9j9Naq2A/jB2y02f6MowmtZpS6prAZgPIbHnwDhpruORdR9aVRaHu3ZpAfZc6Wem/zUDr/wXM8U7Fe22Gi33N3auUcnv/2tfcAh0YeGoicBRopZS6kOCmLpCN92A2YQ/Ugp1ru78dgxyXrGJfI3uDDKbuClIvo/4GYx7WpGTl5/cfTKwJjKnLCcy1mK1ow3TM1wMI7zko5GFESBf+69iRKG+zXnwAKKUKYF7AoezU8yqjMbas9ZVSLzozlFLtMHa1YaOUyq+UetEatPhjB19xTlobh5kI2lop9bGlGfZvs6JS6j7H+vXWxK4ov3IxeAdKO6y0CtZEwYDgJkH6E4xvMC/Llkqpx/y2ewvG7j0V+CyMtrKEUuplpdRFLlk3YDSqxzGCWag2IvAKbD+GKouxXQVLqLe+jvSz85RSPi91pVSkdc2QlfIWs63/p50TgpVSVTEawKxi37sPKKU8WnrrOv0OFwEri/23GYHxIvIwRvibYE3AzAyjMRrUGkAv5wRs66uRbY/d36XvSZjJ4FHAmxiXjTvxHl8nmb4Hcxhba1zLLdMylZuI0Xr/5HwW2yilLvS/T8NgI8aEpI5SqqmjLaVMgLSmQerZ/a3q/0wKhaUoGIc5R4P8rssL8V7vXzjmFWQJpVRHpdQ1zmvISi+C96uaREU918lt9zLyk9+Z/gjPBWPzIHWjMYFQNCbAyASMfWc85hPpt4R2tRhWuiN/qJXfOZx+htH/bbi7r2uN0QZpjPu24ZiAIunAJ1b6hjCPb1GrfCpG+zMSI7CssNJTgFv86lyM173gUYw2+ifr+G6w0v90lH/BSjuCmTRplz1gpa/HclMIXGalJWEC0fyMEX7sIDYJwJWOtisSxOUkvsGMllvbtV33pRM6mFHFIMdLY5Ra4Rzbo3gDaY3BG0wp3Wrn6TDaaOk4diED6mA+72scPtMxZhv2dWkHqxqOCWATLJhRZsrH4g2+FY+xDZ6NCTY0PNjxDOM4F8Prc3w/5r6diBnYrMPrs97/XstU//3q9rPPL9Ais88qq40meIMZrbO2/TtBghn51W3s2L4G3g1RNlP3YKjnUyb2za7v/1yMxRu7YSnGdGcw8JDfc8bWRJ90nJdfMPdHOrDPr92ebtvzKzMQ7/PLDma0wVr/gODvjuV43VEOs/rbzZE/B/dndimrjsY8v0Zjrnk7JsNsggczct0Pt/OC9zm+H+Np6kdMwCr72pqPi994+Z1bP9GkC/9ptNGsXQt8jHmg3ghchXnZX845qonQWk/FaFOmYoTUWzDanU54P4UeDLO5E5jJSGMxdsVtMBrrfJhBTH2t9US/7e/AuEp7AfOCrYfR4DfAmBu8AzjDVk/CBIb5G6NlvA1j474d48Glodba1ihvxkwUnIZ5IbbDRO5Mxby46mqt/wpnx7TWvwBXYl7cZaw+VscIMtdqrTPjuSMrPIsRABTG7r4DxnvFKKCJ1vqLMNrobP2P1u7u2zxorTdhJufFAPdYaVpr3RkzYJmBOf63A5dghLxn/drIbPlka9++wwhabTDH+i3M14osoY1HoYaYazARM7+kLjAEuBojrLjVy1T//Zhh/W/ATMrMSr8XYExxBmPuodut9VlAe6316yHqLsRoh23cTF3sspm9B3MM6xpojREiK2HO+yM43Jta9/d1GJOfRZjzcgdmYJKEeUbfmoXNdwGewZzbRpivVBswWvRQk7JvxdyHxTEmgo9grrGQaGMGeBXQC/NOuRkzkN6AORc36jPUolsMxcwJ2owJeHUH5p31L/AEZhCZkg3bEXIRO5CIIAj/EZRSb2Be0AO11qEEEkEQHCilvsFEM31Ju09WFQRByDZESBeE8xClVGlMqOhdfuk3Yj4f5weuClfjLAj/dSy3i/9iTIUu1lq7auoFQRCyC3HBKAjnJw2AiUqpfzC2vekYM47aVn5fEdAFIWOUUv2A8hhTuFjgLRHQBUE4G4gmXRDOQyyf4a9jbD5LY2zJj2Amfg7SWk/Ixe4JwjmDUmobZhLmLoy9e29tfNsLgiDkKCKkC4IgCIIgCEIeQ8xd/ChRooSuWLFibndDEARBEARBOM9ZtmzZQa21a9A6EdL9qFixIkuXLs3tbgiCIAiCIAjnOUqp7cHyxE+6IAiCIAiCIOQxREgXBEEQBEEQhDyGCOmCIAiCIAiCkMcQIV0QBEEQBEEQ8hgipAuCIAiCIAhCHkOEdEEQBEEQBEHIY4iQLgiCIAiCIAh5DBHSBUEQBEEQBCGPIUK6IAiCIAiCIOQxREgXBEEQBEEQhDyGCOmCIAiCIAiCkMcQIV0QBEEQBEEQ8hgipAuCIAiCIAhCHkOEdEEQBEEQBEHIY4iQLgiCIAiCIAh5DBHSBUEQBEEQBCGPIUK6IAiCIAiCIOQxclVIV0q9oJQarZTaqpTSjl/nLLRVQSn1tVJqu1IqWSl1QCk1QSnVJAe6LgiCIAiCIAg5RlQub78nUORMG1FKXQ7MBIo5kksCtwA3K6Ue1lp/f6bbEQRBEARBEISzQW6bu/wLfAs8DRzISgNKqShgOF4BfTJGOP/IWo8AvlRKVT6zrgqCIAiCIAjC2SFXNela66b2slLq1Sw20waoYS0fB27XWicCvyqlLgVaAvmAp4BuZ9BdQRAEQRAEQTgr5LYmPTu43rG83BLQbRYEKScIgiAIgiAIeZbzQUh3mrHs88tzrlc5C30RBEEQBEEQhDPmfBDSCziWT/vlOdcLBmtAKfW4UmqpUmppfHx8tnZOEARBEARBEDLL+SCkn3Qsx/rlOddPBGtAa/211rqB1rpByZIls7VzgiAIgiAIgpBZzgchfYtjubRfXhnH8uaz0BdBEARBEARBOGPOByF9lmP5cqVUfsf6tUHKCYIgCIIgCEKeJbcjjrZSSnVQSnUAnML15Xa6UqqEVXaoIyJpT0fZKcBGa7kQMEYp1U4p9THQzEpPAgbl7N4IgiAIgiDkPU6ePsnbf7xNanpqbndFyAS5HXH0a6CCS3oX6wdwHTAnWANa61Sl1D2YiKNFMH7T2ziLAM9orcXcRRAEQRCE/xy9/ujFBws/oGyhsjx6+aO53R0hTM4Hcxe01kuB+sAQYBeQAhwCfgWaaa2/zcXuCYIgCIIg5Bpa69zugpAFclVI11pX1FqrDH5zrLKdHWk9XdraqrV+VGtdXmsdo7UuobW+RWs972zvlyAIgiAIQm6xNn4txd8rzq7juwC4toKZondZ6cvOWh9em/kad46+86xt73zkvNCkC4IgCIIgCIatR7dyJOmIR0iPjIikQHQBIlXkWdn+3oS9vLfgPUavGX1WtpcRp9NO8/lfn5OWnpbbXckUIqQLgiAIgiBYJKYkcirlVLa0pbXm0KlDQddzCoXyWS+ZvyRXlruSmMiYHN82wHsL3gur3JHEI9kuOPeY3YM+8/r4pL2/4H26TOnCtyvOLetnEdIFQRAEQRAsivQrQoE+BYLmn047zb4T+0hOTc6wrc8Wf0aJD0qw+bDxXfHDyh8o8UEJVuxdkW39dWN1/GoAth3dBsDhxMPM3jab48nHc3S7NqULmrA1VYpVCVrmxOkTFH+/OC9Pfzlbtz1n+xymbZ7mk3Y06SgAj096nP6L+mfr9nISEdIFQRAEQTgvOXDyAFd+c6XH7MPms8Wf8dK0lwC4c/SdjFs7zpOXkp7iU/aN39/gh5U/eNb/3PUnZT4qQ9y7cR7hOxiTNk4CYPMRU+73rb8D8M/+fzxlPvnzE5oPbU6zoc1ITk1Ga037Ee2ZsnEKAMNWDqPKZ1U4nHiYCesmcPuo2zPc7x3HdgCw/8R+AJbuWQrAqNWjaD60OafTTmfYBsDo1aNRvRRTN00Nq7yN/bWgYEzBoGXSdToAURFRnj6rXgrVS3n6p7Xmlp9vYfLGyQB8sOADVC/F+oPrAfh9y++0+akN6Tqd1PRUbhh2A3O3z2Xu9rk+29pwaINnedKGSZ7lAYsHoHopth/dnqn9O1uIkC4IgiAIQo7xx7Y/whaCpmycQv9F/T0C3JkyZPkQluxZwud/fe5J239iP89PfZ6P//wYgNFrRnPrqFuDttFnfh8eHP+gZ31vwl7Pcu+5vT3La+PXsmT3Ep+6N1S+AYByhcoBUL90fQAqFq3oKfPitBf5Y/sfzN0+l1UHVqHRTFw/kZuG3wTAL+t+YcuRLRxNOkqHkR0Yu3asp+60TdP4eNHHpKT5Diz82Z2wG4BPFn/CH9v/YG382qBlF+xYwKbDmwB4aMJDALT5qU3Q8hsObWDRzkUApKSl8PO/P/Pd398BcCTpSNB6tn18qQKlAJi5ZaYnb9DSQaSmp6LR/LrhV9oOb8s7c9/hlZmvANBoSCMuHXQpLYe1ZOqmqSQkJ7AnYY9PG05+3fCra/pzU58DCHn+c5Pc9pMuCIIgCOcVqempRKpIlFIZF85DpKSlEBURlel+Z7S/zb9vTqSKJPUt30A6aelpKKWIUF59oS2Y5o/Oz5MNngwol5qe6tG8hkNiaiLg1SQDNBvaLKBch5odvPkVmnn2y39bqempHlMSgOsrXe9ZvuSLSwDQPbTnmFxY4ELP/gBcVPgiAC7If4Gn3m21bvMI3kXjiga4S1x3cB0AyanJXFLyEtbEr/Hktf6pNQAnU07SvWl3z+DmynJXAlD3wrqmTy4uGNN1OilpKcRExpCclkxsZCzpOp1rvrsGgJQ3U3i4/sMM+GuATz2tNWk6jdT0VOKi4qjxeQ3Pfvf6oxfvzns34Jg5j2NyajLRkdGeLxZL9piBTdlCZT1lnp/6PPtO7OPt6972pL05+03P8pGkIxxJOsItNW5h4vqJxETGUCJ/CZ/tnjh9gsZDGvPvgX990mdvm83hxMMUz1fck7Z87/KA45MXEE26IAiCIGQTaelpRPeO5sOFH+Z2VzJF/Ml4Yt6J4dPFn2aqXmJKItG9o+kxp0fIcjdUuSEgLbp3NPW+rOda/sDJA57lqN5RNB7SmDXxa4juHc3YNWNd6/iTkpbi0TAfPHXQk77+kDGVqHeh2XZURBSXlLjEk//5TZ9zS41biO4dzZ+7/vSk7zi2g+je0T7a8wLRgbbrKWkpRPeO5rWZr1G6YGmalG9CdGQ0AHVK1eHVJq9SLK6Yp7xzkFIwpqBH0O59ndmOU8BuWLYhYI67k8SURJoPbU5072iie0dTKKYQAIVjCwMwaJlv0PXSBUsT9XYUce/GEfF2BPnezccnf35Cob6mXqOLGhHdOzpAQAf4aNFHRPeOJt+7+Xj818cpElvEk+cvoNvH7Od/fwbMRNG4d+Mo0KcA13xrBgNj1owhXacHTKjtO78v0b2jA7bvZOL6iZ5l/0mxt/x8S4CAbmPb6jcu3zhk+7mNCOmCIAiCkE3YAla4Nr95hT0JewAYsWpEWOW3H91Oix9aMGfbHLN+bDtRb0cxZs2YgLLF8xX3TCA8nnycV2a8wp+7/qTuhXWpUNQt6LjROK87uI6Bfw0EYPHuxZ6JmpER7m4Ef9/yO2U+KsOKvSvov6g/1/9wPf0W9AN8tbSdancC4OHLHqbH7B6kpqfSZ34fHp34KI9MeIQXpr7gmczYaEgjT70KnwT21TYLAehYsyN1S9X1mMa8v/B9DiUeYsHOBYxcNRIwx/m9Be/x9OSnPXbwTjeFj/76KFcPuZqezXry5uw3ufjjiz2DimenPMv3K78HYP6O+R5TFDAmOfN2eMPC2Ofz4KmDDFgcKGhP2zwNja92/cNFH3q+PCzatSigDsCxpGN0m9HNs/7N8m84lnwMgCbfNnGtA3i+FMSfigcgKTXJ54vEXWPu4r5x9wWtnxEPjH+AUatH+aTN3jY7aHnb3KdDjQ5By+QFxNxFEARBELIJf8EnS21ozbTN07ixyo1ZMpmJPxnP6vjVNCjbIOTEPSflChub6Xvr3htW+U2HNzFr6yxql6wNwG8bfiNNp7H1yFZPmRmbZ3Bdpes4nHiYRbsWkZiSyODlg/lg4Qd8sPADCkQXoHzh8p7yTo1xQnICncZ08plgOW6dEWptAdSfz5d8zr4T+9h5fGeAx5Apm6Z4lltWbsnI1SMZsmKIj6Z1yIohYe27E9ueGoz2f3fCbp827QmLg5YNon3N9rQc1hIwGuDFuxbTsVZHn/bsSY22+cXO4zs9ebO2zvIst/qxVch+2fb287bPo8/8PgH5Tht7m2DH1clniyr9Z+0AACAASURBVD8Lmrdw58Kgef8e+JePFn7EyZSTrvln6k99zJoxrgPEYHSf1Z2Rq0cGtVXPK4gmXRAEQRCyCdvn87d/h+eP+cTpEyQkJ/ikDVkxhDY/tfFoTcPB9ooBxs3fdd9flyk7W9vkItwJm7b2dO1Bo5E8lGhMFf7c/ScnT59k1tZZtPqxFR8s+AAwQufL01/2EZ5Pppzkt42/edadA5wXpr3gI6AXjCnoMTN5ZvIzAf1ZG7+W8evGAwSdpGpP+Gx6cVOiIqKCmkJkhsW7F/P+gvcBWLBzAYcTD3vy4qLi6PVHL8CYnVQbUM2nbq2StQBoUzX4pMyssvHwRgBXAf1MaFS+UcaFXNhwaANdZ3TN0CzqbLHj2I4AAT0cl5pnGxHSBUEQBOEM+Hvf36heioU7F3oEzYy0kuk6HdVLUahvIW788UafPNueOJgWXPVSPDLhkaBtL9u7DICdx3YGLeOk8qeVueB9M5HR1lbb3DH6DvK/m5/rvr+OSp9WAoyW+7ZRtwEEBKLRWlPlsyq0+KEFANUu8AqmTjd4/vujeiki3w4eDfPE6RMh98Ep4M/dMde1TNn+ZWnybRNqDqxJanqqa5nM8tWyrzzzD4rGFfXJe+zyxzzLbgOmOdvmoHopHy1/Xkb1Uq4TUM8XgplR5SZi7iIIgiAIZ8C0TSZwyoR1Ezx+s/0jPvrjFG4X7VpEQnIC6w+tp3KxyvSd3xeAfvP7kZqeyl117gKMfbHtSePbv79lSHtjnmH7kM4qTu257X3E5vctv5OYmuixPZ+/Yz5lCpbx5Pvb/U7ZNIWk1CTPuvM42D7Cs4M3Z71JqQKlKBpXlPsvvd9nDkAos4dQJhlZJf5UPLf8fIsnYI6N26TLc52MzGzOZTLjNehskfd6JAiCIAjnEPakxIpFK3psge+sfWfIOk6PHmBsiHvM6cEVZa5g5f6VgNGI3z32bmqXrE3V4lUZs2aMjzlFcmoysVGxtB3e1qetYHbskzdO5qtlX/F9h+9JTk0mKTWJ+FPxHg8gYLTdWms0miW7lwT4uW76XVMOdD3g37QHp4AOeIT77Oadee94lm+/5HYf05jcIK/bNgsZo7XOc25TRUgXBEEQhEwyZs0YklKTuK/efZ7ANNUvqE6+6HwAfL/ye4Z2GBq0vv+ndduM4OIiF3vMVWzqDarHbbVu83glsek0phPj7xofdBv+k1i3HNnCxPUTSU1PpfRHpV3rLNmzhIi3I7i5+s0+kRlDtRsK22Y9J8nfJ3+Ob0M4/zmZcjLsidZnC7FJFwRBEASg15xelPygZFhlh6wY4jFnqFSsEv1a9KNK8So+5hSql+K1ma+F1V7PP3oCgTbhNnO3zw0IbW+7sLu4yMWA14a9QhHjKvD+cfd77L2HrRzmMQOxXf+FIpiADhmb8jhxBu0RhLxMXrS3F026IAiCcN4wZs0YyhUqlyUvFLagfDjxMCv3reRkyklurn4zn/75Kc0qNuO3Db9R78J6FIgpwMKdCzmefByAXcd38drvr3Fp6UsD2nxvwXu83OhlShYoyeSNk4mJjKFl5Zb8tfuvTPUt/lQ8I1eP9ElrXcVEm/y+w/d0/707w28bTtw7cSSnBXqpeGvOW+SLMlr+xyc9nqlt+2N7dgkH/wAzgpBXyQ73qdmNCOmCIAhCtrA3YS+FYwtTICYwCuPZ4o7RdwAmRLmTf/f/y4p9K7i37r0ZenH436z/8eXSLwGI7xbPC9NeCFp246GNnmiWK/etdC1Tb1A9BrUdRIeRJnDKiidWcNXgq8LbIQeLdy/2WZ+/cz6rD6zm878+Z9GuRR7vK27cVPWmbPMi4u9KMBQ//vNjtmxTEHKacN2Pnk3E3EUQBEHIFsr2L+sToTEvMWnDJB4c/2BI13utqhjPFU5/yRPWTQCgRP4SrnWqf16ddj+3A+C1391NW/ad2MfuhN2e9fpf1c9c54Pw976/qfNlHU80x1CcTjvN1qNbMywnCP9V4qLicrsLAYiQLgiCIGQb2REgJhTpOp3qA6rz878/hyyXlp7G7uO7OXnaRDj8cJHxZR3M3/aJ0yeYvnk6ABM3TPSk25EQ7cmhWcUtAM/ZxBkyXhCEQERIFwRBEIQzIDU9lY2HNwaN6Lm/634Ov3KY/Sf3c9HHF/HjPz+y4dAGj+vClsNaMmLViIB6+07s8yzb5iuARwO+dM/S7NyNs05GLiEFQch7iJAuCIIgZAu31bqNj2/8OMfaP5Z0jJS0FACuLHslCckJAWVKFShFsXzFPJ4alFKeAEBgTETuHns3iSmJnvoJyQlBNeyrDqzK7t3IFXrP7Z3bXRAEIZOovOhyJjdp0KCBXrr03NaYCIIgnI+oXoGu//wniKpeilIFSjHj/hlcOuhSWlRqwaoDq9h/cr9PuSvKXMGyvcvQPbRruxkRExnjiXLpXBYE4dzj/Zbv061Jt1zZtlJqmda6gVueaNIFQRCEbKHJt034cOGH2dbe+HXjuXXkrZ71QjGFQpbvOr0rAAdOHmDAYuPD/HDiYRJTEwPK+gcMyixOoVwEdEHIHjK6x3MKp7lbXkKEdEEQBCFbWLhzId1mZJ82quPIjoxbN474k/FM2zSNhNOB5i3df+/O3oS9AHy06CNP+uAVgwFYsW+Fx5+5G6HyBEE4u5QtVDbH2p50d/AAXU80eCLHtnsmiJAuCIIg5Dk2HtroWW71Yytmb5vtWq7v/L50HNkxy9up+EnFLNcVBCF7KV+kPAD1Lqx3Ru00r9g8IK1t9bZBy1e/oPoZbS+nECFdEARByHP89O9PnuW/9/3NewveC1r2uorXZXk7eTHKoCBklscvP7Mosv6MuWNMtrYXilIFSnmW7617L7qHZuWTK7m5+s1ZbnPOtjlhl+3aqGuWt5PTiJAuCIIgAPDhwg/5Ze0vma43YtWILE2+tNl+dDsPjHvAx7a71x+9wq7fb0E/luxewrUVrs30to8mHc10HUHIabpc2SVT5V9p8kq2bn/7se1nVH/9s+vDLrv88eWse2Ydsx6YxYOXPuhJr1ikok+5W2vdyoonVpxRv9ywYyjkRURIFwRBEADoNqMbt426LdP1Fu5c6LN+8NRBFu9aHKR0IFUHVGXYP8P4fcvvmd62zZWDr2Tu9rlZri+4c/VFV+d2F86YonFFMyxTtXjVLLc/8a6JGRfKgEgV6bP+VIOnfCZRXljgwqB1O9TsEFawrWcaugfUuqrcVSx9bCkvXv2iJ+2JKzK20S6Zv2RAP66+6Gom3zOZ6hdUZ8HDC3ityWs8Uv+RkOegXOFy1ChRg+sqXYdS3sF+/TK+kXmLxBYhX1S+oO34D9KfvOLJgD7606R8E+Y/ND9kmdxEhHRBEAQBMILK3XXuds3TWhPMZa8z0M+rTV6l+dDmXD3kanYc25HhNpftWUZqeioAkRFGUElLT8ts14Uc4s9df+Z2F86ICXdN4Lv232VYrlhcsSxvI6MvOFWKVfH86x6ai4tcHFAmTfte86PXjPaZKL3thW3seWkPyf9LDqj7UauPiFAZi3P+/axf2gjBI28fyRVlr6D/jf25tZbxplQgpkCG7b1+zes+6+M6jWPRI4toU60NAI3LN6Zvy74MvmUwR5OOovD92larRC3uuOSOoO2XKVgG8H5V+O7v75i2eZonv9FFjTzLvZr34o/Of/jUf+HqFxh751gOdD3Arhd3AXBP3Xt8ylxf6XqaXNwkw33NLURIFwRBOA/QWrPlyBaOJB4BjKCreinemfsOYLTbqpfi539/9qm34dAGVC/FjM0z0Fr7aLKcJJxOIOLtCD5aaDyovDnrTVcTl/Y12nsidtb70jv5q/2I9q4TPJ3RPUetHgVAclqgICKcn1QoUiFH229ZuSWJKYEuOP2pdkG1sNq7v979AWlF4oqErLP5yGYAapWsBRDW4PWzxZ95lttUbUNcVBxlCpUhKiIqsP3Dm1FKBcQM8Cc51fe+6tW8F09c8QQFYwp60sbeOdbTTpcru/DY5Y8Fba9phaYZbtPmijJX0KZaG8+XmZ7NerLmmTWMumNU0DplCpWhU+1OdLmyC3M7m69ko1aPQvfQ3FD5Bp/5JD3m9GDU6lHUKVWHonFF0T00NUrUoGmFpjw75VlaDmsJwPB/h/tsI69HEhYhXRAE4RwhLT2NpXuWuvr0TdNpVPmsCgOXDPSsA54InbagcirlFFpruv/ena1HtjJv+zwAus7oyuYjmxn+73Dm75jPwL8G+rSflJoEwGd/GeGh34J+nvZXx6/2lHt2yrMezdex5GN8//f3HE48zMT1Exm/bjzpOt2nXafGbsiKIUDef3EKmcfNXOPLtl+62j4/ecWTnuWszDNwEqEiuKvOXSHLTLp7El/f/LVrnlPrXbdUXYZ2GOqT37h845Btr3xyJWufWcuUe6fw060/uZZpVqFZQFpcVJxn2enZyL4PnRSOLexZXvjwwqDbcX4VqVi0Iu1qtGPQzYO4IP8FruU/a/MZbzR9w1XzD1AgugB7EvYABGjJ/bEH/0t2LwFg+pbpIcsDXFb6MkbcPsJnAGW3M2PLjICvPF2mdOHFq1+kZ7OePumjVo9i3cF1Pv38seOPQN6fOC5CuiAIQi6gteav3X9lqs6RpCM0/KYhdb6oE7TMqZRTPusxkTEBZdYdXOdxXWhrAf/Z/48nv+l3Rvvk31/wCgm2icqo1aN8fI0v37vcZ5udJ3T20Qou2+MbRKhmiZrUuKAGYOxgf9vwG12mZG7SnHDmXFXuqhxt/7LSlwWklchfwrWs0/3eN+2+4cMbPuT9lu9z+yW383KjlzO13XSdjlKK2iVrBy3Ttnpbn8Fivxb9PB5HnH67Nx7e6GNWUjxfcS4vfXnI7de7sB41S9SkddXWHmH6tSav+ZTp2jjQu0ilYpU8y26CuZNi+bymOo3KN6JDzQ4++a82eRXwHVDcVTv0wMWmQtEK/PWo+3Nq9JrRlOtfDjBa9VAs3bOUyRsne5QHTzV4Kqzt21xZ7kraVmvLl22/BGDgTQPpc30f2tdo7ymjUDxc/2Gev/r5oO3YQvmttW6lddXWfHLjJ5nqx9lGhHRBEIRcYNg/w7hq8FWMWZN5V2du2h9biO47vy+AR2M9Y8sMAOJPxQMwb8c8j+33BfkvcNWSPXflcwD0/qN30D7YE+2qFK8SkLd833KfdaenFqe7Na01qw+s9nhY+WrZV9z8880+Awbh7PBLJ3evPq2qtMpSe7MfnM3HN37sWXeaVNiDuK1Htrpq2J2TVWMjY3m58ct0a9KN0XeM5sNWH2bK3WB0RDQA8x8Of3Kgc9Dp9J/9RtM3AHj+qucpHFuYdc+so2/LvgH1M7IP79uyr4+Zj5sQHqwNt3T7a5lN/uj86B6aS0pewm21bqNfy37oHpqG5Rr69CFc9p/c75q+5cgWAO645A5G3R7cbMXGee/XKlEr7O0DxEbFMumeSdQpZRQUTzd8mtebvu4zwfWBSx9wrWvXAe/5zBedjyn3TqFGiRqZ6sfZRoR0QRCEXGBt/FrAN2hPdlFrYC2P0G5/Ek5INpPQFu9eTI3PzYtp1tZZAZ5ZwCtEDV051JPmPzDYdHgTAI2GNMKfiwu7fx73R6Np/n3zoEKAcPYIFunxt3t+Y3A7E721SGyg7bWbjTSYYDJOjXjzis35X9P/seSxJcx7yJhY1ShRw+fcD+s4jITXE3y8elQoGmizPrDtwIA0N8Z1Gkd0pBHSi8YVJeH1BPa8tIdf7vQOSOyJkjaFYgrR+/re3FPHTDB0+8LQ5couTL5nMiULlPQZfNjYg9xQbH5us2d5xd4VnOp+ipQ3U+jbwgjPwcxHbCG9V/NeHjOeQ4mHXMs+fNnDPppme2BduVjlDPvnxGl64yQl3QwOLixwIRcWDO59BiDpjSR2vriTNlXNpNJ80cG9tGSGWiVr8Vnrz9jUZRP9Wvbzbi8JXn0V/vzTuHhMfMOY+616apVn+VxAhHRBEIRcwBZEzjSynhvrDq7z2G76CxG2babNi9NexJ8f/zX2mramDPC4Pjtw8gBthweP3AfGFj0YTnt68eKSuzx/lbtZQP7o/J7lqIgorq90Pe1rtHfVqk++ZzKvNH7Fx7PGxi5m4Ok8v40uakTv63vToGwDj2Dv9BZUKKYQ99W7z3O9NqvQLKg9uv/AYNYDszzLk+6exA2VbwCgXfV2PuUKxhSkTKEydKjZgcHtBnNv3Xt9hNhp901j6eNLiVAR3Fj1Rjpf1tnHNeKE9RMA8/XI3yOIHXjnm3bf0P/G/q79dmJ/zQJ4q9lb5IvOR1REFC81eomvb/6az2/6nDZV23Bdxet89s/+Qrb96Ha+afcN397yLU0vdjc1ebnxy9x/qXeia4SK4Jc7f/FMwhwyBP5wOETZswfcHDhVv6C6x4bbpnTB0lxdznzxCMeuOzYqlpjIGIbfNpxhHYdxSclLMqwTDhWLVqTLVV2oUryKz1eGH36A99+HRo0gOjLaM9BwLp8LiJAuCIKQC9gu38Lx4WwTanKW8wVVsWhFYiNjAW94bHty2P+a/i/D7TgF6aF/D2Xmlpk+Xl8mb5wcsv6wf4YFzUvX6bw+83W6/97dJ3iRkL04hc9g2KZW/h48/E0XKhWrxPi7xrNg54KANsoVLsd7N7zHtRWu5Z6699CpdiePxtY5odgZNMr+ijR983SP/bTT3SCYCYLBXH76c10lb8TZttXbeky8nIKwf9uPXP4IP976o4+JRKsqrTzmEK2rtua79t9xZbkrqVKsCtWKVwtwOeikVeVWdKjZgUcvfxSlFC9c9UJYfQcjwNrERMbw2BWPUbNETUbePpIZ98/w2b+YyBgalG1AuxrtKBhTkIfqP+TqkSktDU673F4da3WkXOFyrF4Njz4KzZub9MGDoVw5k+bGvfXu9Vk/cfqE2e6+emxZVBeA9HS3mr4UjSvKffXuy7gg8Pvv8PjjkGgpvvftgxtugOGWg5bZs2HePPe6x4LrCYiPh6NH4eRJaNgQlIING8Lq0llHhHRBEIRcoNoF1ehzfZ+gnhPcKBBTgLKFyvJ287cD8oIJJGPXjgXIUPsdjIcmPMQNw26g8qfhfSJvULZBSJvcEatG8NWyr+g7vy//m5XxgEFwxw6gc2OVG33Sbe24rfUNxe6E3QABgXDKFS7nWR49GjrcfZCyzzzAnmWBkz9PnD7hWf7p1p8YcfsIz7qzXXuiMXjNLi4vcznjOo3josIXBbS7YMcC5u0IIoFlQIOyDbJUz436Zeqz6blNbOiygY41bw1aLiU9hfHrxnu+PuWPzu+jhU9NNUJhuMzcMpPC/QqzZM8Sn/QIFcGSx5ag13bgmmtg7173+ldfDZUqQYqvuToTJ8Lcub71tIY3jLk9334L48fDl1/CkSOB7dq+ya+5+BozoBm0kinvPMEjj0BcHIwaFaiNT0oy7Y8eDXfdBWvWBN/vX3+Fe++Fw4ehZUv45hv44gs4cQKeegpmzjT5iYlw/fVw7bWmfX+KOdzeHz7s25dSpUz+oEGw1HIk1Ti0k55cw92YTBAEQchRthzZQvdZ3WlycRPKFymfYfkHxj1AkdgibH5uM9UGVKNSsUokpSbx8vSX2djF1+vEtqPbPPaiNrZv5nfmvZOl/gaze/WnRaUWQHA3ikv2LOFIknn7h7Pf5yq7XtzFFV9f4Wpvf1npy/h7398ZtnF/vfuDfpWwvWQ4hV8Iz33lsseXccXXV3jW/U0PnOYkd94JUAL4wSS8VhjiEmhRqQW/b/2dQ6cOsX69Ee4OXNGFWuXKe0LU2365+7fqz41VvYOJqy66ih0v7PAI5+ueWRfgG//gKwfD0qSvempVQNrcznM5mZIJiTgMPv8cevWC+fOhht9cw/374Ythu6Gk4tCpQ1QuVpl/DvzjE6CobVuYMQMOHoTixU1a/dL1fQZETuy5IvN3zHeN+nqrNV54/XUYOtQsa2202du2eYXP3buhYkWzvGcPtLc+sMyY4W0rIsJok206WuEM3ngDNm0y/U1Lg/S3NEoZk7cisUV8vgB8+63579TJDEjuccQMGjAA+vTxri9ZApu9Jvkejh6FW24xy8Md7sx37oSHHjKDB2eazd9/m0GJk1MOJ1cHD5o+XXUVXHONN/3QId/lnTuhfB57JIkmXRAEIRewNW67ju8Kq/ywf4bx+ZLP+XrZ1+w6vouBSwby5KQn2XZ0G9+t+M7Hw0NcVJyPPbB/EJOcZPLGyTxaP8g3c2DRrkWe5Yx8K5+r/Hr3rxSKLcQ1F1/jmr/9qNc3+G21bgvajjMsvJMBbQZ4AkPdVO0mHr7sYU+ev+bVDf95EP5uOkPOFThdiBaVWvBLp1/44IYPaF21Nbfeaux/h77Vkldnvuop+sd2Y/DsFOZszABNMXIkHNpXgKKxxenZ0wh5x44Z39/+QYLS0uDhh41mtW4pY2JRu5TXtaLtyjNfdL6g7h2zSpcuRth7zeE9MSkJmjSB0qVh85cfwT/3eeyd+7fqz9D2Qzl0yGiBp083QvQ0b8BMlFIkHS3MDz+YMk5sUzB/zy1btxoTEJsNG7ya6U6dzACialVvvq0NT0/3Cu4Azzzjuz238dCRI/D882Y/a9SAO6zgoKUKlHI9pza9/ZxC+ZuSbNniTS9YEJ6z5tnOnOne3qefwhg/J1jOLwHfuQSUdR7PGjXgwgvN4OVHh2m9U8MOsH69+/ZzExHSBUEQcgHbq0s40QfBeJmocUENnp9qzBmuq3idx0vD45Me53Ci941TILqAj4bVX9uak/x74F9++OeHsMpO2jgph3uTO1xW+jIe//Vxj6mRP/aXBICONQOjsNq4aVABnr3S68O+crHKDGk/hFVPGftvf1vo8oUDVYP+GmrbU489OTjkxLq0GBqXb0zh2MJ0bdwVpZTXfGG9rx28bev+4cIPAejRw2hVbUaONOYPtWpBiRJGUz1qlNE6u9lTL1liBLJnnoHqhS6nWnHfKKHrD61n82Zj5vHtt0ab2rMn7AjvFgsLp/nI9Omw0OkcafwPFFfGJWm1C6pxZ40HKVECyjoc59xzj9Fa//STmZB5YMqTPPgg3H67yR8wwOS/d0M/OHAJs39owl13eTXDHToYExCbRYugdm2zv6NHB2qoe/c2Hk5at/Zq0SF8G+wff4R8+Uy7Y8eagRIYoT8xiJOUdeu8Gv2XXoJ//DyqRliSZ40axgRowABzXLdtC69PYOzTbb7+GtaaaQ4cOwYffwy/uHsUDdoGwJVXhr/9s4UI6YIgCLmA22SvjMqvP+RV9XSb0Y3Fuxd71sv290oChxIP8cJUr7B2tidourl1dGNvQhCD2jxCqC8CwahTqg4l85dk1YHVGRcmuAtDICx/8QdOHuD4cWjf+BJeS9IeO/DtL2xH99DcVO0mH//UEHjtbTu6zbuSEss/cyvC6fy4kpLf5/we8rOCio2MZeZMMyFv3uj6sOJBth7ZyuHD8PbbRmualGQ0nXffbeqcOuVr/7xggdHM+7PL8dHp6uhHPWY1NnVK1eHZZ42w98gjRljr1ctXsxyK9HT46iujUQ1maeMU0u9zmf/Y5cn8dOtmzGJ2G5N/EhICy/XvD8Pa/8yptcYzy7RpZpvPOb03frGaGYOvZeRIsy8QKPDaPPKIe/q4ccbDidO8JVxqubgy370bkpONgF0vhGOqhx4yg6WPP4a//GIhRUQYMx0nkyYFXkuhGOs3/r3kEmjTBooWNQODZcvc6zmZ4Ji2MX48FC4cvGxuIUK6IAhCLpCRve3DEx7mzVlvetb9Q2BnxLh14zzLtiYzr1GzRM3c7oKX1BjY1RDSvQLs4BWDfYqEispp+39edWAVs2fGsq7rIlgffLLun4/8yT9P/uMxT/my7Zcejz+2Bx5P9NikQjDiF1gb6LFl5paZjBhhNJ39+sGI1Wbi5tYjWwETIOrAyQOA0bpveHYDURFRLHt8mcfVYtTpEgwZAtflfw5mvsedt8XChCGQHOgDnNMFfALxPOsbmJYFHfdyww3GtOL554EJQ2HX1T6mBdWqQSF3Sx4PX3/tu/7ll15zC4B6xa7h0cvNIOrkSdjy3BYWPLyArVu9Zf5nzUtOSTEC9d69RvN9xx3wyivGjtlpFjFyJDz5JNSsCVWqmEHBkCG+grEtpCcnuwvf48bBhx9C06YwZUrw/Vu+3Jju5HO4C48IIZEtWhR4rHMaN/OTbdu8tuqbNgWv+0OIj2mpqeZadXLrraEnlPozenRg2tSp4dd3MmuW71eGvIRMHBUEQcgFbA8UV5S5wjX/u7+NoWXv64NH/QyF06Siz/w+IUrmHldfdLXPYCJXGfsTrL0dWj8HVw9wLdKqSiufrxdODp466Flu0wagING/jCPldV977+oXVOexyx/jqouMwP/mtW8SFRHFI/Uf4aLCF5Gansru47s95csXvpijc18lYV1HWNcRevppwec044kvvOuhBn8JSaeIPFYNLjCeVWbcP4Mm3zbhx2deYO9OgH6o2ATj9Xq1e9j4u2s+ykc33gkY7eeIEb75DeoUC6yUWNxHU74rjGkYVasaoevFF40px9NP++bbwnWXLmZS5xdfVOKppwJNGGx++gmio40G1ilc165thOpLL/W1Sd661X0SYaplORaOQPlcBjGNhg2Di8N07vTbb+GVC4fy5X0nXvoTG2vMcsqWNefBKYyfOAEffeRe79NPjUtEf/vxcJk4MWv1wqVhQ7Nv8/2Cz2byo+ZZRTTpgiAIuUCh2EI+/06cE/c2Hd5ESloKl5e5/Kz17WyRmJKHIv+ttYyC/+4ctIgdnbJttUANuduEzWJFogPSfrxuAddEdPWsF4krQr+W/YiOjGbZzzfz05sdfPxN37BqOwmzvBJq5QKWjcHuBjBkPku+8J0BaPugtj2nFI0rCskFQEP8hJeoUsXYGT/21jKaDDDqw707vaYtOtlxPboI6qNefZyCUca3fzCf2gHMiZZyEwAAIABJREFUeoeWLcMsa7FvH3TubOyb3dzj3XabEa4+/9ysT51qJnaG8o89Y0ag9nvHDuPBpkYNKBIYUDWAefPMdi/PptsxO+zlq1ULTLv33sA0gMjI4L7FwZh97Npl3BNC4OTK5cvd6z3xhBkwFXMZo+UV5sxx3/dLLz3rXQkbEdIFQRBygVolatHn+j6ecOynUk6xJ2EPqempPu4U953YR8w7MSzfu5z6pesHa+6cpOcfPXO7C4FEprgm31jlRi4rfRm6h/Z4LfGnSrEq3FvXKx2VKAENyzb0KXNlnRI0auT1cOGkZ0+jhSx42ESzbF21tce1nc2Wbivp3x8uGL8IdjYJaCM6IhqOVKDP/0qYCJLHLoK+J2DkL7CwGwD33w+De19hvh5kkrQ0YzMORisZFvvqc/x45razdq1xbRguEydCyZKhy+zeHTr/pZfC356N7TIwu2ngcPV+4YWhy14W6L6erl3dzWfi4qBCBWN/r7X316+f8Tvepo25bmOsD0ChPKBMnWr2f/lyI9RnJSjQNX4OkKIc9h0lSphBTNu28I6L51h/u/NnnoFWrWDgQO9+OY9BfpdpFjfemLcHFiKkC4Ig5AJbj26l+6zuHleMY9eMpVz/cmw/ut1nYt+o1d7ojyv2rTjr/fzPEek+ydbp89oZwAfg3evfZdeLu9BoUhK9nlHWroUlf3nP5bjWXh94i/2sZpx+nWN1MdpWa0upuLK48fLLcCje3Vp1+d7lMPw3vv2yCA0bwrHZlnvGdS5eZLa0YlDbr1zbCUVkpLEDz0xwHpsztf29+eYzq5/dFC7s6xIxO+jZ0wykbMqUMR5aguFvCgTGN3pqqhGq1671Ct32hFx/E49XXzURPmN8rbN46infdduF4XPPGQF3wgSo79AdPGAFcL3vvsABTJTfJfv558ZDzhtvmCBGJ074TswtWtSY5kyaZNxLOrn99sCvGZ99ZibgOo9H9+7u+2HjvO/yIiKkC4Ig5ALrDxqV1HXfX4fqpTxBbxJTE0nXXnuHAX+520cLZ06ZgmUCEyMCNemFYwuH9PSy7uA6yhUux5ZDWxn7hHeSrtbA4MVgmYk/d4d3/sHzzxtt5vbtxqxj9mxve7u2FCJh9hM0rZ75LyddG3eFeOM7fM8e4M8XHfsWOADZOT24C8hgKAUtWgT3xlG+vLFPdqNOndBtP/WUiQjpxn33GY8hmWXgwMzXCZdevTKOVvnhh+7eYkZ5x9+89JKZlJmUZFxVOgV/pXyPm7PewYPQvHngJM8iRUy96tXNRNikJGNnH0xYDcZHH5lr0/bEYxPsPDz4oDEp+e47M4m2tteNfYAv+MaNzcTZd94xg68CBUz65Mmm3875Ds6vNj17wmBrTrftuvLpp92/HPToYfrfv797fxs2dE/PK4iQLgiCkAtsPuLr0LjbDGOKsOv4rrPq1/xc4toK12aqfO2StYNnrujM3nk3BiRXL1UZThWDDW0g3bwi9728j051jCpvxw5gwDpY8oSnTrvq7fjnH4ie+jVpJ4sGbutYeYiv4TNZLz7emLZ06gTff++rIX71ueLMHdyOlNOZf0VXKFoh40I2sUd5940MbERc2LnT90vAiBG+bvaqVzeaVhOt1JcyZYzw9tZb7uYsjz9ujocbcXHBXSOGws2TTLdu5hxkNCHTaXbiT3w8VK4c3FPNiBHGrMnfjObll41ZSJUq3rQXXzRmKLYwGudwVR8b62uq4TQRueAC89+ihYm8aeOvKVfKaNczO0kyXz4zCIjzc50fTEiPjDT9i4oyQnPfvia9d+9A86hgLhzbtDFfAK5wzKmPdkzv6NHDO3/g22/NoOXDIA6soqIC+z9lijmv3bubtvIy4t1FEAQhFwgVbTOccOj/ReZun5up8rVL1WZ1fKC/8pL5ShM/wQpTWOdniPZGZN1wZBUMnQMH6tHpoXiW/BnHwmvy0aKFye/cGThUA34bBA2/4qF6j3JH7Tss4SeItv0T99mB/p/ws4PWrUNkaj+hP+4YsaowyUkRPPusdxKmk7vvhp9/9k3z947RqZOvjb0tjLldxmXKmIA8HToE5lWsaOyr7cA0YIRB261ibKxxq+jWz1C0aWPqpqTgmZT79ttGcLvpptB1O3UydtlucwhsQdH/mBcqZI5RMCH0qqvMZE9n8B5/IdhJXJwRrmfMMK4fy5QxJiD+Qu+llxpzlOrVQ+9TVvA3gynj8hHKjXbtjA/8otbYdcECMzm1Rw8j0IfLhReaLyn+8w4KFfJ1zRkOrVtncJ/kIUSTLgiCkAtoggviofKEjNn7sgmS1KCMuxr0t9vneFeOX+SbGXkaDhjpauR3JdmythAtWxqhKDHR1yyFGX0Z9/DX2RrR8kxxhp0PQPvq5YpEliY5KYK4OGPPe3PHQAPdCi6KeTfzEadm1fYA4vRSY+PvctDpIcae2OksU7Sod2Jfs2aBkyhHjDCmDs8/bwT4unV9td8vvWQmIB4+bGy0n3/e+Bt3CsVz5hg/7NdaH2o6dDBfCnr1MuXXrHGPhmlrd/Pn942kevSou4C+aBG8+67xTANQvLg3z+kv3aat5UTooYfMf8uW3rS2bXH1mHPvvTljwhHt56goVCAjf4o6Pi41bmzs4m3TlnBRyrisDGa2cr4imnRBEIQcYtWBVVQqWokCMYFvJH+vH2UKlmHvib0UiysmmvQzIH90fk+E1S+WfhGQ//cTf8P+Gp717vW+4pfkZ1lnJ5xwVxG2a2cEdQ9FtsOC1zhK1v1C5zbHDhtV7EUXGSHo3jvzM8nPbb2br3A3lILhw41/7VdfNWn+l/Ejj/iaMIAJcX/NNeYLxSefmDSnABcTY8w4/vzTa3/spHBh30FD795w+rQRiCMiTMAi8JqL2Ntw0qyZ+d13nwlc1KiR2R9nmPgKFYw994IF7lE/nZMngwUluvpqb3/A10zGTZM+ciSsXGn6k9s4Nenvvhta8y9kH6JJFwRByAGSUpOo+2Vd7hzjYpgLnmiPNntPGO2vm9/0855TxWH9zR4b8IwIZWt+KuWU8Q0O3F3nbkrmt76Pr7oDvllEyfRLfVzWpWxowdPpDvuKXcElonXrHCvHvCrmYBMdzxVsQfySSwLzyro7mAGMXbfTFv3uu43G+vnnzbpTSC9d2kz2c7OJfvBB4xfdbVKjUkazfued7nX9zTDstGbNTNRPfw1wKAoWNJreYHbbX3xhhOa5c2HVKt+8Jk2MrfP27eFvTynjRnDJEnfTjwIFQvfnbOI8jiVK5F4//muIJl0QBCEHsLW5weyo615YNyDtqQZPUfuLEJMdz1e+m2s8krR9ChoO8s1Li4LFz0G1yVDSSMluduYApMQSOesjVl5fmITXE8gfnZ++862Za2OMS4xevXyrfPBB+N1cEcQDZmIWYzJVruxu6wzGc8rChUaDn9PYQnq9esb0o0MHY7IBoQcg/foFao2dPqed5i4Z+TAP5gvczQykZk3vgCkzQnh20bSpe3pW7JyzKyhSTuMcDGV0LoXsQzTpgiAIOUBUhNGB1Cnl7nOu/6JA48ovl36Zo33Ks1guA9lkSTlpkUY4B/jrWZj+EQxc617XyV/PkrboGa69FgrGFCRCRTDtPl8j7aSkrHfzpyCxf5wu8cJl5cpA133OAUORIoF+pbXO/CS5cHCatNgaaJtQQnowsw4bpybdLRhNOLgJ6c7w8bkhpP8XcR5np1caIWcRIV0QBCEHiIuK49oK1/LkFU+65n+6OIgj6f80llT3xWr4dDOkK9id8Sy4hQ8vNAtHKgfktarSiu7XdPes+wu+2YG/j2qbYJEMf//daK39XffVrOldjox076tTO13olUuDThIcONC4mGvf3th9h8J/cmjhwt7lkiXxeLZx4h8J1Q1nXzMbmdO28b711sA8pztCN3MXIftxmuNUDrzNhBxCzF0EQRBygAgVwR+d3cPHC0FQGlJjjItDgNOFIM0rhZ3qfor7H0xh7JpfoONDnvS7x95N3xZ9ef1Xd59u18V2o4+1nBNCuht9+hgBsmtX3/T0dK+NsdMLyYEDgXbObn11aqcT8v/DTTcZm2YnlSr5Rl2cPDl0X/3dEDq1poULm2iQmzYZl39vvGGCydQOwyrrTOY/T59uJn/a3kyC9U806WeHhATvcsGCudeP/xqiSRcEQcgBEpITiHo7io8XfeyT/vivj/PAuAdyqVd5HQ2JDvVzsq+Q/kHffIz9uTCs7AynvXYQ249tp0HZBj5+wFev9mpy+/X0qoa//jqMbjT6KKs7ABg3iN26uQszzkmADz5ohPiJE43Gunlz6NLFeEkBXzd3rVqZf3+3hqmOuFf58pnJmfPm+ZZxap5r1NA+Alf1S5IoV863fLLXbTxKmXbr1jX+t0ePDk9AhzMT0kuUMPb4biY1IqSffYJFlxVyFhHSBUEQcoB0nU6aTuOTxb4+375Z/g3D/hmWS73KAdIj4EjF7GlLaUhyCOknS0FyEc+qMzpg81KW15z4GnC8LOk6HdK9mvQ6dcwn+k8+gcJFXBx2O3BOzmzWDFBpruXatjXuFptc455v06qV0YJn5As6MtLYodvbV8r4K7dDsBcvbryezJhhonSCwwd2jJG0U1K87TVubNwc+gvdTiF90ybl068G9QPtRZxC+png5ic9O3CauIiQfnYQIT13ECFdEAQhB7ADEu1N2MuxpGMAbD+aCf9s5wp/PQOfboUlT4Qut///7N13fBTV2sDx32w2nTRaIPQOAoIUAREVQRQQe0GxXkW9em3YyytwrVhQwItiQaygooiiqFRFQKRLk95rIATSs5ud94/J7s7sztZsCuH5fj58dubMmZkT3lfusyfPeU57mPMKFAYoMXlIVx/xvVWw+3zTbi3zb4B93eF//2D531bi81uBwztie/hhAm4NpV8IN28eoHqnzfTqpS0QvfpqqJFonlZT78o3Gfaqe097z5nkyy8PMBAT6enapjXOQPuRR+DNN4F7tL8n/Uy6r9xzfZBeUmKczY8ymaouy+JavfIq968PzCUnvWLoN6sSFUeCdCGEiICn5j6FMlohMy8TwLUhkc1hI3VMqr9bT20/j9c+f3yXm868yXe/d9bD4ifg92d99ylKgm+m+r6u88HTF8NBbWccR1EC553ZDP6+2bTvd9/6T0QfOVLbhGf+fG12OzW2plefV15xB7v9r9nlal+8WJvlHjsWDn77MJ89dqvr2nnngTW2GLpO4tuZxXz2WVA/ml+xsfDQQ0DNHXTL6GaYSfcVSOnj8FGjjNcK7N71I507Yjp34AxXec1y659bUWsMTnfPPQdPPqmlkYmKI0G6EEJEwLQN0wBYvHcxPT/oSW5xruG6rcRG03FNAXd5xlNNizST2mvpa12HS994FPb28O6Tr0thyW7q+wUnG4Y2oGOtQ+vv4ZprtPzx1FStvGLfvlp7bNsFgLa5z7Zt8Mkn2q6YTund3Unf+flaXfGHH/Z+fpMm8H8/jYVL72HQIDWiC+7UkSrLhy83BOm+tG0L11+vLSZ9+mnjteIS79yW227TvrD88EPZxjh2LLRq5bt0Zbj0vwmoChv9nA6Sk+Hll803vBLlR4J0IYSIAOfM+f2z72fZ/mWsPmTc+eb+2fe7ju0OO1XG2mHw0QJjIA1ajsgvr8HKO11Nl7Q02a0l3p2sun1xJ/j8J+/8kvd121LGZZd9zE5/mkTGIZg82b0gU8/aaj4DXnmGpUu1VJibbzZZwHjWByTUPmrY5t3MisNLQHGnP0Va06aB+1gsMG2aVpbRc3ZbUbzHZbFoX1j0pRjD0aoVbNmi/ZaivMhMuqjOJEgXQogglThKKHFoiwZtJcYpzN0ntHzzfSf3ARATFUPneu786kkrJ1XQKEM04zPYfQEseRTWXwvffAr2aMZ2/QmWPgo/vE+0RYvs/rf8f8Z79/aAXRca2wprwv82wg/vuIP14y3d1z2DdIduKtQeV/afJ8V/3v/5uhR3X+kYMVEx1G29x2eQ2qdJH7h8ON8tXRtwdvz8JtoLnX+Hkfbgg/D447BsWXj3J0YHWN1aRb36qvZzy+6XojqT76BCCBGkRm82Is+Wx2dXfsZl0y5j1V2rOKv+WaZ9B34+kPREH3udV0W59WF6aS5E4z8Ysf9FYCAANrvde0rHHgMf/mn+rKPttD/ZzeBmj9n3uBMez3EH5lFFdfBfNyWwmFaLKF7RxPTarFlaveffSsvX+wrSd2bvZGf2Tj690rwKT9PUpqgjg5sZf+ScR3jknEeC6huOuDgYMyb8+2Osp+bKy8ceq+wRCFH+ZCZdCCGCdDD3ICeLTrLm0BoAtmVtc12rV6MeAGP6j2HU+aMAOJx3uMLHaJDZBmZMgeO6oFUFPvsRxmTCl1+72226/dd/fBc++sN9XpiqpcMsGOV+Vm4QX0C2XwwnPPPMPYJbu/u9JQUBKr8E4fYeVzFyJPTurf3RS0gwbjMfZV6kRQghqgQJ0oUQIkj9mvWjd6PedEzvCEDLmu40jncGvwNoO42O+m1UZQzP27TvYO2t2g40ALY4LRVl2yAoqA2brnH31dUj93KyEbyaBb+NJGbqQq0tL8jfEry513he4jFzq/9yYKJdO1i1ytimr5fuqUZcAqNGwR9/eG8mlJSkVUcJZPwl4/npxgDbdAohRDmTIF0IIcKk6EpL1K9RH4DH5lSh38Mfa6t9HijdAWdfDy0NxYy/yipff+U6LD7SlMeU/fC+ey/6mLo7gx+Ts5b54fZwoAvnNTBZjKpTWKjtPql3++2wbp37XL8wUT877rmZTuPGwdXVvr/H/QxsNTBwx2pAqqMIUXVJkC6EEEGat3Mei/cuZsneJdr5jnmua/f9dF9lDctLu9qlgXisR/73IfP8eQCOdPR97Vgbw+lrIzMM55Z4Y7lJv0pioDBZq5v+3kpqW0zKOuqcOAFpHoVnUlONCwb1FVr01T5SPH45UKdOcDPppxMJ0oWouiRIF0KIIPVt2pc+jfvQv3l/AMb+OZbH5zwOwMqDKytlTM3TmtM0tal2klsHVgznsmZDtXPPID2YPPIw9GzbmFvuPh5c5yWPwSvuca3ZuddPZzjrLAzb2IOWtlK3rvt8/373sX4mPV3348bEaAGp56y8EEJUVRKkCyFEkJy1rk8UakHmgZwDvLbktcocEjv+as2u3aXlIKd/CbPeY8zzpQswLR61UorLvjDTTHJsCo/9Jy1wRxNXNbvdq+2222D7dnjgAfjoI+/ZXotFa7vgAu38Ql0VyM7uqpfcWrr5Z3q6O5Bv00Yr3/eVO4NHCCGqJCnBKIQQQVq4ayEAi/Ys8t+xvBSkwmezoev70GUy7OsOn88Giw0G3wu7SrfM/PNhuGQEOHT/xBclQk6G+XPLSFWDy/U207pGd6+2p56C5s1h3Djv/j10G5r+9BPs2aMF3ps2wdKlcOWV7uvdu2ub6TRsaKzqIuX73CTdRYiqS4J0IYQ4VWy8Bvb3hP09ie02jaLlpXnwjmj44X1j31W3a1VZnN7apVV0MdGjR5Cb4bScDdu8F1S2bOm75nggJ054t5kF/OvXw9atcMUV7rb4eC1AB2jbVvvjqVWr8MZ1upAgXYiqS9JdhBAiApqlNiv/l0TnuQ6L/rlAK6/oy/eTjedmAXrDJTzw5FFmzdJmrwOqb8y7HzYM7r8fRo0KfybdbFbb7Fnt2xsDdCGEqO4kSBdCiAAO5hwkpyjHb5+JgyeW3wBONITZbxnLJ37xY5keed2LH8Odvbn+3i3Urg0vvaQF207OfG+Xq2+Ac95wnY4aBe+9B+PHQ3Jy+EG6mUg+SwghTlWS7iKEEAFkjA2cy10jpkbAPmH7bgrs7Gd6qXW7QrZsinOdJ9YoIS/X/1aaGzfCz9lZfPUrpMS66xTqyxM+95y24PLLL0sbOk4zbBb6r39pO3g6BUp3SUiA/Hz/fZwkSK84ku4iRNUlM+lCCOHBoToY9u0wZmyaEVT/pJeTuPDjCwN3DNfhM02bW7eGe++OM7Tdd2/gve5btYKHej7E3of30r5ue1e7PkjPyAC73eNGBfr1g06dtOt6noF1z56q4dwsX9wXCdKFEEKCdCGE8GIrsfHFui8Y+s3QoPrnFudic9gi8/ISK8x9Efb2hMLSkonxx0y7JidrG/vo9dNNuDduDDGJxunr1FQVq1XbLbVhsnGXUf2mQWZBeo8GPZgzB1atMtYjB++Z9HnzFMNi1IZ+NjQF7QuHr2cJIcTpSIJ0IYTwwaFq+8pf0bYcViyqQI7J5kIrh8MfT8OHS+GVkzDvBYg33ygoKck7SO/Z030cHw/FGHPp4+KNM9x6A0sLt6Slac/2DNLvP/t+FEWrU+7J6pE8mZCglUB0Ovdcn68F4Mkn3ceeXwBE+ZF0FyGqLgnShRDCg3PTIlXVPl+88MXIv2Tx4/DGIVgx3Nh+1CMvZNEzUGC+UVBysvdunMnJ7uOcHIgzZsMQF+c7KktPh3/+gZWlRVw8g/TWtVp731TKLNjTt0VHQ9euPm8XlUSCdCGqLgnShRDCgzM4V1HJzMuk/cT2fvunxqX6vW5q7hjtc/Z4Y3uJSUL2MfOE7uRkKCkxvQTA4cNQz2Oq3TNo99SmDTQrrSbpcBivWS3+aw088ojva1FRxi8Qer/+6n9MQghxOpIgXQghdN5b+R7fb/4e0IL1L9Z9EfAei1KGf0pL4mC/borZLEj3ISkJzjvPvB20AF6/GBTg0UeCnzp9802oWxe47A4AtmZt9dvf8116ViukpJhfq1Ur6CEJIcRpQ4J0IYTQuXvW3Tw17ynG9B/DkjuWEGUJnCCdVZBVtpf+9Lb7uDj4Uo7JyaXb3XtsMpSuS3VPTXIH/c89p5VODFb79nDoENBF2xjpeIF5brxTfLzva1FR0KKF+bWkJEm7qCzy9y5E1SVBuhBCeNiZvZNLW19KzfiadMvoFvkXFHkkku/XVXLJrRf0Y1zpIw7jFwnn7HqNGpCU5I7Czjor9KBM379+Un2/fe+7Dzp2hLFjva9ZrXDbbeb3NWkS2phE5EiQLkTVJUG6EEKYaD+xPW3ebkOz1GaRf/i0mV5NZ+8srcm+xyR/Beg5aCv0eh3OfcnV5kxraZlmXND5xhvwn//AokUQm1Dkaq9dO7zhfnT5RwAB/y7S0uDvv+Hhh72vRUVBhw6wbh18/LHxmtRFF0IIbxKkCyGEH6sPrS7zMzKSPHb+Mdk99K/Ne+CfIT6fsbFoLlz8GKT/7WpzzqS/8qpWhuXVV7Xz1FSYMAE6d4bo+GJX/3rBT9IbRCnaTL3d4bm7UfCcJRo7dIBbboGiIrjnHphR+t0kUB11IYQ43UiQLoQQfgz8fGCZn3Eg50DgTmtuh28/83n5pLpPO4hyb5rknEm/ZlUK9V5qwWOPed9Xo4a7Lnq4QXqNmBqGz3B41j6PiYF33oErSkvQ9+unfclYuDDsV4gQOH/b8cADlTsOIYRvEqQLIYROcmwyfZv2jczDfO8bZK7YR41CgBhtU6LvbvzG1aQvaXioeIfpbSV2d9JxjTBj7CvbXYk6UqVVrVbhPQDjYlYzigKPPQbnnx/2K0QIxo7VfpvR1ry6pxCiCpAgXQghdC5tfSm1E8JM3tbbeT6MVmH+aC1Y/3UMfPl16IG7U3SB9hHtbvJVd1xvx7bowJ3K0axZMGoUXHBBpQ5DmJC1AEJUbf53phBCiNNMMHXRA1KBjxdqx78/Bzsugn29tPPjzQPe/sJn83j2Jo+8dYuW5qIP0p11x5/p84zP3UAHX17AssVxRLWfCVwe/M8QIYMHa3+EEEKERmbShRAi0k40Mp47A/QgPX2j98LSXk2606pmK0OQ7qzW8sKFL3BLp1tMn/XUw2kk3nkZdz2/KKQxCCGEqFwSpAshRCRtGQhv7fF93R4X8BFmtatvOPNavhv6neFaamrg4VitUK/jRmqmBH6vEEKIqkOCdCGEiKQvfvJ/3eZnW04/GqTV5Yw6Z3DypLvNEsS/4NmF2ZzX5Dwuan5RWO8VQghROSRIF0KcdnKKcoIri1ge5r8Q1m3ONBc1xIWn+bZ8PlrzEf8c/Ses9wohhKgcEqQLIU47Xd/rSoOxDSrn5dsvCes2Z5A+cCBcfTVMmhTcfQpafsxHaz4K671CCCEqhwTpQojTztasraHfVJDiPi5OgGX3QU6YuwMF4dNPoUcP97kzSI+OhunT4a67gnuOUprEviFzQ4RHKIQQojxJkC6EEKX2ndxnfuHvG2FMNix+VCuvuHAUzH4bPp4fkfcOG+bddtNNMG2a+zw6zHLnzpl056cQQohTg9RJF0Kcdj4Y8gF/H/7b0GZ32Hl9yevmN/z8pvY55zXYfzZsvFY7P9ouIuP57DNt058TJ4ztVt2/0OEG6RZFm4uJtcaGOTohhBCVQYJ0IcRp544ud3i13TLjFqaun2p+g6JbrekM0M3s7WHa/M03Wh65Pw6Hd1skgvQ6iXWwWqwM7zI8vAcIIYSoFJLuIoQ47Yz7cxzXT7/e0BaR6icf/mk4nT0bNm+Gq66CaV+W+L31X//SPq8emu9qi0SQDtA8rTm14muF/wAhhBAVToJ0IUS14lAdfLr2U0ocvoPih355iK82fGVoG9J6SNlefOAsr6b0dGjdWju+/roov7ePGQM//wyfTk5wtUUiSM8tzuXM9DNpVycyqTlCCCEqhgTpQohqZfLqydzy3S2MXzY+qP7FJcUAqIRYgNzJedt7q7wuBQqsH36k2LU4NDYWLr4Y4nV7HemDdGuYyYnFJcVM3zidrcfCqGgjhBCi0kiQLoSoVpxVTKKjAk89/7T1J2JfiGX5/uU8//vz4b3wg6WQW9f0kmeQ/qc+G+bfHfm/5/O43ph1YxCJmXTn38eUtVPCe4AQQohKIUG6EKJaaVGzBQA1Ympw4zc3UmQv8tl3d/ZuQEsJGdFzhJ+n+pll398TPpljeskzsO7Ro3SB6BNVkRNiAAAgAElEQVSpkL4eu8Pu550QpcuQKUtOOsDO4zvL9gAhhBAVSoJ0IUS1oqpaQL14z2Kmrp/KruxdPvtmFWQBUKKWlK1E4ZEzTZtjYrzbFAXWj1jM4+c8Tu2E2n4fG4kg3ZnG4yzFKIQQ4tQg/2oLIaqVI3lHANh+fLvPPoNbDQbg2QXPArDm0Brm7pjr7lCYDL++qu0qGmaqOvgOrNvXbc+Yi8a4dgP1xWIxPw5Hcmxy2R4ghBCiQkmddCFEtfLNpm8AXDPoR/KO0KZ2G0OfWTfOAkAZrQXJx/KPsfzAcneHGR/D5iu045Q9YY+lrCkqegHieZ8SorVqMfd0uydygxFCCFHuZCZdCFGlHcw5yIhfRvgtqaj39cavAdiZreVgnyhyb+O5+uBqlNEKV391tStAB4iyRNGrYS/3Q5wBOsChzmGPPRJBeosWkJoKaWnhP6NTeifSE9PLPhghhBAVRmbShRBV2vAfhvPj1h+5pOUlDGgxoEzP6vJeFwC+3fStob13o94ALN231PsmhxVKTJLLgxCJIP2ff0BVjfnpoYizxrHmnjVlH4gQQogKJUG6EKJKszlsgHtBaKh+3f4rA1oMICbKd6B9+bTLXe/x4rBCYiYU1gz53ZEI0sOtjy6EEOLUJukuQogqrUWaVlIxKTYprPsn/DWBAlsBAF3qdzHt4zNABy1IL0zVjqN8l3M0E+7stxBCCCFBuhCiSpswcAKFzxQac8Z9mL5xuml7ob0QgB3Hd4Q+AEc0FJTOosdnhX6/EEIIEQYJ0oUQVVqUJYpYa2zAcoUA1359rWn7gZwDgJafbeaGDjf4fmhhihaoW/PBWqgbWKHve4QQQogykiBdCFGlPTj7QZTRCsv2LfO6VmQvYuj0oSijFdYcWkPrWq1Nn+FcMOpr8yBnSo2pvLraZ3wWxqLpYdZEFEIIIYIgQboQokrbdHQTANmF2QDYSmwooxW6v9+d3Sd28+WGLwGtHvqWY1v8Pmv9kfUAzBw6E3WkO+B+YdELvm/Kr6N9xmWDogvSVf9BervHpC65EEKI8EmQLoQ4JXyx/gsAjhceB2DFgRUczT8a9P23z7zddXz5tMsNddINPIvIFKZonzF5xvYo88WmzzwD3HoBmxInBT02IYQQwpME6UKIU8Inaz+hyF5EbFSsq00fpO854X9n0ClrpgT3ohKPuonFpVVlrAUYIvgrbjO9/bHH4KpBtbj5zJuDe58QQghhQoJ0IcQpRV/vXNHlhafEls54F6TCL69BZtvQHrz0QZgyHwo9tvYsTgQgNSnOkO5Ss9t87n9um9djoqPhm+u+4ZMrPwnt/UIIIYSOBOlCiCqtQ90OrmOLov2TFW2J5tX+rxr6Jccmawc/vwlLH4V3Q9xl85e3YFdfeP2wsb1Iq5FeJ7mGofmVfq+wJXuD12NiwtucVAghhDCQIF0IUaWNvXgsv932G8/3fR6rxYrNYcPmsKGiUjexrqvfJZ9foh0cbad9lrjTYpzBfVkUH6+LPt1leNfhHLcd8eonGxgJIYSIBAnShRCVxqE6sJX42e0TsDvstEhrwdXtrkZFRVW1QPmJuU+QEpfifYPiMH1PWRUU2Y3VXYBuDTobOw29jCDKuQshhBABSZAuhKg0t8y4hZgX/OeH3PvjvTR8syFnTDyDInsRNoc7qG+U3Mgwm67RBdJ57rrovjYyCtaJopMQVWRoa123mbFT2x/K9A4hhBDCSYJ0IUSl+Xzd515tLy16idcWv+Y6f3/V+4br0zdOdx0fzD3IkTyPlBP9TPprmbD6VgB+vPHHMo012hJD7661DG1Wa5keKYQQQvgkQboQotKYzW4/M/8ZHp/7OAC/7frNcE1F5d8//tt1vuP4Du+Heqa7LPgvAP/97b9lGmuiJZWm/X/SXtF4MQDRHkH6uY3PLdM7hBBCCCcJ0oUQlea+7veRGJ1oem3fyX1c8PEFhjZnPrrTE3Of8L7RM0gvzSP/bfdv3n1DYLdH0bTzbrinE+owbZFqQpwxVadzemezW4UQQoiQSZAuhKhwv27/lbQxaVzf/nqynshytet3AX150cte96ke24GuOWRSZtFr4ajnFqLhqdlxmVaXvd7fEJsLQGK0sSxjo5RGEXmXEEIIUSWCdEVRLlMUZY6iKFmKohQqirJVUZQ3FEWpFfhu1zOiFUW5X1GUP0qfY1cUJVdRlPWKoryuKEp6ef4MQpwqXvz9RfPg1sSO4ztYeWBlxMfw/ebvyS7M5rrp19Hrw16mfSaumOjVpt9t1CeTmfTvrv8unGEa/P55LxSP0i3FRcYvAI/0eqTM7xFCCCGgCgTpiqKMBmYC/YE0IBZoCYwAViiKEuzU1DRgPNC79DlRQCLQHngEWB5K0C9EdfXsgmc5a9JZQfVtMb4F3d7vFvExONNWdmXvYtXBVUHfFx0VHbiTyUz6vJ3zQhhdqQTdgtQaB6hXM4n2ddoD0DC5IQCHTmS7urR46C6iLFIkXQghRGRUapCuKEof4LnSUwfwNHAl8GdpW1PggyCe0xK4Stf0LnARWnBeUtrWCLiuzIMWopqwO+yVPQQvCdEJAPyn+3+8rvVp3MdQ2cUnj1rmKA4m/DUh9MFE2byOu2Z0BWBA8wEA2IrdM+uP3BjcFx8hhBAiGJVdQOwh3fFkVVVfBlAUZSWwG1CAAYqitFdV1Xv/bbdUj/PHVFXNBeYqivIvtNl0ANmwW4hSnoswK5Jn2ojTiSdPYFEsWBQLby9/23Bt0Z5FLNqzKIiHmy8cDVWN+Bhyc5wDawJAWlwaANlF2gx6jC775rr2MgcghBAicio73aWv7vgP54GqqnuBPbprFwZ4znrgoO78NUVR+imKMgJoW9qWC5Q9MVWIasJXoFwepqyZYsiDrxlf03B9/s75XPXlVUQ/H03iS4mMXDCSNrXahPeyCC0ctUZ771K6/sh6APo36w9Av0uzoO0M6t74JLUSJJtOCCFE5FRakK4oShpa7rjTIY8u+vMW/p6lqmohMAhwJrfeA8wF3kDLTZ8L9FJVdXdZxixEVWQrsbHz+M5Ke//u7N0U2gv99rl95u2GPPg7u9zJxEHuhaH9PunHjH9mAFBoL+S/v/+Xe7rdE96AQplJ3zwYvv0YbN4LUqOjvb/E/L77dwAO5BzQ+sQ4YOhVpPT8NryxCiGEED5U5ky6Z3HkYj/nNQjsBLAFLbfd0znAtYqPqUNFUe5SFGWFoigrMjMzg3iVEFXHgz8/SPPxzcnMi/z/7z5+zuN0y/C9cNRWYqPpuKbc9O1NACzduxRltMKYP8b4fW7jlMYBZ/IHtBgQ+oDBJEg3+yeh1NRZ8PctsOwBr0tJiYEXqTo3Y2pXp11IQxRCCCECqcwgPc/j3HMqS3+e6+9BiqKkAkuBoWg/0x1ogX0HYDOQgLZA9UGz+1VVfU9V1W6qqnarU6dO0D+AEFWBM3XEs4a4GX0eutXif0nKsn3LeHflu8weNttnH4fqMLzbOcP814G/fN5z3dfX0WxcM9fOoZe2vpRWNVt59Ru/bLzf8fkUTrpLdlOvJotFhcbmOfDOnzclNgWAC5pcEMIAhRBCiMAqLUhXVfU4cFzXVM+jS33d8fYAj7sacNZBX6uq6mRVVfNKF5u+o+t3fViDFaIKq5OgfbEMFHRDcIG804oDKzhZdJL3Vr7Hx2s+dgXges7Z8C71ugCw7sg68/eOVFFHau/+euPX7Mre5bo2a8ssHuzxII/2etRwz6SVk4Ieq3FQYSwcLU7yarIQBSl7DG2es//OLykVmd8vhBDi9FDZ1V0W4C6d2AeYAqAoSjO0kolO8wM8Rz/97fm/tik+joWoFpyBcZG9KGBf/Ux6vi2f3OJc6ibWNe3rnKF/Zv4zAJyZfiZr71lr+ryDudq67dG/jfb57pyiHJ9fJFrWbEm+LT/g+IPjGZQHEaQXev/T4FBViDbm2jvrpPdqqG3AdKzgGKB9oRFCCCEiqbKru+h/n32boihPK4pyBfClrn2us/yioihTFEVRS/+M0vXRRw7NFUV5T1GUAYqi3AU8rLu2PNI/gBCVbeGuhQDk2TwzyPyr/0Z90l9PZ91h89nvndnGxaiHcw979XHOzH+wStvOICnGe0YaQBmtkPxKMgkvJZheH7dsHI/Pfdx1HtTOor6EM5Ne6FnFFfKL86HzRwB07qN9CelSvwtjB4ylc73OgLuu+3lNzgt/vEIIIYSJSg3SVVX9DXhRN5YXgRlA99K2PcCdQTzqZ0CfODsc+AWYhLuGeibw3zIOWYgqZ0jrIQAkRrvXYk9ePZkPV33o1Vef7nKy6CTgO7j3nB0e0WuEV5+cIq2Q+KBWg7QxxGhjqJtgPjvvy+xtxrz3oHYW9cXisUmTv4WjTiZBenGJDZoshocb8fgErUJsnDWOpNgk19+j8zcJthKb1/1CCCFEWVT2TDqqqj6LtsvofCAbrarLduBNoFswZRNV7X8pLwfuA34HstB2Gi0ANgJvAZ1VVQ2U2y5EpVh1cBVrD60N3NFE87TmgBbYzt46m4M5B7nj+zu484dgvt9CVkGWabs+zzolNoWYKG0vsPk757tyyrdlbQNgxj8zKC4p5lCuVjn1nUvfoSxyi/2uFfdP9fxnLYiZdId3Gk5MfBEvXfgSpOwjOV6bMV93ZB3DfxjO8v3aL+WO5B0B4P1V74c/XiGEEMJEpQfpAKqqfqeqaj9VVdNUVY1VVbWlqqojVFXN9Oh3m6qqSumfUR7XbKqqTlRV9XxVVWupqmpVVTVBVdX2qqo+rKqq96o3IaqIru91pfOkzoa2YHLMAVfAbCuxMeiLQZz70blefXKKclBGK0Q/7z1D/dd+80osN3W8yXV8dburaZGmbVfQ75N+NBvXjNu+u43tx93fe//7m/sXVW8secN1XKE7mzossK+Xsc0z3aWoBtjiPG70Xvh53VNzGNhqIAA2hzZTvmzfMsD9WwbnF6THznmsjAMXQgghjKpEkC7E6a5nw570a9bPdT559WTiXowLapOisX+OBWDq+qkANExu6NVnf85+n/cfyz9m2u6cSU+OTWbymslcNu0y/j78t+v6x2s/ds0oA7Sq2YomKU0AeHSOu1LL43PcueblQgU++xFmTYTFj8GJJsbrme3gROnfiT0aXs6BNzy+s3vOvg/8D/26NeFo/lEAthzbYvrqlLgU1JEqw84cFoEfRAghhHCTIF2IKsCiWAzpJV9v/BqAf47+E/De9ESt+uiSvUtQUDi/yfleffae2Ovzfl9lGZ2zxu8MdqeueJZhHP+Xe+13SlyKK91FLy0+zastoo43g22DYMW/Yekj3tdL4uDN0p8/J0P7LEwDe4y7j2eQbikhzhrn+nn0X04gtFKWQgghRDgkSBeiCliydwlzd8x1nXdK7wRArYRaAe91VhrpWr8rKirZhdleffyVN+xSv4vhfNm+ZWTmZbpKO45dOtZwfWiHoabPWbhrIUUl3ik6ikkqiRlnOk3odM/3qGvuxabb6HiMLhffK48d+jfv7/0mqYcuhBCiglR2nXQhTntm6SYd6nYA3LXK/dlzQgtMnQH94r2LGXfJOArthf5uc9FXhQHo+WFPw/nKgytdxz9u+ZFp66eZPmfcsnGm7U/PfzqocZgF+EFxRLmPrQX++xbrflZ9wK56BN8lWu6+s8RiWpz22wDnl6fejXqHN1YhhBAiSBKkC1HJVh1c5dXWtX5Xxg4YS634wDPpm45uAtwB/ZDWQ3igxwNBv//2mbdzfYfgNuMNJc2jzdttTGur+7Lv5L6g+xqU6Gqq2xJ99wMo8rGfmedMeomWCtO6VmsAzm+qpRCdVf8s3rv0Pc6qf1ZYQxVCCCGCJekuQlSytYe9Sy+uPrSaEb+OIDPfXeBIVVWenPsk27O0iiq7sndhGe3+T9iZ5uJQHby6+FWumHYFE5ZNYOuxrX5LBBbY3bPPY/4YA8DAlgNN+/5v+f+C/rm2HNvCiaITQfcPm11XqcWZc+5Lvo8vPR5BuuLwrP7illucK3XRhRBClDuZSReikpnNNjtnlfNt+UxbP404axxn1DmDMYvHMGvLLNbfu55eH/YyzGzvzta2FHj+9+ddbTM3z3Qd14qv5drG3pcn5z0JuEsOnhL0KSx56b77FdWAb8xTdTyD9P87dzSg5dPXTqhNnFUL2tcdXseIX0fQKKURTVKbeD1GCCGEiBSZSReiCpq/cz4Ah3IPMWrhKF5b8holjhIA7A6765reC4te8PtMfwG6Zw11/SLWYDnz6CvcxwuD63e0jZ+Lxpz0iX9+AECJWsLR/KMUlxQD7tSklQdWIoQQQpQnCdKFqOI2H9vMkr1LOGPiGYA7T9qXjKQAKR8+OIN/MK9sEsj6I+vDeq9fKrB2GBwLofJLVCF0muLd7i9fXb/4FDiacxJw/5ZjY+ZGj2FJCUYhhBDlS4J0ISpZp3qdDOeZeZn8sv0X075r71nL9zd87/d5+lrm6/8dXuCcVZAVuFNF2D4AZnwGE7a52461gK+nwZEzzO+JzwLF4d0+5Tff77HHG89LF44eyTsCuBfnSglGIYQQFUWCdCEqmefM950/3Ok6rpNQx3DtZNHJkJ79xNwnQurvLDloVnGmvLWt3da78aT37ql8+S1suB4mboAlD3tfjz9uHqT741n1pbQk48BWA2ldqzVPn2ssI6mqMpMuhBCifEmQLkQl61i3o+t4y7EtrkWKoFVq0evzUR+GfTuMk0UnOTP9zIDP/nHrjyGNxd+mR+XNdHfVeF0efX5pzfjM9u62X40bLbnuUcoYRJ/7CqCVtdz8n820r6u9s1tGNwDOa3Je2Z4vhBBCBCBBuhAVZPAXg3lijvfMtj7fWV9b/L7u97Ehc4NX/682fMUnaz/x2qq+WipNOwEgt56W6qJG+e4P0HNc6DPpnpIPmjZ3Su/E1Kun0jWja9meL4QQQgQgQboQFeSnrT/x6pJXXefrj6znh80/cNvM2wz9ft/9O6DVJP/un++8nmN32Plz358RHdsnV3wS0edFjH6jonU3GHPTfWn8B5RxYaevxbeF9kK2HttKXnFemZ4vhBBCBCJ10oWoJB3f6Wjarq8csnTfUtM+n6/7PKJjiY+OD9ypMuhn0v/6T3D3JGRC6x9h5T2Qtg2Otwz5tftH7DdtX3dkHc8tfI5WtVrRomYIFWeEEEKIEMlMuhAVpF3tdlx7xrWu80GtBgW8p6JyxK/9+trAnSqDXTeTXpQa3D0WFVrPgtvP5dq33jBeu/TuoB7R5C3zjYo2HNHSj6ROuhBCiPImQboQFeShng8xtMNQ13ndxLoB79EvIj0t6dNd/Ehos9jYoABNFtO3nbG8JXWM9c592XNij2l7TJQ2sx9rDW5cQgghRLgkSBciAgrthXz+9+d+S/NNWz+NRbsXuc5/2WZeC13vaP7RiIzvlGUPLhg+p613SkuHuh2wWjwWmVoLyjSce7rdw1PnPsVT5z5VpucIIYQQgUiQLkQEPDX3KW6acRNzdszx2WfpvqW8tewtRi0cBcDBXPMKIqeFtTfBwv8zv+ZQoKR0uUyQM+nRFu9+64+s915gay0MZZReYq2xvNTvJRJj/OxeKoQQQkSABOlCREDrWq0BSItL89mn0K4FiDM3zwTgzrPu9OqTEptiyFuvtmZ8Cgv/C5lt3G17zoEPF8F/HfD6IVhzCywcHfhZdf8m2hJtemnymsmglLgbyhikCyGEEBVFgnQhIqB2Qm0guCopaw6tYc528xn3E0UnQt5V9JRyvCnk677IHOkAWwZqFRMnL4a952rtBbXgu4/d/dLX+n7mHb2Jtyb4eakuBclaCA3NK+YIIYQQVYkE6UJEwK7sXQCsOLAiqP4DPhvAB6s/ML02b+e8SA2rasmvCeN2wqtZ7ravp8MXP8HGa/zf28zH30mrWRCbi0O3d9HNZ94MaLuCzrl5jnFjI2sh3HgptJnp93X6XWCFEEKIyiBBuhARsOnoJgBun3l7mZ9ld9jL/Iwq6Xgz39d29vV/b43DhtPrXv4AmiyEHhMAUFXFde2PPX+Utqn0b94fFI+Z9IQsaDnb7+tW3iUlFoUQQlQuCdKFKIMle5egjFZc6S7+nFHnDMP5ZW0uK69hVU3+NiOylPi+BpCgq3LTdAHJZyyD2/tCy18BOPts7ZISXcDO7J0ALNpTWknHMJNeFNRQM8aa7zgqhBBCVBQJ0oUogx82/wDAr9t/Ddj3id5PGM6TYpLKZUxV1trbfF9TAgTpibqZ9Ca/MX3TdMPlBx+Ep17az9w/D+NFN8uOxflbCsW7n85pX/pSCCFEpZMgXYgyyEjSZlw3H9sMwKO9HjXtV2Ar4NbvbjW0fbXhq/Id3KlE8V1fHoB4XR67w0p2YbbhckwMvPRUAy7s0pT3Ln0PgB4Nerj6O9VLqheR4QohhBDlTYJ0IcqgXZ12ANSMrwlAt4xupv1sDltQbaeE4gRYdTvk1dLO/3gc3t5grNriyRHgn5riAHXH44/rnmVlSOshPrsO7zqcxOhElu1fpjWoVu9OqslMuqVY++z4mf+xCCGEEBVAgnQhyuDM9DOZcf0MHurxEAAP/PyAab9j+ceCfmb3jO7uk6OtwRZXpjFG3K+vw/eT4csZ2vncMXD0DFhW+rNnN3YH8E6zx/t/Zl66/+seM+kNkhr4f5wtz7sxUErNeS/C8O58OBkKninbzqRCCCFEWUmQLkQZbDiygSu/vJKvNmqpK0fyjriuFdmL+L/5/0eBrcC9iDEIzkox7O4Nb2+GDxdHdMxltrl0weuePsb23PpQlAhv7YbXPHK6l9/n/5n7z/Z/Pc44k779+Ha/3VfetZJldy4zNlrsdK7XmdS4VPObooqgwQpiYxTirFXsi5EQQojTjgTpQgRp74m95NvyDW2Z+ZmAsT76wZyDAExcPpEXFr3AHd/f4ZWP7k9uca52sKU0peNQlzKMuhyU6Hb3XKYLvvPqQq4u5/tkCBVScut7t917BufeMZNPPwWsutQgRzQ5xTl+H9elfhfObuAR+FvsXNT8ItrXaY/pwtEo7R1/7vsz+HELIYQQ5USCdCGC1Pitxlz06UWGttUHV3v1yxibgaqqFNq1Leinrp8a3gujisO7r7zpFmIy+233sT3WGGxvutKw2adPiSYVWQDqbqLDFT9z003e7+/b1F1X3bkeICClhBG9RnBJy0vMr5f+XM4vXkIIIURlkiBdiBAs2bvEcK4o5qX8FEWhR8MeZXtZWYP0rGaw8/zQ7imOh+mfw6YrtPMdF3rnlzuive8D2DYIPtKl9cx+G9YPDfzO2BM+L6mlUf7OB3fq3h9FVoE7R11/7Fdp+cVnz3uWo5+Oh+Q90HuM+3peHcD3/02FEEKIiiRBuhBBirPGuRaIOqmq+VSxqqo0SWlStheWNUgfvwM+XgiZbdxtJVHapwrMmgh/3m+858+HYf2N2qLQv+6FT+bB9x8Y+zhMqqX4svxe9zt9yWrt89LgVoMBSIx2V3+JtSQybf204MfgZHHv5JqWpsDDTeCiJ93X87UgvUu9KpZeJIQQ4rQUwv/aCnF6s1qsWBTj99qGyQ1N+zpUB1aLlXu63kOJWkKJo4TJayaH9kJLhEo0Hm0HKXvgpdJ8+sv+BY0Xw4p/a+d56dDvWe34QFf3fb89p31uvgK2Xgz7e0Dd9WCPD+39RclhD9359x1njYOM5XCgO0Utv+KBrnfx2pLXAEiITgjyYe4gXUEBBVJiU3DN4ydqi37b1G7jfa8QQghRwWQmXYgg5RbnUmB3l+ZbvGcxC3cv9Nl/a9ZW3l35Lrd0uiX0AB2Cn0lXgaIafq5b4J8r3effT9bKKDotesZ9rF/4qV8g+vnPsHA0fPVNcGNyvVuBQh/VVABu8F3vHCA6ShuDzWGD2y6A4d2g7XcMajXI1cdzMa9PUe4vPYqikBidyJ1d7oS7usDZ46HPiygoOFRHcM8TQgghypHMpAsRpHhrPDVi3MHwN5u+YfrG6aZ9VVRWHVwFQJ+P+pj2CUgXVKLieyf7GZ/A3zfDvztA+gbv644oKPBYXLnFIzh2KGBRoVgX7Edk4aoCRSnml2Kzoc0sn3eu//d6zqhzBqDt2EpMPjRYCWgz4KGK8si6ybPlMWXNFMg4BhnaAmAVrXSmEEIIUdlkJl2IIBXYCzhe4K7X7bk1vd6c7XPYlb0r/JeVWI27YpbE+O77983a5+o7zK9P/wrmPx/gfaXPL05yt+nSQ8K2tzfk+tioqCiVtLg0GHi/6eX2ddu7FnF6zpanxGlBet+mfZl06aSghlKCd/B9rCD4TaaEEEKIiiRBuqiWiuxFPhd1lsUHq92LKD9a85HPfoO+GMQ7K94J7yXF8fDqUfjxXXebLYi86/1nw4JRkF/Tu/RhkZ+UE4CSWO2ztMIJ4L1AtMdb5vcOvsf/s3ef5/PSyaKT0ONtn9edMpLcNddTYlNolNyI7Q9sZ+bQmdzV9a6A9wMkxgW3QVEoG08JIYQQ5UWCdFHtFNmLiHsxjsfnPB6xZ5ZHwO/Tvp7eKSJzX4GcdP8bBO3tDb+NhDW3wZQFob3THqulvNh06S55uvz0jOVwhnlqD/HHzdud/nha+0zIhAEjoP5K16UStSSo4SXGJLL1/q0A3NLpFg7kHKDF+BY+043M5Nl9/+ZD73hhgJ9HCCGEqAASpItqx7nwT58/XlZH8o5E7FkB2RK921beDW8cgrH7tVQYf1YOh90XhPbOklj/KTUxuWDxEVDHBRnUdp0E57zpqqIC8Gr/V7WDICrZtKzZkj/v+JPXLnqNo/lHgRB3BzVJ3+nfvL9Xm+Iz+V8IIYSoOBKki2rHmccca42N2DP1VV2CdVmby8J72ep/+b9elOT/+rG2ob/THiBIRwXFR9WTQDPpToVp2me0O788Oba0POMDLeH8UXS/8KDfR/Ro2INYayw5xTkAbD62Obh3g1eQbrVY6Z7R3XXeKb0TALLo5ioAACAASURBVN0yugX/TCGEEKKcSJAuqp3iEq0qyYJdIaZ8+KFfJJpdmM2sLbNIjfOf5/395u/De5m+XKIZNcDmQP6cPR6S9nu3l8S689JN32kBxWQm/aphEJMT3Ls7lG5ApAvS5+yYox2k7oG+o0mrHfqXoaB5fMkY1nEYHet2dJ3bHNpsfrPUZuU3BiGEECJIUoJRVDt2hzZjunz/8jI/q8BWQMJLCYzoOcLVljYmrczPNX9ZCswKolJJXl1IPBreOzJWwObLvdsDzaSrUebpLs3mBbeo9fY+0OQPWtdqzbaYIpzhstdvO5TySzXpWO8Mw/nXG7+mToJ7oWxqXCoKCqrXqlshhBCi4slMuqh2nLtUNk1tWuZnFdoLAZi2IYxt6EP120jYcH3gfhM3wG/PwigVVt4Z2jssdvPSimtvgZPmu6cCWq11s5l0a6GxnrsvGSsA2HJsC47ULa7m9y59z9Ctfo16BCMtTvuidHaDs4PqD4BiDL7zbfmsPrSaxGhtDcCJwhOoqLKZkRBCiCpBgnRR7cREaTPCQzsMjdgzD+QciNizDPLTtJKJYNztM5AFpXXPf3g/cN/EQ+7jKJt5sL3sIfhgme9n+JpJtxYFt+mRtdB93HMcdJvIvHla1Raj4GbSU+JSiLPGGdJVAjlp886dn7dzHq1rtQZgQ6bJRlBCCCFEJZEgXQg/ThadLN8XvJoFrx6D1w/Azr7l844b3OkttWqkGGbS4xKD3FXUYTUN7mslJZGcEES6iz72thZz6zN/ceGF2mmDpAauS5Yg012apjal4JkCbu50c1D9AaKivJ99ffvrWX1otaFt/s75QT9TCCGEKC8SpItqx7mt+xNzn0AZrbB4z+Kwn+XMby93ufWNdckjKcn9W4BjhYcMNdg7dA2udjiqxXQm/VhhJkow6S6lHjvnMQAapzR2te3PcS9kjYmKXEUeT9FRxn/uCp4p4POrPic9MZ1bO93Kt9d9C0BCdBBfOoQQQohyJkG6qHbio+MB6NGgBxD6DpJ5xXkczj3Mwl0L+WbTNxEfn4ujgupx62uQR9kgp3TmWrFjjQ0ywHZE+SzBeMKW6T6JPwp3d+aqJ2exalVpW82trstWi7ZWfdXBVXiKZF17MzZHkeE8zhpHlCXKdXxF2yv46PKPeL7v8+U6DiGEECIYUt1FVDvOzWgaJjdk2f5ltK/TPqT7G4xtwImiE+UxNCO1Ar4jX3GrcWGnPmCPLiCr6DDQwOs2TzGWBIo9012al5ZP1LVbzvwKR/21fMsQvjlLpdHz3ejdsiPTSsuZd8/oTt+mfbmt822ue/o27cuCXQu4vfPt2PaE+POFoKAk37T9cN5hJq2cxLuXvmsYlxBCCFGZZCZdVDtFJdqM6extswF3/etgVUgJvvy0sgfpVvOg06Xtt9D5E2M1F4/jQwUmNdNN3H1tS2O6yw1D4OYBAERHu+u2N6/Z1HDf3pKVTNs8hT6N+wCwZO8S5t86n2vOuMZ9T1pzAM5MPzOosYQr1ipzEkIIIU4dEqSLamPO9jkkvpToykHPt2lB7Oyts0N6TrkvFl38iLZgdH0Zq8/YE2DIcN/Xm8/VPj3TXUpZoxRQTHLuW/8AN1zqOn3nHXj5ZQwz5v+7dDw7HtxB1uNZJMW601Sc6wGcEqMTGdBigOvcLH2obmJdQEt3+c9/tLZ77/X9Y4UrI6Wccv6FEEKIciBTS6LaWLZ/Gfm2fLYc22Job1mzZSWNyIc5r2ufsyeU/VldP9DSTsbtMrZfeTN0/EI79pHukhAb5ypXaWAthPruiif33OO81x2kF5Tk0ixNK3+YVZDlareVGFNiMpIyqBVfi6nbpwKwM3un1+uub389B3IOcHaDs2meBgUFEBfn6wcOnwPzRcDx1njOb3p+5F8ohBBClIHMpItqY3f2bgByi3MBuLHjjQC0qd2m0sbE/q5wtJX5NXuEKpmk7fZu6/QZWEoXeupSXGrEx7uOo60WoqOive+NKoIaByF9DbTU/RZCN5M+Y/N013GHuh1cx6qqrQeoFV8LgK1ZW5nxzwzX9XGXjPMear1OTLliiivtpTwCdICsQvNdWp1fJIQQQoiqRIJ0UW18sPoDw7mzfGJxSZC1wCOtIBXeXwFvbzG/XhJmNNq0tI53xnLz6/2eMp7risg8d97/uZstDpqkNPG+32IHi8quf2ri2DJQ1+4O0nee3OY6Xvfvda7j2vF1AAwLMOOs7p/zgR4PeL1uzvY5NH2rKRuOlO9mQnHRJl9IgO3Ht7Mta5vpNSGEEKKySJAuqh3nQtGvNnwFwLwd8ypnICcberfZzQPFkAwbBEPu1BZvApvu2+S+1vp76PMKAKlxqV631k6o7ToudhSSEpfs1adDvXYAFNjzMewtpCvBGO8j4G1bux3qSJXXB2gpPfse3sfOB3fSv3l/zml0juk9X2/8mt0ndrN031LT65FijYoybc9Iyghp51IhhBCiIkiQLqqdZqnNADi7wdkAtK3dtnIGYos3nu+8AF4IY1a//krjeXQRdP0Qkg4Dnj+fO6qOtngH0s4a8gCWKAfJsUleffLteYBJLXNdussjvR82H6tqrP3eILmB6ZcFPYdqXn890oocAarhCCGEEFWIBOmi2qhfoz4A7etqddHTE9MBLZ2hXBTHw+ZLweYjt9zukc4y2yMfO8pYCcWnK2+GuKzA/cBQ1jEzP9Prsr7+e6zVis3h/aXBGWarqkcpSl26S9s6rU1fnxidaNo+d8dcluxd4mvUFSLXZl77/kDOAa9UKSGEEKKySZAuqo1N923iyKNHaFVTW6j5w5YfAJi4fGL5vPCH92HqD/Dr6+bXbbqA9eupUOCxOFENcsfR6Hztj4ln+jzj8UztP+mvrvmKs+qd5dX/aIn7C0u01WI+i+1rWLqZ9Bpx8aZdohXzPPvzm5zP+U0qt4JKXLTvhbopsSkVOBIhhBAiMCnBKKqNVhNakZmfycyhMw3t5bY50bph2uffN8Pg+2Hn+ZDZHs4u/VJQpEsl2WBSE91hUv7QTHSBzyB95UGPVJjSwP944XFj+42D4GRDCmuku7ta7KB4/900SmnAdrRcbQPdTLqP9O7SRbqhVa1x/gakvAPlhikZpu2b7tsk1V2EEEJUORKki2rDmd7hWSe93DlLHH68UPts8Bc0WAFF3osyw+JnJv3nbT8bG1QLzdOac0XbK7h71t3u9tbOUorPuodtUYmJ8g6oE2K02fCE6ATjBd3CUYuP38HlFxdgFqT/tvs38xuA69pfR3ZhNj0a9vDZJxJ81UmvtDULQgghhB+S7iKqnQJbAQC3dLqlYl5o8Qj+cktnq4trePcNln6xqNX3TLqnbhk9WHnXStcunp705RBjrFasFu/v6bHWaPo162eoBANwSatLXMeKj5QYuyP0RaAd0zsyYdAEGqc0DvneUBzJP1iuzxdCCCEiSWbSRbXjTG/x3KK+3HgG6U6eC0dDYS10H0eVaCkvQUiOSSHVz2t7N+7tOq6RloeC92x/ri2XeTvnuUpZOjVNaRrw/VYlAiUmy0lKvHclGyGEEKKqkpl0Ue2UOLTc6S83fFlBL/QITJ153iVB5pybsdhZ9M8GFmxcQ7va7bxm0p/o/YTpbfqJ7F4Ne3ldV1WV//xvOrT4hc7D3zHN17c5tC83zp1bncYPHB9w2DVizAPhfs360btRb9NrFSXGKnMSQgghTh0SpItT3uajm/lyvTsgd+YYd8voVjEDyK8Lfzyma3AG6aEtoDRQHJzbpj0XtOvMiF4j4KLHiU7Mpc3NbwPw377/Nb3Ns2qiXv/m/UmNS+WFf13EkOf/xyvX/BvFJG/lYO4BADZlbjK0e86sh/r+ypbjowSjEEIIURXJ1JI45U1ZM4VXFr/iOu9QtwOAq2LHHWfdUf6DmPuqd1tZZtJ15Q6fnvc01MnE9mgSjlqtuCHjBmKiPJ7d4QtYfyN369aK6nfwTItLY+6OuRwvPE5KXArf3/A9AMUlWYBxgahiUvEF3L+h8MdXkD5vZyXt+qqTXRRkrXkhhBCiCpCZdHHK+3zd54bzBskNAPhl+y8AfLj6w4odkDPItYc/k94otYHr2Fm15rym57E1aytT108FQB2poo4sfddVN8P9LbnhBvczumd0dx07SzJuPrrZ8J4oi0ktxSDLt5vxtW70gqYXcF6T88J/cAQkxpjXdhdCCCGqIgnSRbWzcNfCyh6CpgzpLjFW7wWYaXFpvm+wOKCWcWdVs1SWAzkHDOcpcd61yds11irDtKjZIpihGlTldJfGui8+QgghRFUnQbqodjxni8uFv2DUUTo7HUK6yx3Djakk+3J3e/WZuXkmt3e+ndcv0nY4VUYrKKO1QLxJShOvkpN/7f/LddwstRmta7VmeNfhPsfw/fdw3XVw3R37AZM66TrJPkrA+wrSF+5ayO+7f/f5vPKUMngMJBzh4pvXVcr7hRBCiHBITrqodgrthYE7lZW/VBZHNOSkh5TucvOwKD58331eVGJeF33y5ZN9PkPxk6dSO6E2fw3/y+d1gCFDtD/7T3bhy2u+pElKE8P1WGssDL2cS9PvpVmzi02fURVn0ot7jYZuT3I46lngisoejhBCCBEUmUkXp7wbOtwQuFMQCu2F1HmtTnCd/W1U9N1H8MYh2DIk6HfX9dx7SAm8SFNv94ndrlx1pxNPnqBVzVYALD+w3PQ+s02JGiQ34Lr215EWb0yvsVqszH3xASaMauNzHFUxSP9p2E+gwLmNz63soQghhBBBkyBdnPJu7XwrUy6fUubn7Dmxh6P5RwN3PNYS/nzI9/WiVO2zOPjNc9LTPRoU9wrMng17AnBVu6t83j9z6EyW3bnM0JYcm8ygVoOCHkMgFsVCv+b9aJra1GcfX0F636Z96dO4T8TGEopoi5bfb1HknzshhBCnDvlfLXHKszvshhreHdM7Gq4nx/pIoPbgL13EYNIqWPRs0OMLRs2a0LjvHN1g3EH60juWuspK+nJZm8voXK+zV7uzZrxn6orrNWWo5GKmKs6kbzm2BYCtWVsreSRCCCFE8CQnXZyyluxdQt3EukxZM4U3/3zT1e4Z0A5pHVzaSWpcanAvDmGGPBR72jwKC9YCEB1t/P68/sh6tmVtC/mZA1sOBKBXI+/dR8uDryB987HN2EoCb4ZUHvbnaAthPSvbCCGEEFWZzKSLKuGnrT8xauGokO4ZMnUI4/4cx/SN0w3tc7bPMZx71lH3tO7wOuq+VpcVB1Zwd9e7/fYtT/rfALSq1dJw7cCIA+wfsd/QZqiT7kOTVG0GfcORDREapX++6qSvuXsN6+9dXyFj8BRvjTd8CiGEEKcCCdJFlTD4i8GM/m10SPdkFWSxIXMDqkc9xAd/fjCk5+zK3kVmfiYLdi1g0spJId0bSQ2SM1zHFosx2q2fVJ+a8TXDfrZzQyRPLVuaNods3DhISYEXXzS/XiexDnUTPVfHVoxuGd2AivttghBCCBEJEqSLU9qCXQu82jyD9kCcu3G+tuS1iIzJS+8xQXVbtMddR3xL1j8B++vrpPtTO6E2V7a90vTa/fcDfV6E4d2CGqMvDzwAx49D27Zleky5MNvUSQghhKjqJCddnPL2ndxXpvudC0ZT41LJLsyOxJCMrAVBdcuz5biOix2Rq/V+LP8YJ4pOmF6LjQX6RWYRbFWNhfOK8wA4WXSykkcihBBCBC/omXRFUeYqinK9oijBb6MoRJjsDjvDvh3G+iPln8fsLM1XLgE6gLUoqG7NajZ1n1hCq5Puj4rKF+u+iNjzTjWXtLyERbcv4vI2l1f2UIQQQoighZLuchbwBXBAUZS3FEXpGOgGIcK17vA6vlj3BddPv56ft/1cbu85lHuI7ce3l9vzic6FVj9qx3FZfrtee8Y1ruOBrS4J+Oivr/2aT6/8tEzDOx0oisK5jc+VtBchhBCnlFDSXeoDVwF3APcD9yuKsgL4AJiqqmpuOYxPnCamXzudJXuXuM6dtc03Zm5k4OcDWfyvxZzT6JyIv/fMd84kMz+T5Nhk/+kQJVaIsof+gvjjUG8dPNACLDZ4a4/PrhZdEBltDfz9+RpdUB/Ihc0u9Hmtb9O+xEdL5RMhhBCiKgl6Jl1V1WJVVaepqnoR0Bx4AUgHJgEHFUX5UFGU3uU0TlHNXX3G1bxx8Ruuc8+g8Vj+McP5Td/eFJH3Oque+FpYCcAP78LzNjiZ4buPL7FaLnjthichNsdv1xUHl7uOLZbI7QqUkZRB89TmPq/Pv3U+P974Y8TeJ4QQQoiyC6u6i6qqu1VVHQk0Ay4BFgC3Ab8rirJRUZSHFEWpEblhiuruy/VfcvcPWo3ypXuXctt3txmuq6gooxV+2fYL83bMC1j7PFgXt7gYgI/Xfuy708rS2ulrbwn5+e0bNeLgIwd57aLXQPGfZ94hvb3rWFV8FBwPw4GcA/yx94+IPU8IIYQQ5a+s1V06A5cBfQAF2AY4gLHAE4qiXK2q6hI/9wsBwNBvhgIwacgkzplsTGv55aZfXDPpl3weOFc7FJ3SO/HL9l+C6+wI/T+XjFop1KuRwuI9i8HiP10mKTbRdWyJitxM+hO9n+DsBmdH7HlCCCGEKH8hz6QripKqKMp9iqKsAlYAdwK/AP1VVW2tqmoHoD+QD/wvoqMVp6UBLQYY0l+uPePaiD37ho43BN950dNwuH3gfjo1Sn+fdG7jcyGq2G/fdUf+dh0rSuSC9Ff6v8JV7a6K2POEEEIIUf5CKcHYT1GUz4EDwAQgAXgcaKCq6lBVVec7+5YevwKEFtEIYUIZrXA0/6jrPCE6IeRnFNrN645/svaT4B9ij4d3ApSEjDfmzjuD9Fs730re//mv030o76DrODVessWEEEKI01koM+lz0Kq7zAD6qqraVlXVN1RVPeaj/zZgcVkHKARArfhaANzV5S5m/DMj5PtPFJpv5vPh6g/LNC4vVuOXgRq6WDvQl4uJg92/eEqJS4rosIQQQghxagklyXYE8Imqqv6LPZdSVXUB2oJSIcpMRUv/UBTFtflQOPd7iugulImHIMq4cVGNICbEv/9e+6yZkOZqUyKYky6EEEKIU08oJRjfCjZAFyJUv9z0C69f9DoAQzsM9bq+cNdCACatnMTNZ94c8vOPFxwv0/iCEp/ltbtoMEH6kCHap0X3X+PRgiMRHJgQQgghTjWh5KTfpyjKXD/Xf1UU5e7IDEucbga0GMAj5zwCwNSrp3pdn/DXBNdxicN/KUMz648EyCX3dDID8tMC99Oz2L0WhwYTpDvpN8S0hFUcVQghhBDVRSihwG3AVj/XtwD/KtNoxGnr/ZXvc/6U85mwbAILdvrPkpq4YmLIz/9ozUem7YNbDTY22GMgpx6M3Q+vhviLI0tJyOku7dq5j/VBegn+K8EIIYQQonoLJSe9FWAe6Wg2ADeWbTjidJFXnIdDdZAUm0RucS53zboLgN93/+7Vd+T5Ixn92+gyva9WQi3T9k7pnfhxq263zfFb4WRj9/mf9wf/EosdWs+CA+6a5L6C9Msvhw4d4K673G36IF2J4I6jQgghhDj1hDKTHg3E+bkeF+C6EC6pY1JJfiUZgKSX/Vcy6d+8f5nfd17j80zb69WoZ2zQB+gAP48P+h3xMTFw7itw5U2utsRE876pqfDCC9BY9zpjuosE6UIIIcTpLJQgfQtwkZ/rA4DtZRuOOF3YHdrum7YSW8C+fT7qE/6LbHFgj2Fw68Gml19Z/Er4z/agWBxgtTHz5etcbfHx5n2LTbJZ9EF6WnxKxMYlhBBCiFNPKEH6VGCAoijPK4oS42xUFCVaUZTRaEH6F5EeoKieLmp+EcmxyXy14StDe+OUxj7uCENJFLx8At7axZ4Te0y7HMg5ELHXZaTUBaBr/a6utthYY5+00rWoPXp4369fLJoSJ5sZCSGEEKezUIL0N4HfgWeAA4qi/KEoyh/AQeD/gD+ANyI/RFEd5dvyOVl0kptm3GRov6z1ZZF5gUOBz2eDIwZy6/PdppnB3VMGUVHap76Ou2eQ/vffMGUK3Huv9/2GnHSpky6EEEKc1kKpk25Dmy1/EtgHnFX6Zy/wONBfVVUpSSGCsvj/2bvvOLmq+v/jrzPbstkkm03vnSRAgISEFmooCliQJiIIiCiKgKj4EwsoCBYQ+SqoSBFBqvSi9BpASoIQElJI732T7CZbZ87vjzs7c6futN17J/t+Ph77yJ1zz733g4mPx2fOfu7nrErcjPYL47/ALe/fUpgHrDoUlkarsz5csyBhyrq6dbEDwfKEOdnY0eJ0g6lvro+MxSfpw4bBuedCWVni9e4kvbYx1Ua+IiIi0hVk092lLVG/PvwjkrOpg6cye93smLFUu4LmpDX2HeaWxsSsuDkY950yzyS9vMxZSu9W2o2zzoJ582CffTK/3p2kt9KUeqKIiIjs9rRliniiV0WvhLFnFj1TwCfEJvzBpm5Ya3lm0TOpN0MKViQfz1DPbs5bopVlldx7L3zwAZRm8TXYnaQvqU23JYGIiIjs7rJaSQcwxgwEpgE1JEnyrbX3FCAu2c29ujz9hkV5M7FJet+SETy96GlOevAkrjv6On56+E/Z2hC3WVGeK+lNdpfzZ6uzCm6yLHF3vzjaVt8uIiIiXVPGSboxJgD8GbiA9CvwStLFd/bpcxDrwp1clm9bDsC9c+6NndSa30r6hp1rAKhtrGVor6FZX+9O6isrkhSti4iISJeRTbnL5cCFOK0YzwUMzkuk3wU+BWaRvo+6SOexscvY+/Y5hJG9RwLOLqMA2xq3xV6T50o6JU4ZjbW51da7k/Q9+o3JLxYREREpatkk6ecCz1lrzwGeDY/NttbeCkwF+oX/FGnXCeNO6NgHxCXcizesZUT1CKYMmsI+A523OScNmBR3TX4r6RP6jwWgf1X/nK53J+nVlSm2KhUREZEuIZskfQzwXPg4FP6zDMBauxO4C6cURqRdm3dt7tgHxCXpb3w6m1G9R3Hr52+NJOdLapekvSZbEw9YDYAht37r7iS9R7duqSeKiIjIbi+bJL0BaNvDvR6nfcYA1/n1wPACxSW7uffXvt+xD4hLuFdtX8virYs56I6DeG35a4Crm8yaaTDnqzkl6W+8AddcA088AQ17/w2Auua6nEJ2J+n1rdtSTxQREZHdXjZJ+gpgLET6pS8GjnedPxbYULjQZHd2yLBDOvYB8Qm3NXyy6RMA3lz5JgDj+oxzzt3+Pjx2H6w+OOvHHHggXHklnHQSjKoZATh90nPh7u4SNOqTLiIi0pVlk6S/Apzs+vxP4ExjzKvGmNeA04F/FTA22Y2VlXRw95Jnb475GDABdrU4LRLbXhidMWpG7DWbJ2T9GPfOoaN6jwKgoiS32vaYHUebNuV0DxEREdk9ZNMn/ffAC8aYCmttE/AbnHKXs4EgcBvwi8KHKLujN1a80bEPaKyJ+TioahAmvMGRxfLu6ne55f1bYq9p6Z7+npMegLlnAvDuuxAKxa5+L9u2DICmYG6r4O4kvaQklHqiiIiI7PYyTtKtteuAda7PQeDS8I9Iu7704JcY1msYt5x4S/uTs1XfH5p7QJ9lSU/v0Xc8sBBwWiQ+/MnDrK9fHzuppZ2OKt1qI4cHHph4etbaWQCRFftsuZN0U6okXUREpCvLKEk3xvQAngLus9be2bEhye5q5sqZjKnpoP7fv9/o/Nl3IVw4JeH0/oOmMrTXTsCph3/m02cS79HcTpJeuTXt6X+d/i8en/844/uOzyjkeO4kfVDPfjndQ0RERHYPGdWkW2vrgQM6OBbZzQ3vNZwhPYcA8MUJX+yYh2yZAH+dkzBcUVrB0F5DmTxoMmP7jI12dnFrZyU90H172vMDqgZw4bQLswo35v6u/zf2Up90ERGRLi2bF0c/BPbsqEBk9/fRho94auFTAKzcvjL3G62dAi9dBy0pXtCsHZcwtGTrUkZWj+TD9R9y3D9TbIwbrknvMXYO++0HZ5wRe3r57TcAcOKJOUeelnslfXCv3DZEEhERkd1DNkn6L4BvGmNmtDtTpB0frv8w94tv+wDe/Cn894fO51Caf8YTHwdgfd16Vu9YnXjeuo7D5S71fV/jww/hmGNip/53x7/Yvh2eeir30NOJ2XG0u1bSRUREurJsurucDawEXjLGfAQsAuLfkLPW2m8UKjjZfR024rBIv/KcbQ2vmLdUpp5T2gA4uXjSLwYh1/8FdoVXr0ud7izBoGve147jjEdewv7iy7nH247Y7i4d9hgREREpAtkk6ee5jieHf+JZQEm6JDWx30QG9xgMOB1WqO8PJgRVW/K7cWuaJN1El8obWhsSzwddjc7b2jaWNAPQ0uKaN/alPALMnpJ0ERGRri3jchdrbSCDH6UWklJVWRWVZU5C/daKt52OLDdszu+mIZN+Jb2NNRhM4ngoyaZK4SS9tTU6NH349BwDzI2SdBERka4tm5V0kbzMXjc7+iFYHj0OBSCQpC/4h+dA/UA47AbndzTWQMBVRN5aAbcshJ3pXrJ05qfc4dQdR5sSp9zFnaQ/e9azLN+2PM1zCiuQzdsiIiIisttRki6dZkDVAPYZsI/zIeRKjlsroDxJKcoTdzt/7ncPPHUnfPo5KN8RPb/iCKgbltGzJ/Xfh6G9tiWeCCZJ3sOr6z17Rod6VfRi34H7ZvSsQtBKuoiISNeWcZJujHklg2nWWntM+9OkK+pb2Zc+lX2cD+7kuLVbYpLu7tjS2s1J0AGae0XH20vQAy2RmvSSQAlDew5NnJOk3GVSr8MAOO88ePllOO209I/pCErSRUREurZsVtLHENuwru36wTi17ZuBnQWKS4qEudrwtX2/xj0n39Pu3Pmb5zN/83zqmuoSk/R4Ta5l7Kbq3IK7aBJjPr6LpcCizZ9yevXwxDlJyl3WbXa+MHTrBg8/nNuj86UkXUREpGvL5sXRUdba0XE/w4Eq4GfA+SAhtgAAIABJREFUNqBz364TX9jasDXpeGNrIxf9+6KE82+teit2BTtpku5KzB95MLfAqjawtHYxAJt3bWZ9/frEOUnKXbbUtiTO62SqSRcREena8k4FrLVN1trfAO8Cf8g/JCkm/br3Y2T1yKTn7vnoHv46669c+cqVMeMtwZbYFexkLRQbXUn65hw2uu1WCxXR+nVrYdbaWYnzFpycMHTo1zKp7OpYWkkXERHp2gr54uibwG8KeD8pApt3bebt1W8nPderwqkfH1A1AICDhh5EdbdqXl3+avpylzlnQt2Q/AK7cAoELP2q+rEZCJgAzcHmxHmvXJcwdO5x++f37ALQSrqIiEjXVsgkfTSQpJ+d7O6S7uQJzN80H4De3XoDYMJbahpMbLnLiiNgzQEw7W+weQI8dn/+QYV7nfco78FmwJgUWe/gWbBuWv7PK7AePbyOQERERLyUTXeXESlO9QGOBS4FXitATFJkSkzy2oz7Pr4PgAWbFwDwzup3AJjUf1LsSvrzNzl/9lvg7ECao332a+bjj8LfEwOtjK0ZS11LPQAVJRW8vSpuxX/7sKQJeqQDjRfO+BI0VVNTc7d3MYiIiIjnsvml+nJgWZKf2cD1wAqcRF26kD6VffjOtO/EjK2vX09tQy0VpRUA1IcT5SE9h3DKnqcwedDk5JsI3f0a1I7JOZZvfN11z0ArCy5ewL4D9wNgvwGTmb95fuwFb1/u+hD9cjCh34ScY8jbnk/C5PY75YiIiMjuLZtyl2tIbMFoga3AIuAla21Oy6DGmC8ClwBTge7AKuAp4NfW2i1Z3uso4GKcTjN9gR04XyDeAi631nrfumM3N/jGwZQGShnfdzwADS3hloal3agsrWR49fCk/ckBmPnTnJ/r3nyIQCslpoRh1U5v9EAgwMadG2MvKHHVqJfvhGbnBpMGTMo5BhEREZFCyDhJt9b+siMCMMZcDVwVNzwO+AFwijHmCGvtqgzvdT3wo7jhfuGfqTitIpWkJ1HfXM+QG4fw0GkPccIeJ2R83daGrdzy/i3cfOLNAFzx0hUAtIZa+WTTJwB8sO4DAJbWLmVp7VIaWxuT7/SZp5g67kArFsuOpjqgF/M3zmdB44LYC7rVRo8HfQgrDy94TCIiIiK58LSHhDHmcKIJegj4KXAy8E54bBRwR4b3uoBogl6H02nmJOB44ELgPiBYiLh3R4u2LKKuuY6fvfKzvO7zu7d+lzDW0Bq7m+ij8x9NXu4CsavbydQsSXnKnaT//eTbCZgAQdsKwM6WXYkXdNsWPf78t2H/2/i/x2amf34Hmz58OseOOdbTGERERMR72bw4ejVwqrU2aS2AMWYO8C9r7bVZPP8y1/Hfw/3WMcbMxilRMcBnjDF7W2vnpYmtDLjaNfRFa+1rcdNuyyKuLqeixKkfH9V7VMHvvf/g/Z3Vc7dU5S62nQbhR/8cHn0g6Sl3kn7u/mfF3ja+UAtiX1KtWQJfvJA9Jv07/fM72PmTz6espPC/ZRAREZHiks1K+snAi2nOvwicluXzZ7iO32w7CJe3rHSdO7qd+xwMtDXWXg0cbYxZYIxpNMasMMb80RhTk2VsXUpbR5PPjv1sXvc5dPihCWP/+fQ/zLjb+as+enT4rzJVuUtLko2N3EqaUp7q3j163NZnPOB0feR/6/+XeEEo/B11wpNQ5tzXYNI/v4M9Mv8R/jrrr57GICIiIt7LJkkfDSxIc35heE5GwkmzO3GO37Pd/XlsO7fb13U8DLgSmABUACNwus68bYzpnWl8XU2P8h4M7jGY3771W/7x4T+45b1bcrrPnv2S7w7atqGRtZb/N/3/ucpdQnDZCNj7Qefj9lHpH1Aal6TvG+2EUp6mS//q7WtiBxp7wWu/dI57L4sM9+veL/3zO9imnZvYtHOTpzGIiIiI97LdzChdklsDZLOZeVXc5/hiZPfn9rZ2iY/rE5z69v7AjUAvYCJwRfgnhjHmW8C3AEaMSNUOfvdW31zPuvp1AHz9ya8DcPGBF2d9nzv+l/wVgqE9nS4rry5/lXfXvAuhU5wT+9wPvVdl1h99/FOJK+mH/AGGzOLPX/hT0l06S0ucf+LV3apZ7T7x3P9BQzghL2mhLFBGv+79GNZrWPtxdKDZ62Z7+nwRERHxh2xW0ufhvIiZwDhbSX6R9Cvt8XbGfa5I87m+nXvFFTzzQ2vtk9baOwB37cCJyS621t5mrZ1mrZ3Wv3//dh61e2oJdWzTm8cXPB453tWyK1ru0vaiaCZJ+ue/k/hiac0yOPhmLrqIpEl6r4peAJw04UuxJ9YcGD0OtNAScn4G9xzcfhwiIiIiHSybJP1O4GBjzD+MMZFMNnz8d5y68DszvZm1thZw9cBjUNwUd7aUuqWHY0Xc52Upjqszi67r2bxrc4fef319XDVTW7lLSfjLQSCDxjslTYnlLuV1kcMeSX7fYsIl5v27x335cn8pCMfQ0f8biIiIiGQq4yTdWns7cD9wDrDeGLPaGLMap3b8XJzOLtm+8faq6zjSpNoYMxoY7jr3Sjv3eZPY9oqjUhzHJ/OSxrbGbZirDffOubfwN2/r7hIIJ+kZrKQfNuagxHKXQLRty4gRwFFXwRcuiIztaNoOwLyNn8TdzdXuJaDW+SIiIuIvWfVJt9aeDXwFeAbYHv55CviytfbMHJ7/J9fxecaYnxpjvgQ85Bp/qa39YngV34Z/fumKaz3wiOua3xtjTjLGnA+496y/P4cYu6y//+/vAHzt8a/FjH+65dPIRkWZ6FbaLXEwodyl/ZV0U9LCSXt/Lmbs6NFHM7HfxOjAUb+CqdFf6ISsc99ltctirku2ku4Hh404LNoBR0RERLqsbF8cxVr7L+BfhXi4tfZ1Y8x1ODuBBoDr4qasBC5IuDC5S4HJOF1dJgFPxJ1/Erg992h3b5Wl0daHfznxL+xq2cUhww5JOnf8LeMBsL9I1nw80cR+E/lw/Yexg65yl7fPf5vpT33c7n1mrn6ZD09/middY6dMPIX65vZeWXCtm1tgV7/Y8hofraRfMOUCAsbTPcZERETEBzLOBowxpcaYXmnO9zLG5JL0/xynB/srwDacri5LgJuAadbajEpUrLUbgYOAX+O0g2zCeTn1XZzV9FOstdpxNIW2PukA3zngO/xw+g8x4YLuvfvvnfQac3VmPcUTEnSIlLuM7juc6X+fnlG5S5+q3lz03Ldixp5f8jwPzXsoxRVE/ht2te04OvOncMMmWDc1OinQ2u6zO8u9H9/LrbNv9ToMERER8Vg2SfWNwAnA+BTn38cpg/lhtkFYa58gceU72bzzgPPSnN+Osyqf3972XVCP8uhblzf99ya2NmxlTM0YALY2bGXuxrk8NPchrplxDceNOY4Xl6bb1yoD4XKXZTvCDYEyeHF07nfmMuTK2NX9xVsXs2jLosjn5p83E7KJCf/2xh3OwbwvJ5yrqqygPsPfCnS0ldtX0tDS4HUYIiIi4rFsfq/+WeDRNOcfxUnipRNYa3lr5VvYpPvdZ6+2Mdpo5wcv/IBrZ15LacD5DtfQ2sDhdx3OtTOvpa65LtUtshPf3SWDlfTBPQdD9y0xY/M3zyfo+gVJWUkZFaXR7p3lpc5/w/y2Gvokz9mj38hsIu9Qi7YsYtWOVV6HISIiIh7LJkkfTvpWiEuJ7cgiHeifc/7JYXcdxgNzHyjI/d5d/W7CWNvGPqN7j6Y15JSEGEzyF0GzldDdJbNKpEF9e8Dg1Bv+jPnjmJgynB7lPcNH4bGK7QnXfLjpfczVhtJrsq7WEhEREekQ2STpzcT2Lo83CMhgRxophLomZ0V7Z3P8nlC5ufN/iS3u21ao/7f+f0wbMg2AgAnErFznzNXdpfFnje2upFdXO78xWPX9VXDGydB3YUyrxTbLtsV2cTHusvmQgRVHJd48/EWhIP9dIiIiIgWQTZL+IfBlY0x5/AljTBlwBjCnUIFJem1tB2PaD+Zo8dbFPLv42YTx4/55XOT4ubOeY8cVO+he1p03V76ZMPfWWXEvOzb2gt+vgaduS/7QtpX0khanPMVdk372ZxOmzwn/yyoNlELvVXDJRJh6J9UV6fen2t64zTmwBj46J/kkH7VgFBEREYHskvRbgL2Bfxtjphljyo0xZcaYacC/gb3Cc6QTWAr3ouOR/ziy3TklgZJIjfqOph0J57/z7+/EDvx2O9QPgQ++mfyGkZp0p0/62L6jo+dKG6PHQ96Hc45muKuQ6uIDLgbg1D1P5ZgxxzBpwKTIuT8e/0e+Pvnrkc8x/zstPCl5LBU7uPyQy7nqiKuSn+9ER4w8gqNGHeV1GCIiIuKxjItwrbWPGmN+A/wEp62hDf8EcAp+f2etTd0LTwpq7sa5AHy88WMOH3l4O7PT296YWKcd7/h7j+flZS8nP7ljMPRYn91Dw+UuJaVOEn3YiEMjLzyctvcp0Z2papbCmFcjrRQB/nTCn7jl/VuYNGASI6pHcNDQgyLnLj3o0hQPNFCe4qXXka9zw2eezy7+DvKNKd/wOgQRERHxgWx3HP0ZTi/yW4DngRdwdg09yFr7k8KHJ6lMHjQZgL3675X3vc7a56y058fUjOHdNYkvlgKw+DPwh7Xw2H30vb5v5g8Nl7ucM8XZqPbJTx+LPq9P+m4rbQn7oi2LeHrR09z38X1p5ro+VCT+BoDPXwhlTRkG3fHu/uhubv9Ae26JiIh0dVlvbWitfd9a+z1r7efCP9+31r4PYIw5uPAhSjrxLRittfzp3T8lLUlJ5ex9z057fmnt0tS7es660Plz7plsbdia8TPbyl2On3A0ANsao60VdwbbX9kHpzXknA1zmLMhg1chrIGKJCvpNUsYWe2fFowLNy9k4eaFXochIiIiHst7/3FjTH9jzA+NMfOAtwoQk2Rg5oqZAMxaOytm/OVlL/O9577Hxf+5OO312xu3R3bhrKmsyT0Qk2NtfLjcpTz8/qj7xdFPty1o9/KKkgom9J3A0tqlaeeVl5ZFPwSSvCBa0syJe5zY7vM6y5q6NWzatcnrMERERMRjOSXpxpiAMebzxpjHgNXADUBfQL+n7yQNrc6ulM3B5pjxElMC0G4v896/683YP40F4J8f/TP3QDLYhCipcLlLWblJuM/how5JdkWMpmATCzYviLzMmkp0J1UDwbZNjlwxl7Qwvm+qTXRFREREvJHV7i3GmD2A84FzcPqiAzyOU5c+0xZq+0tp19CeQ4HohkOR8V7OeCYdQtbXr+eW927h+revzz2QTJJ0S2QvoYhwuUu3ikDCfQKBzP4ZPbnwSbb+v600BVPXlEdq0q2BDfs6x73WwI5wu5jSBib0nZDR80REREQ6S7sr6caYSmPMucaYN4AFwA9wurt8Dyf1ut9a+4YS9MJZuHkh/a7vxxMLnkg5Z2wfZxU831XgS569JKN5r5/3OvYXcX/FFph3RuLk+H8JoZLEOS2VAFRUhCe7dhx9Ydl/MooJnFKdQT0GpTwf6ZP+zmWw+ATnuGpjdEJlLY2tjYkXioiIiHgobZJujLkdWA/cBVQBlwFDrLWnAIm730hBBG2QLQ1bEkpZYuaEnKQ2ZGNXsluCTt11U2v6jiUzRs3gkgPTJOghA+9cCuv3AWBwj8Fs2hlXK71ljxTXxv2CxiZJ0pucTYh693Y+Duw5IHLqK/ucnjb2NqfvlcG8tpr5bWOiY+4kvds2Xlr6UkbP6wxHjjySI0e237deREREdm/traR/A9gAHGytnWqtvdlau6WdayRPCzY7L04+s+iZlHPaXpiM72xSEnAS4sqyypjxX7z6C8zVhrqmOp5e+DSvLn+Vs/c9O9LKMcHHZ8Fzf4RbnfuPv2U8A34/IHZOMGHzWUdrRezn+KQdoNHJznv3dupRjt/j2Mip4yccmzg/TokpYY8+Kb4ktKfS1YWmvI7jxx2f2306wJVHXMmVR1zpdRgiIiLisfaS9PeBccALxpjbjTGHdUJMXV7bani6Moy25Dq+3KVtY6JFWxbFjN86+1YAdrXsoraxFoDznjiPb0/9dvIHbNy7/UCTrZBDYvLuTtJbusE9L0CdUzvftpLeHIqu/JekuG3MI2yQhVvab1Vo4mvhIbLLKQAZ1r93lmPGHMMxY47xOgwRERHxWNok3Vp7ELAPTrnLScDrxpglxpirgNHprpXctW1lb5JmmMnntmlr3/femvdSzu/XvR8A8zfP58cv/TjFjTNo/BO/Qr4uvCofn6RvGxU9/ugcWHqcc2xaqapyDgMl0f+O0gxeZ752xrX85LAc98+K29Qooz7rIiIiIp2o3UzMWjvPWvt9YCjwFWARcBXOjqMWmG6MyWKrSWlPdYVTrz2qelTKOW111LPXzs76/uvq1kWOtzel2DgolyT98XvgjZ/Av/8SO37rR06NO8SWwnTbHlnp3rAzGlMmK+k/O+JnTB0ytd15FaVJSnImPQATnoDP/ABwepOLiIiI+EnGLRittS3Aw8DDxphhwNeB83C6vVxqjJkJPGqt/Uvqu0gmhvQcAsBBww5KOaftpdKgDcaM9yzvCcCI6hEx43VNzm6b2xu38+H6D9sPIqMkPS6b3rgPvLJP8rnBcgg0xbZs7L4Jp70+NIUaIsOZrKRnqntZVeLggLlw5smRj3v227NwDxQREREpgJw2M7LWrrbW/spaOxY4Fid5PwS4uZDBdVW9u/XmK5O+wvBew1POGV7tnBvdO7bqqK1P+vTh02PG2zY/em/Ne7yz6n1o7Jk+CHeSXpeixWGqmvRkWsObK73w++hY32jdfFlp9HmZrKRnKmnFUMXOmI9t7SxFRERE/CKnJN3NWvuKtfYsYDCQWdNtSauhtYHH5j/G8m3LU85pWykf12dcZOzNlW/SXrv65mAzs359A/x2B+wYnHqiO0n/18MADOo2AnbVRMeDZWmfFaO1GwRLIOjaCdWVpI/tE/2yUVoKP/xh+MMhN2b+jCS2tfVJb3PMFZHDto2g6pvr83qGiIiISKHlnaS3sdZuV6lLYVhraQ42J/RAd2vrANMScv58auFTHH7X4dz0zk0AtIZak153wdMXwMrDnQ9LPhM9sWMw3DIPZn0zHITrn8Yqp6nP+t+9AtdvhXcvhuduhBVHZP4f9cTdcF1D7Fj3aDfPnt26R45LSuD3v4ddu+DYI6ozf0YSCSvpPTZEDo8Y6cTfrbQbIiIiIn5SwOpfKZSPNnwEwDOfPsMZk2J39Ny0cxNH33N0pEPLvI3zWLh5Iec/dT5ApL1iZWlsn/Rkelb0pK7tw9s/gs17wTO3wbTbk9ek14bLQp7NoappyWcTx8ojT6duZ0vkuCL8bmllJTx/9vMpv3DkJBB9zoS+E2j4WYOSdBEREfEdJek+1Lab6POLn084t3L7SuZunBv5PKr3KM567KzI57YNfuZtmtfuc+qadzhdV+J7hT/wBCw8KZfQs1O2K3LYx06IHLtXvwMmQHlJik2TMhCIX0kviSbpI6pHKEEXERERXypYuYsUXlvPc7f4vugWGzNvyy6nhOSDdR8kveexY1y7eT5xN9z+ntNIs9S1cVJnJOgQkzDXbumg74vxSXqggKvyIiIiIh1ESboP1VTWpDwX/2JofDK+pHZJ2nsP7Tk0dmDdNGjpDqumJ7+gI7lKT1o7KHeuKKmIHQi0cPgIpyb/rZVvdcxDRURERPKkJN2HBlYNTHku3cukADuanN00+3aP7i+1tm4tfHA+3PUqd7/3aOJFSz4DK47MLdhkTv0K/Khf+/NcK+k//kkQBn0Ap55ZuDiA7uWxtfl3nvw3pg2ZBsDy7csL+iwRERGRQlFNug+1Jdjj+45POBcwsd+r4ue01as/t/i5yNjzi5+Hp+50Prx3ceIDH3o8n3ATmRBUbWl/nqv0ZMyoUvh22w6iDxQsFOc3D9Gal4ryQKQDjoiIiIhfZZykG2OWtjPFAg3ASuAF4HZr7c70l0gyja1OfXjbLqFue/Xfi30H7sucDXOAxM2M6pqdazbu3Jj85vWpV+kLprlHZvNGvRo5LA10zPfFuuYdQLSNY8/K6Mr6/oP275BnioiIiOQrm3KXlUArMAqoAbaFf2rCY604SfrBwB+A2caY/gWMtctZV78uYayqvIrBPaKbELXtJJrOxH4Tox+acuw7Hor7p1KzGK5MkViHMki4B3wM3aJfQtpq7XtV9MotvgxVlpfSv7vzz3LSgEkd+iwRERGRXGWTpF8G9AEuAgZYa/e31u4P9AcuDp/7BtAPZ+fRPYBrChtu1/D+mvdTnluzYw3PL4m2Zvxk0yft3q+qvCr6oTHHJD1+d9HWblASTD53n3C5yuS7Ut8vrstKW619W019ocRvZtTU2sLL57wc80wRERERv8kmSf898JC19lZrbeSNP2tta3in0YeBG621IWvtn3EKiz9X2HC7hvg2i27Lty2P+dy2tT0WePpWePv7kXPbGrcBcPkLl0cvyGUlvXIzhJIk6cnULIGKeuf4pPPh/BRdY+I2S4qvte8oqzbXsveAvbn6qKs5cY8TO+WZIiIiItnKJjM6CJiT5vwcnFKXNm8DnVAAvfuJb7MYcy4ugW/b+IiNk2D2hfDCHyLnznn8HABeXR6t/WbZMZkHMjb88mlZQ+JKeqq6c3fybYDqlSnnRb5gACZ+ybtA4m9bUREiYAJcdeRV9K9SNZaIiIj4UzZJehNwQJrzB4bntKkA6nMJqqtzt0+MF5/AR3YfbemeMHfVjlUAtIZybEJeGv7rtAae+7/Yc8EUK+nBuN1BK2uTTjvm62+y6vurcosrCxWlsX3Sp03Xu8wiIiLif9kk6U8BXzfGXGGMiWSExpjuxpifAOeG57SZDiwqTJhdS7/uTo/x8pLyhHORlfSdfeG/32PXjnCyHCpxTwLgw/Uf8sgnj8CCL+QWSFvdeLAC5pyT2TWtcZsHle1KnDPxcZYPuTFheGT1SMb1GZdlkOlVlrm+TIx+mZ4VGXaeEREREfFQNn3vLgemAL8GrjHGrA2PDwnf52PgRwDGmG5AI/DnwoXadQyoGgBAc7A54VykVeEjD8KyY/lP4waYcRm0uF4Obe0GZU4bx9MfPh0eTF0+k1bbjqC7kpSFDHs7+TXBuCQ9WRVLzdKkO6Pu2X9PtuzKoL96FpyXQ8PfRU2ww1o9ioiIiBRSxivp1tqtOHXpFwMv4bRbbABeDo8dYK3dEp7baK39mrX23sKHvPtrCbakPLffwP2YPnw6LDsWgI/+G06g3TXiNy+EOV/lpAknYZJmyRkKpCmT+cqXko/Hlbucs985cPLXoNzVteXw65Je+tzi53h/berONrnY2eKquDIhupcllgWJiIiI+E1Wy4rW2mbgL+Ef6SBBm6K1IU47RXcv8UAgvEre1DM6accIeOIunty3gkOHH8pbyW406hVYfnT6QKbfCB+fnfxcj03Jxw/7TeRwTM0Yfn74z7nno/Gwz32wbn8YOAdKk38JmTp4KoN6DEofUz5MKG3nHBERERG/0O/+fejd1e+mPLesdhnPLX4u8tm09Spv7hk7MeSsaC/eujj5jdrbcOikr0PPNe3GGnHALTDtb9B/XmTovP3Oi9TXE7AwdHbaWxhjCt67PKa7iwnR1NqUcq6IiIiIX2SVpBtjqoCv4mxU1JfEimNrrf1GgWLrstKt9i6tXRrzuXFnOfxpIYx9Men8DU9flPxG8bXj8XquAZPFqrMJwcC5MUPV3aqZsyFd185Ys9bOyvx5uQgE2bBzA0N7De3Y54iIiIjkKeMk3RhzIPAMzo6iqVicXUelg1gszP1y7ODW8c5PMm9clXzc3Q0mmZplkFVpSGLte1VZVcr2j4N7DM7i3rkz7i8aJpS2B72IiIiIX2TTgvEPQDnwZaCftTaQ5KedzE8yESkRScJaC488lP9DkpW7VGyDff8Jx10OfRc7q+N52Lxrc8pzp+91el73zlRFiasFY57/PSIiIiKdJZtyl6nAr621j3RUMOLoU9kn5nNTaxOPL3ic/Qfvn13NdijNdzB3kt57mfNi5+S7neS8TTblLkn0KO+R8gvHaXudljB2xMgjCJhsvje2r1uZq6zHBNNuFCUiIiLiF9lkRDuAwjaxlqTiS0FW7VjFmY+eyYXPXJh0g6OUtqQogQGwJbDno87x9BvgmCtjE3RnUubPsonlLm+vfpuSQPJfrvSvSuy9HgwFC56kx3ypMSHKAmUFvb+IiIhIR8hmJf0x4LOo/WKHi2/B2NY3fX39eg4cemDmN7rvP6nPhUrhlLNg429hcIquK3mupM8YNSPl5kHJuqy8teotepb3TDI7d06fdKdlZf8efamprCno/UVEREQ6QjbLlj8GBhhjbjbGjDXG5LFLjqSTqk2gtZaq8qqk55LaNjr1uVAJlDXB0FlOe8TkT8z8WXEvjj546oN8Y8o3Um4etKYusb1j3U/qWPfDdVk8MzsVZeXazEhERESKQjZJ+jbgQOAiYBHQaowJxv2k2aJSMvXummif9D++80d2NEV361yweUFhHpLJO755vGg5vu/4tH3PTxh3QsJYj/Ie2X0JyYD7q2RlWRalQiIiIiIeyqbc5R6yW1qVHLnbBF72/GXsP3h/wNnsx9mcaGL+D2lvMyPIrNylWy001sCImTHDvbv1BmD+pvkJlxw35jg66xcxxrXC36d7dac8U0RERCRfGSfp1trzOjAOcYnfzChgAgRMgEsOvITr37oe+Hz+DznkDxlF0p73PqznwF+eCxOejowN6zWMYb2GATC8enjCNS8uTb7xUkdwfxdoDDZ02nNFRERE8pHVjqPSOfp3j+18MrJ6JO9/830Avvuf7+b/gG/vCwM/bn9eBivpT625DSY+HTMWDAUpK3G6qEwaMCmnEAulojTagnF1/XKcTqIiIiIi/lbYfndSEG2lIgD2F5Z7T7mX22ffztyNcwuT9A78ONkGoYkyqEm/dua1CWPxdei/OPIXmUZWcBWl0TrZtQ7nAAAgAElEQVT0z4w7zrM4RERERLKRMkk3xoSMMa3GmHLX5/gXRfXiaAeILxFZtX0V33rmW1z4zIUMCh6Q+sJ97k1/40kPwOS7MkvQgVxfQdiwc0PM58/t8bmc7lMIwVC0nWXPboV9KVVERESko6Qrd2l7UTQY91k6mPvFUXO14dz9zgVga8NWpjady0upLhz/DHx8duobn/bV7AJJVe5y2G+yus3aurXZPbeAGoO7AKf3eol+byQiIiJFImWSHv+iqF4c7Ty7WnbFfL77o7th9QEEq0IsW92Y+sJAMPW5nKRI0o/5aeTwhuNu4Ecv/ijtXVZsX1HIoHJWkkHXSRERERE/0NqiD/139X9jBz49Hu54j09/908+WLgp9YV59DUHp4tMu/fruzBSLvODg3/A9w76XsKUL074Ysznd1a/E/P5xD1OzCvOrLhKe0oC+ucuIiIixSGnrMUY090YM9wYMyL+p9ABdkXuchcAFpzk/Ll5T9Y+l6acxeS3kn79sdfH3S/ZSroz1rO8Jzd+9kZKA4m/jCkvSb9p0AOnPpBriFlz90kvKdEmuSIiIlIcMk7SjTEBY8wVxpg1QB2wHFiW5EcKrbVbZvPyKHepKqvikoMuiR1MltOGV9df/JrT6zzZpkQJK/JxelX0yinGXLjDM4H8ftMgIiIi0lmy6ZP+W+ByYB7wKLClQyISBlQNiB3INEnPYyV9Z8vOdlfAnWc4ie6CzQs4aNhBSaccNvywmM/HjTmOB+Z23uq5W0VJtE+6ql1ERESkWGSTpJ8NPGet7cSC4q7JvdI8Y9QMFnQfzbpMLszzxdF/fPiPmM99K/smfhMb+QYA5z15HudOPjfpfeJX1ztz5TxeeWlZ5FhJuoiIiBSLbNKWGuDJjgpEokbXjI4cX3f0ddTWZbidfZ416Vsbtsbezp1sH3wTHH8pHPf/Eq47atRRae979OijOWe/cwAoMZ3bYsXdJ11JuoiIiBSLbNKWj4HBHRWIJDf979NpbMywPX2eK+nxL6xu3rU5+qHnWjj4ZqjYmXDdqu2rYj5P7Dcx5nNNZQ13fOEOAIK20G0i02sKRltWqgWjiIiIFItsyl2uBu40xtxprV3V7mzJ2fbG7bEDmdSkf+Gbea+k52pJ7ZKYz+P7jk+YU1ZSxqKLF3m6sZFW0kVERKRYZJOkTwVWAJ8YYx7H6eQSnxVaa+2vChVcV5XQJ331Ie1fNPUOWDm9YwICstlsdkR18k6ce/Tdgz367lGogDJiXG0kSwJqwSgiIiLFIZsk/Zeu41TNui2gJD1PMWUnwSz+ivJYSS8vKY+phW/PV/f5atLx9urTO180MS8tVZIuIiIixSGbAoDRGfyMKXSAXV5rRftz2uRRk/7N/b/JKXuewo8P/XFG89tqzAG2XxEtz3n6zKdzjqEjxPRJT7o5k4iIiIj/ZLxMa61d0ZGBSNTAHgOjH4JZJOl5rKSbpDsXpVZZVhk57lXRi7JAGS2hFsoCZWmu6nwVpeqTLiIiIsVHaYsP9SjvAcC4PuMgmMEGQ23yWElftHURf5v1N3731u8iY30q+0QntLMKnWznUT8oK4l+D1V3FxERESkWKVfSjTFX4dSYX2etDYU/t0cvjhbA+L7j+cqkr3Ds6GO5YOk1mV+Yx0r63v33pr65PmYsZENJ547unXntuteCoVba/plrJV1ERESKRbpyl1/iJOm/A5qJfXE0Fb04WgAGw4NzH+TBuQ9CaxbdUEzypDqZj779Efvdul/k86odqxhbMzZmzrbGbRnfb9+B+zJr7ayM53eW5lAzStJFRESk2KRL0kcDWGub3Z+l421p2BL90EHlLkN6Don5/Mgnj3DfKfexfNtytjZsZWCPgfzmzd8kvXbZtmUJYzefcDNvrXyLshJ/1aS7KUkXERGRYpEySY9/UVQvjnae/65y9UnP98XREW/AyiOgz6LYqa4XRasrqvnuAd+lvKScGz5zQ2R85sqZvBn5lL4m/eBhB3PwsIMzj7WTuEvl1YJRREREikU2fdKlk1h3QpzvSvrAOXDyOdBjQ8xwaSD6V7/tiuRlLTO/PhNzfuaP9zutpIuIiEixyDpJN8ZMAw4CakjsDqMXRwstmyQ92Up6IAg1K/j+wd/npnduAqD+J/VUlVflHFJ87bqfuVfSlaSLiIhIscg4STfGVAKPAZ/B2cbREt3O0brGlKTnaVCPQdEP+W5mFE7ce3frHRnKJ0EHWFK7JK/rO1NFSfR/P7VgFBERkWKRzdriVTgJ+nXADJyk/FzgBGAm8D6wV6ED7Iq6l3WPfijESjpw6PBDI0Nlv3Je7jxv8nmMqB6R4b2jJTjuUhm/K3Vl5lpJFxERkWKRTdpyGvCwtfYqYG54bI219nngWKAcOK+w4XVN9398f/RDNi+OJl1Jd9oyDus1LDLUGmoFnD7o2e40WlVWRcuVLVld46W2/1ZQki4iIiLFI5u0ZTjwevi4LRssB7DWtgIPAF8pXGhd10PzHop+yHclPTxW3a064dQ9H93Diu3ZNe3Z2bIzq/leaw1Fv1AoSRcREZFikU3dQp1rfh0QAtzNtrcDg+IvkjzlXZPurKT3LO8ZGXrw1AcBeOT0R/hk0yd5hVdMVJMuIiIixSKbtcUlwHgAa20QmIdTAoMxxgCnAKsKHWCXt3nP5ONfOheq41bB09Skr6lbw3cP+C59K/tyxqQzADh1r1O58sgrMwwkfZ90v1J3FxERESlG2aQtLwGnGmPa1iP/BhxvjFkCfIpTl35ngePrchpaGmIH1k5NPnH0K1C1MXYsTXcXgDv/d2fsbqZZ6Ne9f07X+YmSdBERESkW2aQtvyXa1QVr7V+Ay3HKXGqBnwLXFzrArqa+uT52oLlH8okmBDb2pc9xfcckzgsn7hUlFUweNDnnuJqCTTlf6yX3SrrKXURERKRYZFyTbq2tBxbGjf0B+EOhgxLg3zfDrn7QWpn8vAlBXGeWf5z8dw77QbJ5UBIoYfLAySzZmluP87qmHTld57Xy0uiLt1pJFxERkWKRUZJujOkBfATcbK39v44NqWurbax1Dt6/2Pmz+6bkE5OspE8Zsm/yeTitCOdvns+mXSnut5sqDahPuoiIiBSfjNKW8Cp6X6C+vbmSn5ZgXA/yXSlqwZOspAcCBg79bdw8p9ylNFDK6yteJ2fhzYz6F1ltuvqki4iISDHKJm15B5jWUYGIwxiTWSMVE4IvfBMqtsGXznGGMHDQzYnzgD6Vffjkok94/IzH84rvlD1Pyev6zha00SRdNekiIiJSLLLpk34F8Iox5l3gH9ba4uzJ53PVFdUQLGt/ognB0NlwRU1kQT1kQyRk+OEXR6217Nl/T/bsn6KlY4b27r93Xtd7SSvpIiIiUizSJunGmBHAJmttA84LorXAHcD14daLu+IusdbaYzok0i6iNFAKoQyTdIipeCkJlETKUqLzon3Sx/cdn3d8I6pH5H2PzqQ+6SIiIlKM2ktblgFfCh+PCc9fiVObPhAYHfeTpAegZCNogxmupCf2RC8vKee9b74XO9h3kTM9rn49W30q+wCwrXFbXvfpbO7/bpW7iIiISLFor9zFEO2LPqrDoxEqSyuzW0mPM6L38OiH8U/D2Bed+5alaOWYoeZQMwAVpRV53aezaSVdREREipHSFj/KtCa9PfvfESmHKTH5LSPXN9UB0KM8xeZKPlVeEv3fUkm6iIiIFAulLT6zpWFLTivpbbXiJqaqJVqf3hxsLkB0sLR2aUHu01kCrsxc5S4iIiJSLDLp7nK4MSabnUnvySOeLq8l2JLTSvpdJ93lDLuTdNdLpKWBbBr5JHlcuDPkgs0L8rpPZwuGgoCTnWslXURERIpFJpnbt8I/7TE4eZyS9Dy8/VoVvHhD+xPjkvSygJPYp1pJ71XRK6+49h6wN0cdcDHXzLgmr/t0thBK0kVERKT4ZJKk34azkZF0ggu+PAKIa3N47gx491JYcHJ0LK7V4gfrPuDwkYenXEm3Ge2QlNrcjXP5+MSb25/oY0rSRUREpFhkkqTPtNbe3+GRSGqDP4Ae62PH4joqDuk5xBlOsZK+cefGvFbTe+a5Eu8V9/8eqkkXERGRYqG1xWJgQhBoSTvltL1Oc6bm1w49pQkF2AjJa1pJFxERkWKhtKUYmCCUpO/OYpJl565yl6qyqrxCaGhtyOt6r6hPuoiIiBQjpS3FwISgJP1KemRqinKXkkB+tR7zNs7N63qvuPukq9xFREREikXamnRrrZJ4P8ig3CUyNcWLo02tTQUOqji4f8OglXQREREpFkpbioEJtVvuEpmaYiU9YPL7qz5uysS8rvdKyAYjx0rSRUREpFgobSkGgWBu5S6ulfQe5T1yevS0n30fjrmC3pNn5nS910I22k9eSbqIiIgUi/y2oZTOYch7Jd2drGbj7M+PYlbZZTz8CcBDOd3DS2rBKCIiIsVIa4vFIs+a9K0NW3N6bNsmSKN7j87pej/RSrqIiIgUC6UtxSLDlfRY+e0yCvDn9/8MwCHDD8n7Xl4IBPTiqIiIiBQfpS3Fwp2kH3UVAKfueWrCtFQr6T0reub02J3NOwFnx9Jip3IXERERKRZK0ovFno9D5WaY+jc46lcA1DbWJkxL2Sfd5Jah7tV/LwBeWvpSTtd7rdTVH14r6SIiIlIs9OJosei2A340AAJO4l1eUp408U61kp7rjqGfHftZXl72ck7X+oH6pIuIiEgxUtpSTMIJ+rg+47DWcsTIIxKmdFSf9AumXJDX9V6xasEoIiIiRUhpSxFavHUxLaEWvjD+CwnnUq2kV5ZW5vSsF5a+AMDOlp05Xe81SzRJV026iIiIFAsl6UVsff16AA4YckBkLHYlPSro2nkzGyeMOwGAB+Y+kNP1XnP/76GVdBERESkWSluK2PJtywF4f+37kbFU5S7bG7fn9AxrnXu0vUBazJSki4iISLFQ2lKExvcdD8B7a95LPzFc7vKTw37CmJoxOT3rT+/9CYB9B+6b0/Vec784qnIXERERKRZK0otQt9JuQGwC2iZ+JX1iv4n8+phfJ52biUsPvBSA+ub6nK73E62ki4iISLFQ2uIjbaUl7alrqgPA0E6SbixHjzo6r5h+OP2HADyz6Jm87uOVEldmriRdREREioXSFh+xZJakV5RWYDAMqBqQcC5+Jb2spKwgseXaHcZr6pMuIiIixUhpSxEyGCyWo0YdlXgubiW9qbUp7+f1qujFhVMvzPs+XlALRhERESlGStKL0PzN8wHYe8DeAMwYNSPl3Pvn3t8pMRUDraSLiIhIsVDaUoSG9hwKwLyN8wBYvWN1yrk7mnbk/bwdTTt4dP6jed/HC+4y/xzfnRURERHpdKVeByDZe+v8t5i3aR4rt68E4NOtn6aYmVmNe3te/NqLkbaPxcaGlJmLiIhI8dFKuo8k69aSzLr6dZy4x4kxmxilskefPfINi2PHHMuI6hF538cLGTbMEREREfEVJek+kmkv81XbVznzM0jqK8uKsytLoVirlXQREREpPkrSfaS9PultteiR+e2WsxiOGHFEnlEVt1Co/TkiIiIifqMk3UdCNnVGOar3KNbUrYkZS9YnPV7AdO2/YiXpIiIiUoy6dgZXRI4ZfUzC2PTh0wE4YmTq1fJtTds6LKZioJp0ERERKUZK0ovEW6veihwP7jkYgCmDpvDEGU/w+BmPp7zusfmPdXhsfqaVdBERESlGStJ95K+z/pry3ILNCyLHUwdPBWDOhjlc+tylrK1bm/K6+ub6wgVYhJSki4iISDHyRZJujPmiMeZFY8xWY0yjMeZTY8yNxpi+Od6vhzFmsTHGun6OKnDYBXfJs5ekPX/tjGu57fO3RV4YXbF9BSu3r2RD/YYUV6jWQ0m6iIiIFCPPk3RjzNXAk8CxQA1QAYwDfgDMMsYMz+G2NwFjCxakTxhj+NYz3+KDdR8AMHvtbAAWbVmU8pp9BuzTKbH5lZJ0ERERKUaeJunGmMOBq8IfQ8BPgZOBd8Jjo4A7srznF4ALgMbCRNl5Dh9xeNrzbcn5urp1MePpWjGWlZTlH1gRU5IuIiIixcjrlfTLXMd/t9b+xlr7BPBlorUanzHG7J3JzYwx/YHbwx9/XLgwO8fzZz+f9nxbwt226VEmmx8dOvzQ/AMrYuruIiIiIsWo1OPnz3Adv9l2YK1dZYxZCYwMDx0NzMvgfrcBA4GXgJuBPxYozk7x0fo5wEFJzz1y+iMcMfIIBnQfwEkTTgJgcA+ny0vvbr07K8Sio5V0ERERKUaeJenGmBqcGvQ26+OmrCeapLdbX26MOR/4ElALnGettZmsNPvJifd9HtgUO1jSBMCpe50KwB9PiH7vmDrE6fIyrs+4lPdcXx//P2vXoiRdREREipGX5S5VcZ+b03zuke5GxpjRwP+FP37HWrsm3fwk13/LGDPLGDNr06ZN7V/QQWp31SYOljRz/Ljj+fXMXyecOnDogbx8zstM7Dcx5T2fWfRMIUMsOkrSRUREpBh5maTvjPtckeZze82+bwZ6Avdbax/KNhBr7W3W2mnW2mn9+/fP9vLCsUn+OkqaeG7xc/zslZ8lnOrXvR9Hjz6aXhW9kt/PWBpaGwocZHFRTbqIiIgUI8+SdGttLU5pSptBcVMGu46XtHO7YeE/v+rujR4359XwuI8LuJOU55TE/4JBsqGVdBERESlGXnd3edV1HOk/GC5fcfdHf6XTIvJS0pV0J0k/YdwJOdzPcODQA/MMqrgpSRcREZFi5HV3lz8Bp4SPzzPGLAE+wemX3uYla+08AGPMP4Bzw+NXW2t/GT6+AUhWp3KT6/jPwGLAt/Ufx4w+jpfjB8NJ+jNf7dq15blSki4iIiLFyNMk3Vr7ujHmOuBnOKv618VNWYmzMVF797kv2bgxxp2kP2KtfS3HUDvF42c8Qa+4/9pBw5p54NxXCZjcfulx0NDkLR27CiXpIiIiUoy8LnfBWvtznF1GXwG24XR1WYKzCj7NWrvCw/A61Vsr/hs7UL2cwy+5i6NGHZXzPW0Xf3NSSbqIiIgUI6/LXQAI7zL6RAbzzgPOy+K+RdUo/fR/fRlwdY888yS+dvi1ed1z+fbleV1f7Lr4dxQREREpUp6vpEtUfVNcV0oT4tOtn+Z1zxeXvJjX9cVOK+kiIiJSjJSk+0r8wr/ljRVv5HE7S1OwKa+Iip2SdBERESlGStL9JL4FowmxpLa9FvGSjpJ0ERERKUZK0v0kvoTehNiz35553XL68Ol5XV/sVJMuIiIixUhJuo/07x636aqx9Ovez5tgdhNaSRcREZFipCTdR2aMOjp2wIQYUDUg9xuWNjJl0JT8gipyStJFRESkGPmiBaM4Zq2dHTdi2bRzU/Y3+vyFsGlPGDAXOLIQoRUtJekiIiJSjJSk+8jSLctiB0yI48Yel/V9DjrpI4L2A2athYVbFhYouuKkmnQREREpRip38ZXEF0c/3ZJbn/SFm53k/PXlr+cbVFHTSrqIiIgUIyXpfpLQgtHy6vJXs77Nu2vepa65DoCWUEshIitaStJFRESkGClJ95MkLRhX7ViV9W3G1IwpUEDF78wznT9nzPA2DhEREZFsKEn3kySbGU0aMCnr2yy5dAkXTr0QgJMnnlyIyIrWkUfC6bf9iInfv9TrUEREREQypiTdV+JW0rH0ruid052uP+56AA4bcVieMRW/h9f+nr9+cLPXYYiIiIhkTEm6j3x5r6/EDpgQg3sOzule5SXlnDDuBEZUjyhAZCIiIiLSmZSk+8ibK96OHTCWHU07crpXMBTk2cXPsqx2WfuTd3Pdy7ozrNcwr8MQERERyZiSdB9ZW7c2dsCEOHjYwTndqznYDMCTC5/MN6yit+OKHSz/3nKvwxARERHJmDYz8pMkL44u3ro4p1uFrNN7cP7m+flGVfRKAiVehyAiIiKSFa2k+0l8C0YsLy97Oadb1VTWcMzoY3jotIfyj0tEREREOpVW0v0kyUr6+vr1Od0qYAK8dM5LBQhKRERERDqbVtL9xMaVZRjLfgP38yYWEREREfGMknQ/CcX9YsOE6Fne05tYRERERMQzStJ95PSJZ8UOmBBDew31JhgRERER8YySdB95ecnrcSOWhpYGT2IREREREe8oSfeRrfXbYwdMiH0G7uNNMCIiIiLiGSXpfpKkJj3XPukiIiIiUryUpPtJQpJueXHpi97EIiIiIiKeUZLuJ0lW0rc2bPUmFhERERHxjJJ0P4lP0rFMGTTFk1BERERExDtK0v1kQ9zGRcZSVVblTSwiIiIi4hkl6X7y6q9iPxsY2XukN7GIiIiIiGeUpPtcS7DF6xBEREREpJMpSfe50TWjvQ5BRERERDqZknSfW1q71OsQRERERKSTKUn3uReWvOB1CCIiIiLSyZSk+1x9c73XIYiIiIhIJ1OS7nOTB032OgQRERER6WRK0n2usrTS6xBEREREpJMpSfe5MTVjvA5BRERERDqZknS/OvgmACzW40BEREREpLMpSfchU9YIx/8AgP7d+3scjYiIiIh0NiXpPmSD0b+WZduWeRiJiIiIiHhBSbofjXs+cvjc4uc8DEREREREvKAk3Sesq/TcfPHbkePmYLMH0YiIiIiIl5Sk+8RFF0WPLz/6G5HjKYOneBCNiIiIiHhJSbpP3Hpr9HjfgftEjitKKjyIRkRERES8pCTdh7qXd4scHz/ueA8jEREREREvKEn3oRPHnxA5HtpzqIeRiIiIiIgXlKT7UFOwIXJcWVbpYSQiIiIi4gUl6T5kTPR4xbYV3gUiIiIiIp5Qku5DxsBfP/dXAJ5d/KzH0YiIiIhIZ1OS7lMGZzm9JdjicSQiIiIi0tmUpPvUw588DMA1M67xOBIRERER6WxK0n2oorSCl5e9DMDAHgM9jkZEREREOpuSdB8qLyn3OgQRERER8ZCSdB8KhoJehyAiIiIiHlKS7kMNrQ3tTxIRERGR3ZaSdJ+q6VbjdQgiIiIi4pFSrwOQ5Lb+eKvXIYiIiIiIR7SSLiIiIiLiM0rSfWrG3TMwVxuvwxARERERDyhJ96GKkgpeW/6a12GIiIiIiEeUpPtQWUmZ1yGIiIiIiIeUpPuQ+qSLiIiIdG1K0n2osbXR6xBERERExENK0n1qRPUIr0MQEREREY+oT7pPrbhshdchiIiIiIhHtJIuIiIiIuIzStJ9atpt09QnXURERKSLUpLuQxWlFcxeN9vrMERERETEI0rSfag0oFcFRERERLoyZYM+FAwFmfn1mTy54EmvQxERERERDyhJ96GmYBOHjTiMw0Yc5nUoIiIiIuIBlbv4kEEvjIqIiIh0ZUrSRURERER8Rkm6DxmjlXQRERGRrkxJug+p3EVERESka1OS7kNlJWVehyAiIiIiHlKS7kMBo78WERERka5M2aAPhWzI6xBERERExENK0n3A2tjPwVDQm0BERERExBeUpPtAfJIuIiIiIl2bknQfCMYtnKsFo4iIiEjXpiTdB+KTdBERERHp2pSk+0Ao7j1R9UkXERER6dqUpPtA/Eq6WjCKiIiIdG3KBn1ANekiIiIi4qYk3Qfiy12s2r2IiIiIdGlK0n1AL46KiIiIiJuSdB9Qki4iIiIibkrSfSChu4tq0kVERES6NCXpPqCVdBERERFxU5LuA/Er6SIiIiLStSlJ9wGtpIuIiIiIm5J0H1CSLiIiIiJuStJ9QOUuIiIiIuKmJN0HtJIuIiIiIm5K0n1ASbqIiIiIuClJ9wGVu4iIiIiIm5J0H9BKuoiIiIi4KUn3AfdK+sQffcu7QERERETEF5Sk+0BkJX3Ie/Tc40NPYxERERER7ylJ94FIkh4IctKEkzyNRURERES8pyTdByLlLiZI3+59PY1FRERERLynJN0HIivpJsSCzQs8jUVEREREvKck3Qfc5S4twRZPYxERERER7ylJ9wF3ucuUwVM8jUVEREREvKck3Qfc5S51TXWexiIiIiIi3lOS7gPucpf7597vaSwiIiIi4j0l6T7gLne575T7PI1FRERERLynJN0H3OUu4/uO9zQWEREREfGeknQfiKykB4Jp54mIiIhI16Ak3QfcK+kiIiIiIkrSfSCapGslXURERESUpPuCyl1ERERExE1Jug+o3EVERERE3JSk+4DKXURERETErdTrAASOPBI47QyoXgGc43U4IiIiIuIxJek+MGoUMOlfXochIiIiIj6hchcREREREZ9Rki4iIiIi4jNK0kVEREREfEZJuoiIiIiIzyhJFxERERHxGSXpIiIiIiI+oyTdB9bWrfU6BBERERHxEV8k6caYLxpjXjTGbDXGNBpjPjXG3GiM6Zvh9dXGmEuNMY8ZYxYZY7YZY5qNMauMMfcZYyZ39H+DiIiIiEiheJ6kG2OuBp4EjgVqgApgHPADYJYxZngGt9kT+CNwMrAHUA2UAcOArwLvGWNOKHz0hVFZWul1CCIiIiLiI54m6caYw4Grwh9DwE9xEu13wmOjgDsyvF0IeAY4HzgOuALYGT5XBtySf8QdI2RDXocgIiIiIj5S6vHzL3Md/91a+xsAY8xsYAVggM8YY/a21s5Lc5/VwBRr7RzX2EvGmBbgxvDnMcaYAdbajQWMvyB2tuxsf5KIiIiIdBlel7vMcB2/+f/bu/P4qKr7/+OvD2QhLEnYYtgDsruwJF/AX5FFbQG/CgqiVqWgosGi6NeKuIu0VlukVL8ufKsFKm2xFaS4Ii6sLkioWq0YK5AgBGQLSNgh5/fHnQyTyWRfZgjv5+NxH7lz7rnnfu4dmHxy5txzC1acc98BmwO2XVBSI865LUEJeoHMoNcRmQ03iG4Q7hBEREREJIKELUk3s8Z4Y9ALbA+qEvj6zAoe5qqA9feccxGZpDtcuEMQERERkQgSzp704O7joyW8bljexs3sLmCM7+U+4LYS6t5sZhlmlrFz587yHqrS9h/ZX+PHFBEREWGSFKQAACAASURBVJHIFc4kPbhXO7aE13llbdQ8M4DpvqK9wDDn3Pri9nHO/cE5l+acS2vevHlZD1VlCnrSm9ev+WOLiIiISOQJ242jzrlcM8vl5JCX5KAqLQLWN5SlTTOLBV4ErvQVbQEuds59UZlYq1vDGO+Lgks6XxLmSEREREQkEoR7dpdlwEjf+vnAXAAzaw8Ezo/+fmkNmVki3nzrA3xF/8JL0LdWVbDVpWCe9G/3fBvmSERE5FR3+PBhtm/fzr59+zh+/Hi4wxE5rURFRZGQkEBycjL16tWrXFtVFFNFPcXJJH2cmW0AvsKbL73AuwXTL5rZXGCsr/wR59xUX3kSXsLf3bftO+BeoL0v4S/whXNuXzWcR6Ucyz8GwKrNq8IciYiInMoOHz5MZmYmSUlJdO3alZiYGMws3GGJnBaccxw9epTdu3eTmZlJly5dKpWohzVJd86tMLNHgfvxxsc/GlRlMzC+DE1152SCDl4v/Bsh6g0Glpc/0uqVd7TMQ+5FRESKtX37dpKSkmjRokXplUWkSpkZsbGxtGzZEoB///vf9OzZk7p161aovXDPk45z7gG8p4y+j3eT51G8MegzgTTnXHYYw6sRBcNdNF+6iIhUxr59+2jSpEm4wxA57TVt2pT8/HxWr15deuVihHu4CwDOuX8A/yhDvXHAuBDly/GeTnpK01eSIiJSGcePHycmJibcYYic9mJiYqhTpw6ff/45/fr1IzY2eBLD0oW9J13gwDFvNkoNexERkcpSh49I+AX+P9yzZ0+F2lCSHkHaJbQLdwgiIiIiUoVOnDhRof2UpEeA+Nh4AH7c4cdhjkREREREIoGS9AhQL8qbnufz7z8PcyQiIiJSmuXLl2NmmBkpKSnhDkcCjBs3zv/eTJ06NdzhVIqS9Ahw9MRRANbmrA1zJCIiIpEvJSXFn4iVZVm+fHm4QxYpt4iY3eV0t//I/nCHICIiImXUq1cvVq3yHkBY2adKStW6//77GT/ee8RO27ZtwxxN5ShJjwAxdb3pshLrJYY5EhERkci3YMECDh8+7H89e/Zs5syZA0BycjIvv/xyofrnnHNOyHaOHTuGc67c01YmJCTQv3//ckYtFb3e5dGpUyc6depUbe3XJA13iQAO5/10LsyRiIiIRL60tDT69+/vXwJ7TGNjYwtta926NYmJif6hL9u2bWPcuHEkJSURGxvLV199xe7du5kwYQJ9+/alRYsW1KtXj7i4ODp27MhNN93Exo0bCx2/uDHpWVlZhYbZ7Nmzh4kTJ9KiRQtiY2Pp3bs3b7/9dpnO8cSJE0yaNInzzz+fVq1aUb9+fWJjY2nXrh3XXnstn332Wcj9VqxYwZVXXkmbNm2IjY2lcePGpKWlMX369EL1jhw5wlNPPUX//v1p3LgxMTExtGzZkksuuYSPPvrIXy/wfLKyssp9DariehdYvHgxl1xyCcnJycTExNCsWTN+9KMf8ac//clfp6Qx6QcPHuS3v/0tffr0IT4+ntjYWDp16sSdd97Jzp07C9XNz8/nqaee8teNjo6mefPmpKamkp6eztdff13S21c1nHNaApbU1FRX07JysxxTcUylxo8tIiK1R0ZGRrhDCIuHH37YAQ5w7dq1K7Rt06ZN/m2A69SpU6HXn376qVu/fn2hsuClcePGbsOGDf42ly1bFvJ4pR0LcDExMS4rK6vUczp06FCJMcXExLiPP/640D4PPfRQsfV79Ojhr7d7927Xq1evYuvOnDnTXzewfNOmTZW+BhW53vn5+W7cuHHF1h8xYoS/7tixY/3lDz/8sL98586d7uyzzy62jVatWrmNGzeW6VoCbv78+aW+hxkZGe7JJ5903333XbF1gAxXTE6q4S4RpG3CqT12SkREIteguYOKlF151pX8/L9+zsFjB7n4LxcX2T6u5zjG9RzHroO7uOLvVxTZfkvaLVx19lV8t+87xiwaU2T7L877BZd2uZTMXZmkv55eaNvyccsrfC6VsXnzZqZNm0bfvn3Jzs6mWbNmREdHM23aNLp06UJCQgL16tVj//79/O1vf+PPf/4zubm5zJgxg2eeeaZcx8rNzeX5558nMTGRO+64g61bt3L06FFmzZrFY489VuK+UVFRPPjgg3Tt2pUmTZoQFxfHwYMHeeedd5g5cyZHjx5l2rRpvPHGGwAsXbqUadOm+fcfPHgw6enpxMfH89lnn/Hxxx/7t9166618+umngPdkzNtvv51Bgwaxf/9+3nnnnQo9HbM4VXG9n3/+eebOnetv84orruDqq68mJiaGTz75hO3bt5cax8SJE/nyyy8B6NmzJ1OmTCExMZEXXniBhQsXsnXrVsaOHcvKlSsBWLhwIeC9DzNnzuSss85i9+7dfPvttyxZsoTo6Ogqu0bFUZIeAQrGog/rOCzMkYiIiNRu06dP57bbbitS3rt3b5577jnWrVvHrl27OH78eKHtgUluWT377LOMHj0agA0bNnDPPfcA8M0335S6b1RUFEOHDmXmzJmsWbOG77//nqNHjxYb0/PPP+9fT01N5d1336VOHW9U87BhJ/OLffv2FRqzP336dCZNmuR/fdVVV5XnFEtVFdc78Nwuv/zyQvFfeumlpcawd+9ef9INcPfdd9O6dWvA+4Pl1Vdf5dixY6xatYrMzEz/Hw8A0dHRdO7cmd69e/vLCt7H6qYkPQJE1/X+GsvIyQhzJCIiUluV1HNdP7p+idub1W9W4vY2CW1K3N6lWZew9ZwHGzVqVJGy2bNnc+ONN5a4X25ubrmPdeGFF/rXmzZt6l8vy2Pi33nnHYYNG1bi0yoDY/rqq6/865dddpk/QQ/2zTffFEqIR44cWWoslVEV1zvw3CoS7zfffFPoOl5zzTXF1v3yyy/p0qULEyZM4MMPP+TQoUMMGTIEgKSkJHr27MmoUaO44YYbiIqq3jRaN45GgBP53j+c5g2ahzkSERGR2q1FixZFyh5//HH/+tChQ3n11VdZtWoVM2fO9Jfn5+eX+1hNmjTxrwcmdK4ME0VMnz7dn1j26dOHBQsWsGrVKubPn1+udqpSYHIffKNlcWryeleFvLw8AMaMGcOKFStIT0+nT58+JCYmsmPHDpYuXUp6ejqTJ0+u9liUpEeAvKPeP4hLO5f+lY2IiIhUnJkVKdu8ebN/ffr06Vx66aX079/fn7CFQ2BMDz74IKNGjaJ///5FhoUU6N69u3998eLFRZLcgoS+c+fO1K1b11++aNGiIm0FJv+NGzf2r2/ZssW//tprr5XpPKriegeeW2nxhhJ8zpmZmSFv1MzLy2Ps2LH+NgcMGMCsWbNYs2YNubm5rFmzxt9G4B9L1UXDXSJIXatbeiURERGpUh06dGD9+vUA/OpXv+LGG29k3bp1PProo2GNKTMzE4CZM2cSHR3Nhg0beOCBB0LWHz9+PAsWLAAgIyODIUOGcNNNNxEfH88XX3zB6tWrWbx4MQkJCYwePZqXXnoJgMmTJ7N161YGDhxIXl4e7733Hj169OCWW24BvAS3IDmdOHEiEydOZN26dcybN69S51ae6z1+/HgyMrwhwa+88gpXX301V111FdHR0axbt44tW7YUGrceLDExkZEjR/rHsl988cVMnjyZjh07snfvXrKzs1m5ciVff/21f2rF0aNHExUVxaBBg2jVqhUNGjRg6dKl/jYD5+mvNsVN+3K6LuGYgjHnhxzHVNystbNq/NgiIlJ7aArG0qdgDGXWrFkhp9kbNGhQyHbLOv1goDlz5vjLBw4cWOo5vfXWW6XGFHyM++67r0xTMO7atcude+65ZZqC8S9/+UvIOoHTGZb1GlT0ep84ccKNGTOmUlMw7tixo8QpGIOPOWTIkBLrTpo0qdT3UFMwioiIiFRCeno6zjmefPJJsrKyaNOmDRMnTuTcc89l+fLlYYlp6NChLFy4kF/+8pdkZmbSvHlzxo0bx3XXXUfnzp1D7vPoo49y4YUX8uyzz/Lxxx+zY8cO6tevz5lnnsnVV1/tr9e0aVPWrFnDc889x8svv8xXX33FwYMHadasGb169aJv377+utdccw05OTk8/fTT5OTkkJKSwi233EKPHj0K3RhbHuW93nXq1OHFF19kxIgRzJ49m4yMDPbs2UN8fDxdu3blsssuK/WYzZs355NPPuHZZ59lwYIFrF+/noMHD9K8eXPatm3LhRdeyOWXX+6vf8stt5CUlMTatWv5/vvv+eGHH2jYsCHdunXjpz/9KbfeemuFzr08zNXwTQeRLi0tzRV8pVJTtu3fRsvftWTWf88iPS299B1ERERCWLduHampqeEOQ0Tw/j9+8MEHjBw50j/lYzAzW+ecSwu1TT3pEaBJXBOWjV1G56ah/zIWERERkdOLkvQIEBsVy6CUQeEOQ0REREQihKZgjAAHjx3kr1/8lW/3fBvuUEREREQkAihJjwC5h3K59pVrWbZpWbhDEREREZEIoCRdRERERCTCKEkXEREREYkwStJFRERERCKMknQRERERkQijKRgjQPMGzcm4KYO2CW3DHYqIiIiIRAAl6REgpm4MqS31hDgRERER8Wi4SwTIO5rH/2X8H1/v+jrcoYiIiIhIBFCSHgFyD+Uy4Y0JfLD5g3CHIiIiIiIRQEm6iIiIiES85cuXY2aYGSkpKeEOp9ppTLqIiIicUlJSUsjOzi5z/WXLljFo0KAqjyMrK4u5c+cCkJiYyB133FHlx5DTl5J0ERERkQrIysrikUceAaBdu3ZK0qtZr169WLVqFQD16tULczTVT0m6iIiInFIWLFjA4cOH/a9nz57NnDlzAEhOTubll18uVP+cc86p0fhOV3l5eTRs2LDa2k9ISKB///7V1n6k0Zj0CJDcMJnMWzO5ovsV4Q5FREQk4qWlpdG/f3//0rbtyeeMxMbGFtrWv39/oqOj+e1vf0ufPn2Ij48nNjaWTp06ceedd7Jz585Cbefn5/PUU0/560ZHR9O8eXNSU1NJT0/n66+9mdhSUlIYPHiwf7/s7Gz/eGkzIysrq8RzmD9/PiNGjKBjx44kJiYSHR1N06ZNGThwILNnz8Y5V2Sf7du3M2XKFM4991waNmxIXFwcHTp0YMyYMezYsaNQ3RUrVnDllVfSpk0bYmNjady4MWlpaUyfPt1fZ9y4cf54p06dWmj/lJQU/7bly5f7ywcNGuQvnzNnDr///e/p1q0bMTExPPDAAwA8/fTTDBs2jPbt2/uvYVJSEkOGDGHRokUhr8eGDRuYOHEiXbt2pX79+jRo0ICuXbty8803c+TIEaD0MelLly5lxIgRJCcnExMTQ/PmzRk+fLi/9z3Qxx9/zIgRI2jRogXR0dHEx8fTsWNHRo0axV/+8peQMdY455yWgCU1NdWJiIicijIyMsIdQlg8/PDDDnCAa9euXaFtO3fudGeffbZ/e/DSqlUrt3HjRn/9hx56qNi6gJs/f75zzrl27dqVWG/Tpk0lxnzVVVeVuP/tt99eqP7atWtd06ZNi63/6aeflukcevTo4a83duxYf/nDDz9c6HiB57ds2TJ/+cCBA/3lnTp1Chlz3759Szy3mTNnFjrW66+/7urXr19s/dzcXOecc8uWLSv2fZ4yZUqx+9epU8c999xz/rrr1693sbGxxdYfMmRIie9dWWVkZLgnn3zSfffdd8XWATJcMTmphruIiIjUcmbhjiC0EJ3FVW7ixIl8+eWXAPTs2ZMpU6aQmJjICy+8wMKFC9m6dStjx45l5cqVACxcuBCAqKgoZs6cyVlnncXu3bv59ttvWbJkCdHR0YA35Oajjz5i0qRJQNFhNi1atCgxruHDhzN48GBatmxJo0aNyM/PJysriylTprBr1y6efvpp7rnnHpKTkzly5AijR49m9+7dACQlJXHvvffSvXt3tm7dyvz58zHfm7x06VKmTZvmP87gwYNJT08nPj6ezz77jI8//rgqLisA//nPfxg+fDjXX389ZkZUlJdWjh07lptvvpkzzjiDhg0bcuzYMTIzM/nFL37BkSNHmDp1KrfeeitRUVHs3LmTa665hoMHDwLQoUMHpkyZQkpKChs3bvTfmFuSt956i9/85jcAxMXF8cgjj9CrVy/+9a9/cd9993HkyBFuu+02LrjgAjp37szrr7/u750fPXo0N954I/n5+Xz33Xf+fweRQEm6iIiI1Ep79+71J90Ad999N61btwbg1ltv5dVXX+XYsWOsWrWKzMxMunTpQkJCAgDR0dF07tyZ3r17+8vuuecef1tpaWnk5eX5XxcMsymrIUOGMH36dJ555hk2btzIwYMHCw1xOXHiBGvXruXSSy/l3Xff9Q+fqVOnDkuWLKFXr17+utdff71//fnnn/evp6am8u6771Knjje6ediwYWWOryxSU1NZvHhxkfJLLrmExx9/nPfee4/Nmzdz6NChQtv37dvH+vXrOeecc/j73//ODz/8AEDDhg1ZuXIlrVq18tedMGFCqXH88Y9/9K9fccUVnHfeeQD06dOHCy+8kDfffJPjx48zZ84cHnvsMf/7CdC2bVu6detGmzZtMDNuvvnm8l2EaqQkXUREpJariR7rSPTNN99w4sQJ/+trrrmm2LpffvklXbp0YcKECXz44YccOnSIIUOGAF7Pdc+ePRk1ahQ33HCDv8e4og4dOsSPfvQjMjMzS6yXm5sLwFdffeUva9++faEEPVhg3csuu8yfoFeHkSNHFinbvn07aWlpRcbIBwt1bn379i2UoJdVYBvz5s1j3rx5IesVfKMyYsQIHnroIbZv386MGTOYMWMGcXFxdO3alQsuuIDbb7+dNm3alDuOqqYbR0VEROS0V9ArPmbMGFasWEF6ejp9+vQhMTGRHTt2sHTpUtLT05k8eXKlj7Vo0SJ/gt6gQQOeeuopli1bxqpVqwrNRJOfn1/pY5XEAsZBHT9+vNC2Xbt2lbp/qCE9s2fP9ifoZ5xxBn/84x9ZsWIFq1atolmzZv561X1uoRS8x0lJSfzzn/9k2rRp/PjHP6Zt27YcPnyYTz/9lBkzZnD++ef7e/fDSUm6iIiI1EqdO3embt26/teZmZkhb9DLy8tj7NixgDehxoABA5g1axZr1qwhNzeXNWvW+NuYP3++fz2wl7o8SefmzZv960OHDuW2225j0KBBnHvuuWzZsqVI/e7du/vXN23axOeff16kTsFQmcC6ixcvLhJX4JCaxo0b+9cDj/v+++9z4MCBUs/DQtzsEHhu1113HTfccAMDBgygbdu2/jH1gQLjXbNmDTk5OaUeN1i3bt386/fee2/I9/jEiRO89dZbgHcNWrRowYMPPsjSpUvJzs5mz549/mEy2dnZfPjhh+WOo6ppuIuIiIjUSomJiYwcOdJ/Q+fFF1/M5MmT6dixI3v37iU7O5uVK1fy9ddf+6dWHD16NFFRUQwaNIhWrVrRoEEDli5d6m8zcH72pk2b+tdzcnJ48cUX6dChA3FxcaSmphYbV4cOHfzr7733HvPmzSMhIYEnnnjCPwwk0EUXXUS7du3Izs4mPz+foUOHcu+999KtWze2bdvGSy+9xGOPPUaPHj0YP348CxYsACAjI4MhQ4Zw0003ER8fzxdffMHq1av948g7d+7sP8b8+fNp37499erVKzRNY3kFntuCBQs477zzyM/P55FHHgk5reSVV17Jvffey/79+8nLy2PgwIHcfffdpKSkkJWVxZw5c3jzzTdJTEws9pg33ngjr7zyCgDTp08nPz+fAQMGUKdOHTZv3sy//vUvFi9ezLx58xg0aBAvv/wyv/vd7xgxYgQdOnQgKSmJnJwcNm3a5G8z8H0Om+KmfTldF03BKCIipypNwVh0ar4dO3aUOAVj8D5Dhgwpse6kSZP8dY8fP+5at25dpM6ZZ55ZYrwHDhxwHTp0KLJfcnKy69q1q//1nDlz/PusWbPGNW7cuExTMN53331lmoJx3759Iad1bN26tUtMTCx1CsbA+Aps27YtZJzdu3d3SUlJIdtcvHixi4uLq9QUjHfffXeJ71vgMefPn19ivdatW7sffvihxPewLCo7BaOGu4iIiEit1bx5cz755BOeeOIJ+vXrR0JCAtHR0bRs2ZJ+/fpx//33F5oB5pZbbmHMmDF07dqVxo0bU7duXRISEujXrx9PPvkkM2fO9NetW7cuixYtYsCAAdSvX7/MMdWvX5/333+fyy+/nCZNmpCQkMDw4cNZvXo1Z5xxRsh9+vTpw5dffsldd93F2WefTf369alXrx7t27fn2muvpWXLlv66jz76KO+99x6jRo2iVatWREdHk5CQQO/evbn22mv99eLj43nzzTfp378/sbGxNGnShDFjxrBmzZpCM6CUR3JyMsuXL+eiiy4iPj6epk2bct1117Fs2TLi4uJC7jN8+HA+++wzJkyYQKdOnahXrx7169enc+fOjB8/vtj9Av3mN79h6dKlXH755f4HFDVu3Jju3bvzs5/9jAULFtCvXz/Au0H1rrvu4rzzzvM/+Cg2NpaOHTsyYcIEPvroIxo1alSh869K5k7XW76LkZaW5jIyMsIdhoiISLmtW7euxGEWIlJz1q1bxwcffMDIkSP9U38GM7N1zrm0UNvUky4iIiIiEmGUpIuIiIiIRBgl6SIiIiIiEUZJuoiIiIhIhFGSLiIiIiISYZSki4iIiIhEGCXpIiIitYimVhYJv6r4f6gkXUREpJaIiori6NGj4Q5D5LR39OjRSifqStJFRERqiYSEBHbv3h3uMEROe7t37yYvLw+AOnUqlm4rSRcREaklkpOT2bFjBzk5ORw5ckRDX0RqkHOOI0eOkJOTQ05ODjt27MA5R3x8fIXai6ri+ERERCRM6tWrR5cuXVi7di1bt26tcA+eiFSMc468vDx27NjBtm3baN++PQ0bNqxQW0rSRUREapG4uDhSU1NZsmQJW7ZsoU6dOupRF6lhzjnat2/PkCFDKtyGknQREZFapkGDBowaNYr9+/ezf/9+8vPzwx2SyGmjTp06xMfHV7gHvYCSdBERkVqqUaNGNGrUKNxhiEgFaLCaiIiIiEiEUZIuIiIiIhJhlKSLiIiIiEQYJekiIiIiIhFGSbqIiIiISIQxzZ1amJntBLLDdPhmwK4wHft0oWtcM3Sda4auc/XTNa4Zus41Q9e5+pX3GrdzzjUPtUFJegQxswznXFq446jNdI1rhq5zzdB1rn66xjVD17lm6DpXv6q8xhruIiIiIiISYZSki4iIiIhEGCXpkeUP4Q7gNKBrXDN0nWuGrnP10zWuGbrONUPXufpV2TXWmHQRERERkQijnnQRERERkQijJF1EREREJMIoSRcRERERiTBK0sPMzIab2TtmtsfMDpvZf8xshpk1DXdstYGZ3WFmL5vZJjNzAcu4cMdWW5hZLzP7tZmtMrPNZnbIzA6Y2edm9rCZNQx3jLWBmbU1s+fN7J9m9r2ZHTOzg2b2jZnNMbNzwx1jbWRmw4I+O7LCHdOpzsxSgq5pqOWScMdZW5hZPd/vwg/NLNeXa2w2syVm9tNwx3cqM7PlZfi37MwspSLtR1VtuFIeZvYI8FBQcUfgTmCkmQ1wzn1X85HVKlOBhHAHUcul+5Zg5/qWK83s/znn9tVsWLVOB2B8UFkU0Mm3/NTMBjvnPqrxyGopX2fJ7HDHIVJRZtYCeAvoEbSpjW/JA+bXdFynoWMV2UlJepiY2fmcTNDzgQeA9cAUoB+QArwADAlHfLXIF8A3QAZewp4U1mhqrz3APGAZcBwYC4z2besO3A5MC09otUbBL9NlwFa869wfuBfvszwWuBVQkl51/g9IBg4D9cIcS231FvDrEOX/rulAahszM+BvnEzQvwCeATYAjfA+m4+HJ7pa4zZCdwROAQq+DfrQObe1Io0rSQ+fOwLWZzvnHgMws3VANmDAT8zsLOecPqwqyDl3fsG6mU0JZyy12F+Byc65/QUFZvYW0AWvJx28PzylEpxzGcA1QcVLzawHMNz3Or5mo6q9zOxnwChgH/A74JHwRlRr7XDOrQ53ELXUxUDB78D1QD/n3MGA7YtqPqTaxTn3RXCZmSUAAwOKnqho+xqTHj6DA9b9H1C+4S2bA7ZdUGMRiVSAc25lYILuK8vH+wajQF7NRlX7mVlDMxsC/Cig+O1wxVObmFlb4H99L2+l8GeyVK3hvnHSR8wsy8xmm1nncAdVS4wMWP8nMM/MtvnuZcnw/SEqVe9mvG8qAP4DLK5oQ0rSw8DMGgONA4q2B1UJfH1m9UckUrV8Y3kvDCh6NVyx1DZm9nszc8B+YAnQFNgFPAw8G87YagMzqwP8Ce9bib875/4c5pBqu8ZAIhADtAOuB/5pZv8vrFHVDoE3k1+Ll7QnA3FAKvAnM3s8HIHVVmYWBUwKKJrp67SqECXp4dEg6PXREl5rZgw5pfi+6lvMyT9El+ANiZHqFQvUDXcQtcCdwCAgB7glvKHUWg74DHgQb0jRULx7VgqGYjTAuydLKicx6PUfgGEUfmz93WbWveZCqvWuAlr71ncBcyvTmMakh8eBoNexJbzWMAE5ZZhZa7wbwc72Fb0PjKpMT4IU8SSwAO8XcBrwC6AZcB/ejdE3hS+0U5uZtQJ+hZdEXu+c2xPmkGol51w20Cuo+G0zywFm+V53M7MznXMbaja6WuVwwHoOcItzLt/MlgKXAi3w7n8bCnwVhvhqo18ErD/rnDtUmcbUkx4GzrlcIDegKDmoSouAdX1AySnBzM7Bm1mkIEH/O3Bx0I1KUknOuU3OudXOudedc1Mp/EvhejML/qNfyq45XieJ4SWNzje0aE5AnXa+8n+EJcLa7YOg12eEJYraIztgfXNBZ4nvZ+A2TVNcBczsAk7+8XkYeLqybSpJD59lAeuBM5C0x5u7tMD7NRaRSAWZ2WBgFSe/5psBXO2cOxK+qGoXM6tfzKbAbynqohleJMKZWaqZcz9/fgAACyZJREFUxYTY1D/o9baaiKcWWxGw3tZ3v0XBfRdtA7ZlI1UhsMPkRefczso2qOEu4fMUJ++8HmdmG/C+brovoM67mn6xcszsJ0BBchOY5PQ2s72+9dXOuV01G1ntYWaXAy/h3fgF3lze/wB+5E3TC8Bh3xSCUnHLzWwL8C6QhTckIw2YHFBnY1X8YjiNbQX+J0R5H6DgyYy5eOOn9S1nxd0GXGRmf8HrPT+MN0vRXQF1Mpxzm8IRXC3yJ7xnsMQDLYFnfN8AXeZ7Dd6Q2tfCE17tYWbd8Mb7g/fZPKNK2nXOVUU7UgFm9ivg/mI2bwYG+MbuSQX5HuHdrpRqg51zy6s/mtrJzObiPbyoJNnOuZTqj6b2MrPPKPrUwEB5wHDn3LIS6kgFmNk4Tg550b/lSirDZ8YO4AJ1UlWemY3C60QJ1Sl7HBjjnHupZqOqfczseU4+EfpV59yIqmhXw13CyDn3AHA53pCWvXizumwAZgJpStBFJMCTeA8f2Yg3/eIJvAftrAOmA2cpQZdTxON4T9xeDWzB+913AO+JmI8DZytBrxrOuYV4D5NbgPfHz3HfzwXAeUrQK8/MkoDrAooq/PCiIm2rJ11EREREJLKoJ11EREREJMIoSRcRERERiTBK0kVEREREIoySdBERERGRCKMkXUREREQkwihJFxERERGJMErSRUREREQijJJ0EZEwM7MUM3NmNrUSbcw1Mz34ogr43ou54Y5DRKqHmd1hZi+b2Sbf//eCZVwVHiPZzKab2b/N7ICZ7Tez/5jZS2aWVpY2Qj0mVkTktFbOZLe9cy6rumI51fiu3RvOuUsCyu4A9jrn5oYtsABmlgjcASx3zi0PczgiUvOmAgnV1biZ9QdeBRoHberoW1YDGaW1oyRdRKSoMUGvzwduBv4ArAratrMKjpcNxOE9sruibgImVEEs1eEOIAuYG94w/BKBh33ry0NsjwNO1Fg0IlLTvgC+wUuUpwJJVdWwmSUDiziZoL8GvAR8DzQH0oDNZWlLSbqISBDn3J8DX5tZFF6S/lHwtmBm1sg5t7+cx3PA4XIHWriNY8CxyrRxKqrI9S6Nc65S74WIRDbn3PkF62Y2pbT6ZtYH+B+gP3AGcABYBzzlnHs1qPovgGa+9T8558YFbX+prHFqTLqISAWZWZaZLTezXmb2tpntA/7l29bIzH5lZmvMbJeZHTGzb83scTOrH9ROkTHpgWVmdomZrTWzw2a2zTfOMSqojSJj0gvKzCzBzJ4zsx2+Nj4ws74hzqepmc02s91mlmdm7/vObbmZZVXwGjmgHTAwaOxnSkCdNDNbFHCdMs3s/hDnuNx3zTuY2QIz2wP84NtWx7fPSjPbbmZHzWyz77ybBrQxCNjke/lwQDxZAXVCjkk3s/Fm9k8zO2Rm+8xsqe9r7SLn7Lv255nZCt941N1m9oKZNazIdRSR8DCznwMfAVcDrYFovG/jLgQWm9mvg3YZGbC+1cxW+T4v9prZW2Z2XlmPrZ50EZHKaQu8D7wMLAQKkrBWwHhf2V/xhrIMBO4GegFDytj+xcDPgVnAbGAEcBeQCwT/cijO23jDcqYBTYE7gTfMrH1BL7SZxQLvAj3xhqV8ApzrK9tTxuOEMgaYCewCHg0o3+k77n8DrwDfAjN8xzrPF2tPYHRQew2BFcAHwP2c/Jo6BpiMd70X4/V0/RdwI9DfzFKdc0eB9Xg9YjPxvpJ+xbd/XkknYWa/wXvvPgHuAxrhfbuyzMxGOOfeDNqlJ/A6MAfv/R/kiyXft5+IRDgzOwv4X7xO7XzgMbzPn/bA43hDWu41s3edc++bWQOgQ0AT9wU1ORS4yMxGOudeKzUA55wWLVq0aClhAcYBDhgXVJ7lKx8fYp8YIDpE+S99+/QJKEvxlU0NUXYASAkoN+BLYFtQu3PxjZwJLgOeDSof7StPDyj7ua/s/qC6BeVZZbxWDng9xHVaHqJuPWA7sBKICtr2P762BgWULfeV/SpEWwbEhSi/0bfPlSVd7xDnMDfgdRe8X9CrgZiA8pbAXt/51Q3aPx/oG9TuG3hDkhqG+9+0Fi1avCXgczzUZ/wTAdvewRvuUrD8MWDbfF/9VgFlDjiK1yny377PuYLyrYGfGcUtGu4iIlI5e/B6Swtxzh113jhxzCzKzBqbWTO8nmmAIsNNivEPFzB7jPN+EywDkssxdGJm0Ov3fT87BZRdinez5JNBdV8A9pXxOOX1Y7zxnXOARDNrVrAABT3TPwmx3xPBBc5zCMDM6ppZoq+dgnMt6/UOZQTeHwG/dV5vfMExc3yxt8P7diTQR865NUFl7+N9g51SiVhEpOZ0D1i/CG/igILlhoBtZ/t+Bt/P8opz7nfOuTco/A1aS6BHaQfXcBcRkcrZ4JwLOROIbyzjBOAsit4DFDw1V3E2hijb7fvZlFKGaYRqwzm328wK9i/QHshxzuUF1T1qZpvKEW95dPP9nF1CnTOCXu90zu0NVdHMrsS7aasX3rjRQJWJv73v579DbCso60DhKdVKe99EpPYo6DDZA+zHGw4HJ+9/CV6HMkwBqSRdRKRyDoYqNLM78cZYLwWeAnLwvvpshTcMpazfZJY0FaCVpYHi/ogo6/7VqOD4k4HPiqmTE/S6uOs9Evgb3pjx24Hv8Hq16gJLqPmJEir9volI2K0HhvnW5zvnrglVyTcWHeecM7NVePcSQeFvzVKCdssu7eBK0kVEqscYvLGOw5xz+QWFZjY0bBGVLAvvhqaGgb3pZhaN15Mcsve6jIp7ONR/fD8POOfeLaZOWY3BS8oHO+f8ibyZdS1HPMUp6BU/C9gQtK17UB0RiXBm9hOgYJatwNm2eptZwWfdarwOlTvw/sj/qZntx7sh/AjeTC/dgeF4N/HP9e33v5xM0kea9zC3b4DAqR7XOOdK/czQmHQRkepxAi8Z9Pea+qYUvCdsEZXsNbxe59uDym+i8k/mywOahCh/G9gB3GNmRbabWZyZNSq6W0gF19v/e828MT0PFBMPxcQUyqu+tif7/mgpaL8FcD1ej9inZWxLRMLvD3izOy3Ce8BQgdsCys92zn0BTMK7ERy8ceWv4n12/RFveF3gvT0455Zw8j6gGN/6G8AAX9lOvM+NUqknXUSkeizAm67rLTN7BYgHriFyHzj0ApAO/MrMOnJyCsYr8aZHrMzvi4+BG83sl3hfH+cDrznnDpjZz4B/AJlmNtt3rESgK958w5cT+qmgwRYAo4D3zexFvDHpl1G4lwzwj8n/FrjazDbgPQnwgCtmSjTnXKaZTcebgnGlmf2Nk1MwNgSuLWFIkYicwpxzz5hZwTC684FkvG/ttuE9F+M1vGlfA/e508w+AibiTcdaH9iCl6w/5rvpvFRK0kVEqsd0vF70G/FmTNmON2Z6DvBVGOMKyTl3xMwuxIt7BF5yvgbvgR0vECLZLYf78XqtJ+Il4IY3hOaAc+5tM/svvG8YrsPr1crFG1byO3wPhypD/C/5et3/B2/2l1y8X573cPKGzUDX4vVw/Rrv3LJ99Ytrf4ovsf853vzIR/GuzzXOuVVliVFEIoNzLqWc9dfifT6VZ5+X8Z6fUWHmm9dRRESkCDOri/cgojXOuUgdTy8iUutoTLqIiADeGPAQxRPwer/fqeFwREROa+pJFxERAMzsz3hPAf0Qb/aC8/DG0W8Aejvn9ocxPBGR04qSdBERAcB3E+dEoDPeDZHf4z3580Hn3PfhjE1E5HSjJF1EREREJMJoTLqIiIiISIRRki4iIiIiEmGUpIuIiIiIRBgl6SIiIiIiEUZJuoiIiIhIhPn/eYQlJrI5uEUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x864 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy: 93.32290291786194%\n",
            "\n",
            "Precision: 94.0498495213126%\n",
            "Recall: 93.32290036515388%\n",
            "f1_score: 93.48734514861322%\n",
            "\n",
            "Confusion Matrix:\n",
            "Created using test set of 5751 datapoints, normalised to % of each class in the test dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x864 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l8pZ5E1RPPq4",
        "outputId": "14372cb7-3c25-4909-ce85-50616fd5eb9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "\n",
        "X_val_path = DATASET_PATH + \"X_val.txt\"\n",
        "X_val = load_X(X_val_path)\n",
        "print(X_val)\n",
        "\n",
        "preds = sess.run(\n",
        "   [pred],\n",
        "   feed_dict={\n",
        "       x: X_val\n",
        "  }\n",
        ")\n",
        "print(\"ok\")\n",
        "print(preds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[307.589 162.976 319.364 ...   0.    329.752 161.651]\n",
            "  [307.567 162.979 319.362 ...   0.    328.527 161.655]\n",
            "  [306.298 162.951 319.351 ...   0.    328.495 161.681]\n",
            "  ...\n",
            "  [293.291 122.534 307.676 ... 128.953 315.438 119.884]\n",
            "  [289.392 140.743 307.615 ...   0.    315.393 139.408]\n",
            "  [295.848 161.658 307.628 ... 160.331 314.112 160.264]]]\n",
            "ok\n",
            "[array([[ 4.901822 , -1.793342 , -0.9453874, -1.9769709,  0.2525671,\n",
            "        -0.8459303]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IO3I35VSPPq9",
        "outputId": "c5918259-c937-4869-b9dd-90a092114360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#sess.close()\n",
        "print(test_accuracies)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.22187446, 0.28169015, 0.32481307, 0.24587028, 0.34533125, 0.32811686, 0.3611546, 0.4550513, 0.44322726, 0.33333334, 0.41679707, 0.4468788, 0.4712224, 0.39106244, 0.4086246, 0.46757087, 0.45574683, 0.45748565, 0.47487393, 0.4771344, 0.49817422, 0.4974787, 0.5106938, 0.45661625, 0.45644236, 0.43818465, 0.50895494, 0.47939488, 0.4399235, 0.5070422, 0.5136498, 0.51591027, 0.5303426, 0.47800383, 0.51591027, 0.53834116, 0.49017563, 0.53138584, 0.46844026, 0.5129543, 0.53173363, 0.5016519, 0.5291254, 0.5235611, 0.5256477, 0.51677966, 0.53242916, 0.543036, 0.5287776, 0.5166058, 0.47035298, 0.5359068, 0.5359068, 0.37958616, 0.5256477, 0.51677966, 0.46339768, 0.48965397, 0.4993914, 0.5023474, 0.50895494, 0.5315597, 0.50686836, 0.48826292, 0.53851503, 0.52860373, 0.5327769, 0.55033904, 0.5418188, 0.51608413, 0.54129714, 0.54147106, 0.5329508, 0.51990956, 0.5388628, 0.55938095, 0.5381673, 0.53955835, 0.5452965, 0.54320985, 0.55346894, 0.52703875, 0.54008, 0.5496435, 0.5406016, 0.5343419, 0.5437315, 0.5294731, 0.5512085, 0.5409494, 0.55764216, 0.5496435, 0.54477483, 0.5306903, 0.5310381, 0.53642845, 0.5546861, 0.53608066, 0.548948, 0.5461659, 0.5508607, 0.5685968, 0.54460096, 0.56007653, 0.5559033, 0.4460094, 0.53764564, 0.53364635, 0.5339941, 0.53486353, 0.5402539, 0.5051295, 0.53538513, 0.5579899, 0.5141714, 0.5515562, 0.54268825, 0.53834116, 0.5571205, 0.55259955, 0.5567727, 0.5491219, 0.5559033, 0.55033904, 0.55694664, 0.54860026, 0.5705095, 0.52112675, 0.57816035, 0.5437315, 0.5649452, 0.543036, 0.5449487, 0.5357329, 0.5524257, 0.54633975, 0.56285864, 0.56129366, 0.557816, 0.56285864, 0.5532951, 0.5614676, 0.5595549, 0.5811163, 0.55259955, 0.58215964, 0.545992, 0.5532951, 0.5639019, 0.5468614, 0.545992, 0.5379934, 0.5419927, 0.54860026, 0.557816, 0.537124, 0.54946965, 0.5378195, 0.55173016, 0.56042427, 0.54164493, 0.5661624, 0.548948, 0.55972874, 0.5632064, 0.57190055, 0.59989566, 0.6228482, 0.60615546, 0.6016345, 0.60389495, 0.6091115, 0.60302556, 0.6103286, 0.60302556, 0.6028517, 0.60806817, 0.5520779, 0.5579899, 0.59711355, 0.599374, 0.58494174, 0.6035472, 0.6117197, 0.6066771, 0.5945053, 0.6106764, 0.56042427, 0.6115458, 0.6205877, 0.6063293, 0.5835507, 0.58302903, 0.61484957, 0.58494174, 0.60806817, 0.5992001, 0.61954445, 0.6197183, 0.6181534, 0.61954445, 0.6134585, 0.6056338, 0.60806817, 0.60737264, 0.5873761, 0.6277169, 0.5851156, 0.5974613, 0.60111284, 0.6134585, 0.6181534, 0.6155451, 0.62110937, 0.6012867, 0.6185011, 0.59989566, 0.62041384, 0.61641455, 0.6315423, 0.62128323, 0.6117197, 0.63102067, 0.6188489, 0.5955486, 0.62910795, 0.62076163, 0.61519736, 0.6252826, 0.61919665, 0.63536775, 0.62910795, 0.6510172, 0.6311946, 0.6440619, 0.6529299, 0.6400626, 0.3731525, 0.43453312, 0.49991307, 0.55033904, 0.54234046, 0.57103115, 0.56164145, 0.5649452, 0.5826813, 0.57624763, 0.58963656, 0.58911496, 0.584594, 0.5732916, 0.5651191, 0.57346547, 0.5988524, 0.5802469, 0.581638, 0.6094592, 0.60337335, 0.6138063, 0.6122413, 0.63762826, 0.6233699, 0.6171101, 0.6240654, 0.6056338, 0.62110937, 0.61519736, 0.63571554, 0.657277, 0.6451052, 0.64840895, 0.6461485, 0.62980354, 0.65884197, 0.63171625, 0.64162755, 0.6508433, 0.6411059, 0.6198922, 0.6329334, 0.657277, 0.62736917, 0.6715354, 0.651365, 0.63745433, 0.65866804, 0.66649276, 0.6411059, 0.63536775, 0.62945575, 0.6536254, 0.66597116, 0.6510172, 0.68509823, 0.6816206, 0.6362372, 0.6619718, 0.68249, 0.6950096, 0.70509475, 0.68805426, 0.64041036, 0.69605285, 0.69170576, 0.64058423, 0.66266733, 0.6673622, 0.67988175, 0.67709965, 0.7024865, 0.6729264, 0.6776213, 0.66597116, 0.65779865, 0.6777952, 0.6875326, 0.67553467, 0.64980006, 0.66284126, 0.6722309, 0.6804034, 0.7009216, 0.7108329, 0.6847505, 0.7040515, 0.68144673, 0.70387757, 0.6753608, 0.6864893, 0.6965745, 0.6944879, 0.69518346, 0.7075291, 0.6685794, 0.65345156, 0.69709617, 0.7078769, 0.7148322, 0.70944184, 0.68822813, 0.65901583, 0.6955312, 0.7026604, 0.6644062, 0.7221353, 0.7056164, 0.71952707, 0.704747, 0.7217875, 0.69240135, 0.69831336, 0.6922274, 0.6863154, 0.6906625, 0.7257868, 0.6115458, 0.64632237, 0.7024865, 0.728569, 0.72474355, 0.73343766, 0.7030082, 0.72996, 0.6998783, 0.7143105, 0.7466528, 0.7330899, 0.709268, 0.7280473, 0.7313511, 0.7190054, 0.7167449, 0.7078769, 0.69727004, 0.7308294, 0.71222395, 0.71639717, 0.7249174, 0.7170927, 0.7127456, 0.71639717, 0.71100676, 0.7221353, 0.72474355, 0.71952707, 0.7238741, 0.72039646, 0.7207442, 0.7170927, 0.71570164, 0.7188315, 0.7363937, 0.7221353, 0.7207442, 0.6955312, 0.73152494, 0.73256826, 0.73274213, 0.7118762, 0.72630847, 0.72648233, 0.7297861, 0.71535385, 0.7217875, 0.7320466, 0.7304817, 0.7191793, 0.7410885, 0.7238741, 0.7348287, 0.727004, 0.7301339, 0.7282212, 0.73187274, 0.7341332, 0.70509475, 0.7341332, 0.7390019, 0.72996, 0.7391758, 0.7322205, 0.73065555, 0.73778474, 0.7360459, 0.6990089, 0.71970093, 0.72248304, 0.73065555, 0.7402191, 0.7390019, 0.72439575, 0.73865414, 0.7190054, 0.6805773, 0.7237002, 0.71257174, 0.7237002, 0.721092, 0.6990089, 0.7177882, 0.6918797, 0.7146583, 0.73343766, 0.73865414, 0.74161017, 0.7322205, 0.7170927, 0.71970093, 0.73152494, 0.72839504, 0.7130934, 0.7395236, 0.7410885, 0.72752565, 0.7167449, 0.7330899, 0.73465484, 0.7381325, 0.73587203, 0.7370892, 0.73848027, 0.7363937, 0.7351765, 0.7379586, 0.6906625, 0.7383064, 0.73343766, 0.7336115, 0.74839157, 0.7188315, 0.7369153, 0.740393, 0.7297861, 0.74143624, 0.73778474, 0.73621976, 0.7450878, 0.73621976, 0.7490871, 0.74474007, 0.7198748, 0.68144673, 0.73865414, 0.74178404, 0.7421318, 0.721092, 0.72752565, 0.7376109, 0.7466528, 0.7442184, 0.721092, 0.73274213, 0.7423057, 0.7322205, 0.7348287, 0.7301339, 0.72004867, 0.73065555, 0.7313511, 0.7311772, 0.728569, 0.7339593, 0.7426534, 0.74804384, 0.7341332, 0.7532603, 0.7341332, 0.75673795, 0.746305, 0.7549991, 0.73743695, 0.7348287, 0.7250913, 0.76021564, 0.7513476, 0.7395236, 0.75360805, 0.74804384, 0.74839157, 0.7569118, 0.746305, 0.7457833, 0.7360459, 0.7410885, 0.73152494, 0.7310033, 0.7473483, 0.75865066, 0.75899845, 0.74143624, 0.7576074, 0.7158755, 0.7593462, 0.74804384, 0.7560424, 0.7482177, 0.749261, 0.7471744, 0.75899845, 0.7525648, 0.74682665, 0.75465137, 0.7743001, 0.77099633, 0.764041, 0.7616067, 0.75986785, 0.76386714, 0.7553469, 0.7400452, 0.7682142, 0.7556947, 0.765606, 0.7636933, 0.7654321, 0.77325684, 0.76091117, 0.7569118, 0.7565641, 0.76769257, 0.771518, 0.76021564, 0.7654321, 0.7671709, 0.7659537, 0.7767345, 0.7696053, 0.7864719, 0.7713441, 0.7541297, 0.7682142, 0.771518, 0.77169186, 0.78003824, 0.78699356, 0.78421146, 0.7781255, 0.8191619, 0.79255784, 0.77829945, 0.76734483, 0.78229874, 0.8127282, 0.7704747, 0.77395236, 0.7762128, 0.7856025, 0.6915319, 0.7289167, 0.7541297, 0.77082247, 0.7737785, 0.77117026, 0.7675187, 0.7781255, 0.77638674, 0.77795166, 0.7868197, 0.75865066, 0.7619544, 0.78386366, 0.7755173, 0.74334896, 0.7723874, 0.76734483, 0.76699704, 0.79916537, 0.79134065, 0.74456614, 0.7395236, 0.765606, 0.77169186, 0.7652582, 0.774474, 0.8195096, 0.7501304, 0.7876891, 0.7722135, 0.8179447, 0.8108155, 0.7896018, 0.8132499, 0.82837766, 0.8362024, 0.8089028, 0.8362024, 0.82090074, 0.79916537, 0.8028169, 0.7934272, 0.8179447, 0.8089028, 0.82455224, 0.8322031, 0.85011303, 0.83272475, 0.83741957, 0.83863676, 0.8452443, 0.8148148, 0.8120327, 0.84854805, 0.8464615, 0.83046424, 0.8381151, 0.8506347, 0.84611374, 0.82281345, 0.82246566, 0.841245, 0.8328986, 0.802643, 0.81464094, 0.8158581, 0.8073379, 0.83776736, 0.84089726, 0.81742305, 0.84367937, 0.8334203, 0.8462876, 0.84298384, 0.82455224, 0.8555034, 0.8572422, 0.8261172, 0.853069, 0.8572422, 0.860546, 0.8542862, 0.8614154, 0.84802645, 0.8542862, 0.85376453, 0.85758996, 0.8523735, 0.86002433, 0.84889585, 0.8523735, 0.8612415, 0.84698313, 0.84976524, 0.8433316, 0.85359067, 0.8506347, 0.8424622, 0.8513302, 0.8443749, 0.8676752, 0.8502869, 0.8687185, 0.8746305, 0.84454876, 0.8716745, 0.8708051, 0.8582855, 0.86819685, 0.85167795, 0.8614154, 0.8615893, 0.86384976, 0.8614154, 0.8464615, 0.85915494, 0.8643714, 0.86611027, 0.86541474, 0.87097895, 0.8767171, 0.87010956, 0.8617632, 0.8648931, 0.853069, 0.87602156, 0.8751522, 0.86002433, 0.8704573, 0.835333, 0.86298037, 0.87323946, 0.8704573, 0.8810642, 0.8767171, 0.8708051, 0.86906624, 0.8765432, 0.87323946, 0.885759, 0.88436794, 0.8826291, 0.8544601, 0.8614154, 0.87289166, 0.88123804, 0.8817597, 0.8817597, 0.8836724, 0.8833246, 0.8864545, 0.88593286, 0.88123804, 0.87289166, 0.8788037, 0.86697966, 0.87532604, 0.885759, 0.88193357, 0.8833246, 0.8880195, 0.88402015, 0.88506347, 0.8794992, 0.87723875, 0.87323946, 0.86906624, 0.88210744, 0.8841941, 0.88089025, 0.88228136, 0.8941054, 0.88906276, 0.87132674, 0.8608938, 0.8633281, 0.86906624, 0.88402015, 0.8847157, 0.87254393, 0.8796731, 0.88036865, 0.8904538, 0.89306206, 0.8920188, 0.88315076, 0.89445317, 0.8941054, 0.8864545, 0.8868023, 0.8941054, 0.8845418, 0.8873239, 0.8923665, 0.8937576, 0.8986263, 0.8948009, 0.88836724, 0.88523734, 0.89027995, 0.88906276, 0.8847157, 0.8899322, 0.8970614, 0.87810814, 0.8767171, 0.88193357, 0.88715005, 0.8948009, 0.88436794, 0.88715005, 0.8833246, 0.8836724, 0.89845246, 0.89149714, 0.87289166, 0.8848896, 0.8908016, 0.89027995, 0.8838463, 0.8838463, 0.89306206, 0.88715005, 0.8939315, 0.88749784, 0.89062774, 0.8909755, 0.88002086, 0.88541126, 0.8908016, 0.8847157, 0.8838463, 0.89132327, 0.89027995, 0.8909755, 0.89653975, 0.8848896, 0.8968875, 0.8954964, 0.9034951, 0.90140843, 0.89932185, 0.8994957, 0.8908016, 0.89775693, 0.8998435, 0.8756738, 0.8814119, 0.87010956, 0.88819337, 0.88436794, 0.89740914, 0.8814119, 0.8892366, 0.8960181, 0.8796731, 0.8904538, 0.8841941, 0.88193357, 0.89340985, 0.8633281, 0.8801947, 0.8826291, 0.89932185, 0.89219266, 0.8963658, 0.9026256, 0.87810814, 0.8779343, 0.8716745, 0.8695879, 0.8888889, 0.8873239, 0.8847157, 0.8688924, 0.88941056, 0.8908016, 0.8695879, 0.8675013, 0.8706312, 0.88228136, 0.8754999, 0.8824552, 0.88210744, 0.89775693, 0.91236305, 0.893236, 0.8789776, 0.8878456, 0.86628413, 0.888715, 0.8897583, 0.90210396, 0.88941056, 0.90123457, 0.9090593, 0.89932185, 0.9034951, 0.893236, 0.8892366, 0.8972353, 0.90332115, 0.9031473, 0.8963658, 0.90245175, 0.91114587, 0.8960181, 0.89653975, 0.89427924, 0.8866284, 0.9054077, 0.8873239, 0.8956703, 0.8958442, 0.90732044, 0.9008868, 0.89966965, 0.90210396, 0.89445317, 0.8968875, 0.89845246, 0.89740914, 0.8904538, 0.88836724, 0.9034951, 0.9034951, 0.90210396, 0.8939315, 0.8951487, 0.9019301, 0.9008868, 0.8880195, 0.89775693, 0.9017562, 0.90210396, 0.9113198, 0.9000174, 0.9057555, 0.89775693, 0.8960181, 0.89827853, 0.9047122, 0.89775693, 0.9040167, 0.9041906, 0.8920188, 0.88836724, 0.8986263, 0.9019301, 0.8897583, 0.8941054, 0.90749437, 0.9038428, 0.9130586, 0.8989741, 0.9061033, 0.9031473, 0.9031473, 0.90245175, 0.90053904, 0.8892366, 0.9137541, 0.90732044, 0.918449, 0.91392803, 0.90210396, 0.9113198, 0.9069727, 0.9090593, 0.912537, 0.91236305, 0.9078421, 0.91079813, 0.9034951, 0.91636235, 0.92662144, 0.9057555, 0.912537, 0.9099287, 0.91358024, 0.90871155, 0.91062427, 0.91079813, 0.9181012, 0.90123457, 0.9137541, 0.90766823, 0.9097548, 0.9099287, 0.91740566, 0.9029734, 0.90245175, 0.91584074, 0.9134064, 0.90523386, 0.9085376, 0.9102765, 0.90923315, 0.91062427, 0.90679884, 0.91462356, 0.91636235, 0.916884, 0.9189706, 0.90749437, 0.90332115, 0.9038428, 0.90019125, 0.91392803, 0.9134064, 0.9161885, 0.9047122, 0.9054077, 0.90645105, 0.9069727, 0.90366894, 0.9102765, 0.908016, 0.91827506, 0.91549295, 0.90523386, 0.91584074, 0.9189706, 0.92070943, 0.90836376, 0.9156668, 0.922796, 0.92766476, 0.91062427, 0.91358024, 0.91271085, 0.9210572, 0.9147974, 0.9029734, 0.9041906, 0.90453833, 0.91062427, 0.9179273, 0.9040167, 0.90923315, 0.9132325, 0.9101026, 0.9208833, 0.91984004, 0.91079813, 0.92383933, 0.9200139, 0.912537, 0.91984004, 0.9137541, 0.91444963, 0.9156668, 0.91705793, 0.91636235, 0.9102765, 0.9078421, 0.9134064, 0.9200139, 0.9099287, 0.91427577, 0.918449, 0.91271085, 0.91757953, 0.9191445, 0.92453486, 0.918449, 0.9078421, 0.9153191, 0.9161885, 0.9193184, 0.9151452, 0.9189706, 0.9153191, 0.92053556, 0.9147974, 0.9156668, 0.9193184, 0.9071466, 0.92488265, 0.9069727, 0.9172318, 0.9224483, 0.92731696, 0.91984004, 0.9132325, 0.92366546, 0.9141019, 0.9141019, 0.92296994, 0.91636235, 0.90523386, 0.9151452, 0.9179273, 0.9118414, 0.91671014, 0.92453486, 0.9172318, 0.91114587, 0.9137541, 0.9149713, 0.92609984, 0.89932185, 0.9134064, 0.9048861, 0.9290558, 0.9085376, 0.910972, 0.9172318, 0.916884, 0.9081899, 0.91671014, 0.9149713, 0.91462356, 0.9116675, 0.922796, 0.910972, 0.91427577, 0.91705793, 0.9208833, 0.91062427, 0.9187967, 0.912537, 0.9130586, 0.92349154, 0.9149713, 0.91549295, 0.9019301, 0.90506, 0.9027995, 0.90958095, 0.9250565, 0.92296994, 0.90871155, 0.91862285, 0.9153191, 0.9193184, 0.9240132, 0.9153191, 0.8655886, 0.908016, 0.9081899, 0.90958095, 0.9137541, 0.9189706, 0.92070943, 0.9191445, 0.92662144, 0.92296994, 0.92679536, 0.9233177, 0.9200139, 0.9181012, 0.9201878, 0.9141019, 0.91705793, 0.9281864, 0.91444963, 0.9120153, 0.9026256, 0.9097548, 0.9149713, 0.91949224, 0.92070943, 0.92679536, 0.9203617, 0.93444616, 0.93166405, 0.91949224, 0.9099287, 0.91671014, 0.9340984, 0.9210572, 0.9179273, 0.92453486, 0.90923315, 0.9156668, 0.9141019, 0.9102765, 0.91271085, 0.91462356, 0.9179273, 0.92070943, 0.9189706, 0.9153191, 0.92349154, 0.9264476, 0.92366546, 0.9212311, 0.9287081, 0.9219266, 0.93288124, 0.93079466, 0.92383933, 0.9271431, 0.9181012, 0.91705793, 0.91584074, 0.9262737, 0.92296994, 0.92488265, 0.92609984, 0.92888194, 0.9233177, 0.9221005, 0.92366546, 0.9281864, 0.9283603, 0.9017562, 0.92296994, 0.91149366, 0.91549295, 0.92453486, 0.91984004, 0.9161885, 0.91671014, 0.9221005, 0.9300991, 0.9271431, 0.9247087, 0.924361, 0.92696923, 0.92731696, 0.9208833, 0.92609984, 0.9231438, 0.92696923, 0.92853415, 0.92349154, 0.9212311, 0.9311424, 0.91757953, 0.9212311, 0.924361, 0.9179273, 0.91584074, 0.930273, 0.9201878, 0.9311424, 0.9299252, 0.9314902, 0.9233177, 0.92731696, 0.92957747, 0.9222744, 0.92366546, 0.9212311, 0.916884, 0.9097548, 0.91462356, 0.922796, 0.92609984, 0.92453486, 0.9231438, 0.9203617, 0.9240132, 0.916884, 0.922796, 0.9271431, 0.92383933, 0.9240132, 0.92766476, 0.92731696, 0.93166405, 0.9300991, 0.9193184, 0.9208833, 0.9128847, 0.9240132, 0.899148, 0.9247087, 0.91862285, 0.9200139, 0.9221005, 0.92488265, 0.9262737, 0.9262737, 0.91775346, 0.92296994, 0.9191445, 0.922796, 0.9231438, 0.9233177, 0.91757953, 0.9241871, 0.92053556, 0.92679536, 0.93044686, 0.9179273, 0.91705793, 0.92296994, 0.9274909, 0.91705793, 0.9250565, 0.922796, 0.9299252, 0.9281864, 0.92679536, 0.92296994, 0.92453486, 0.9271431, 0.9187967, 0.922796, 0.91984004, 0.9215789, 0.9215789, 0.9247087, 0.91775346, 0.92731696, 0.92488265, 0.9231438, 0.9210572, 0.92175275, 0.9274909, 0.9221005, 0.9259259, 0.9224483, 0.9241871, 0.9210572, 0.92140496, 0.92540425, 0.9181012, 0.9193184, 0.9233177, 0.9212311, 0.92175275, 0.9193184, 0.91705793, 0.9283603, 0.92296994, 0.91827506, 0.92070943, 0.92766476, 0.92662144, 0.9250565, 0.9181012, 0.92366546, 0.92262214, 0.9240132, 0.92696923, 0.9241871, 0.9252304, 0.9191445, 0.91862285, 0.92662144, 0.9210572, 0.9334029, 0.92575204, 0.9161885, 0.92453486, 0.9193184, 0.9247087, 0.9187967, 0.91549295, 0.9247087, 0.9222744, 0.9241871, 0.9219266, 0.9262737, 0.9290558, 0.92696923, 0.9318379, 0.9262737, 0.9274909, 0.9241871, 0.93253344, 0.9193184, 0.91949224, 0.9299252, 0.91549295, 0.9259259, 0.92766476, 0.9187967, 0.92262214, 0.92853415, 0.91984004, 0.9278386, 0.92366546, 0.92575204, 0.9212311, 0.9247087, 0.92488265, 0.9215789, 0.93079466, 0.9240132, 0.92488265, 0.92349154, 0.92540425, 0.92366546, 0.93357676, 0.92975134, 0.9262737, 0.9181012, 0.9306208, 0.9294036, 0.93044686, 0.930273, 0.92140496, 0.92488265, 0.9278386, 0.92453486, 0.9255782, 0.92366546, 0.9231438, 0.9231438, 0.91549295, 0.9311424, 0.93079466, 0.92366546, 0.92262214, 0.9241871, 0.92296994, 0.92766476, 0.92679536, 0.9274909, 0.9231438, 0.9300991, 0.9247087, 0.92766476, 0.9306208, 0.92696923, 0.92957747, 0.92662144, 0.9224483, 0.92366546, 0.91705793, 0.9219266, 0.930273, 0.92957747, 0.92679536, 0.92609984, 0.9255782, 0.924361, 0.92453486, 0.9271431, 0.9196662, 0.92296994, 0.92366546, 0.9259259, 0.9314902, 0.9233177, 0.92696923, 0.9255782, 0.9196662, 0.9346201, 0.922796, 0.924361, 0.9231438, 0.9283603, 0.9309685, 0.9281864, 0.92696923, 0.92853415, 0.92888194, 0.92975134, 0.9221005, 0.9274909, 0.93201184, 0.9250565, 0.9196662, 0.9252304, 0.9299252, 0.9306208, 0.93270737, 0.9281864, 0.9349678, 0.9287081, 0.9250565, 0.92975134, 0.924361, 0.9262737, 0.92957747, 0.9281864, 0.9281864, 0.92383933, 0.9252304, 0.9306208, 0.92296994, 0.9281864, 0.92853415, 0.9314902, 0.9340984, 0.92853415, 0.92957747, 0.93166405, 0.92731696, 0.92609984, 0.92975134, 0.9264476, 0.9370544, 0.92662144, 0.9264476, 0.9334029, 0.9262737, 0.93253344, 0.9274909, 0.9292297, 0.9313163, 0.92488265, 0.9365328, 0.9274909, 0.93514174, 0.9290558, 0.93357676, 0.9294036, 0.92731696, 0.92957747, 0.9340984, 0.9334029, 0.93288124, 0.93253344, 0.9311424, 0.9300991, 0.9294036, 0.93044686, 0.922796, 0.9252304, 0.9294036, 0.92662144, 0.93201184, 0.9281864, 0.90366894, 0.90923315, 0.9130586, 0.92175275, 0.91740566, 0.92175275, 0.9255782, 0.92731696, 0.9255782, 0.9292297, 0.92679536, 0.9274909, 0.9278386, 0.9250565, 0.92540425, 0.9354895, 0.9292297, 0.9262737, 0.92696923, 0.9300991, 0.9283603, 0.9349678, 0.93253344, 0.9311424, 0.9337506, 0.9290558, 0.9300991, 0.922796, 0.924361, 0.92696923, 0.9259259, 0.9271431, 0.9292297, 0.9250565, 0.9231438, 0.9299252, 0.92575204, 0.9294036, 0.9201878, 0.92731696, 0.92853415, 0.9342723, 0.9300991, 0.9231438, 0.92679536, 0.9281864, 0.93444616, 0.9250565, 0.91949224, 0.9247087, 0.9201878, 0.9222744, 0.92853415, 0.92453486, 0.9247087, 0.92731696, 0.9311424, 0.9292297, 0.9318379, 0.92731696, 0.92383933, 0.9281864, 0.92679536, 0.924361, 0.9259259, 0.9247087, 0.9259259, 0.9252304, 0.9321857, 0.93392456, 0.93444616, 0.92453486, 0.9334029, 0.9219266, 0.92975134, 0.92957747, 0.92696923, 0.92888194, 0.93583727, 0.9280125, 0.92575204, 0.93201184, 0.9250565, 0.92731696, 0.92731696, 0.9274909, 0.9337506, 0.93288124, 0.9191445, 0.93479395, 0.93601114, 0.93514174, 0.9224483, 0.9250565, 0.92609984, 0.9280125, 0.92731696, 0.9280125, 0.92575204, 0.9354895, 0.93253344, 0.93044686, 0.9309685, 0.9212311, 0.9281864, 0.9294036, 0.924361, 0.9262737, 0.9271431, 0.9274909, 0.92888194, 0.92888194, 0.92488265, 0.92696923, 0.92975134, 0.92975134, 0.9292297, 0.9262737, 0.9294036, 0.9264476, 0.9386194, 0.9354895, 0.92731696, 0.92957747, 0.92679536, 0.922796, 0.93201184, 0.9280125, 0.9271431, 0.9278386, 0.92888194, 0.9334029, 0.9318379, 0.9287081, 0.9278386, 0.9287081, 0.9281864, 0.93514174, 0.92070943, 0.92609984, 0.9241871, 0.9210572, 0.9287081, 0.9259259, 0.93514174, 0.936185, 0.93079466, 0.93566334, 0.9311424, 0.933229, 0.933229]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7AXbWCH_bftp",
        "colab": {}
      },
      "source": [
        "saver = tf.compat.v1.train.Saver()\n",
        "# !mkdir model\n",
        "save_path = saver.save(sess, 'model4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MOjRD6QzxAaF",
        "outputId": "e6ff680a-ca18-41f6-8ee4-077b9b006bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "def freeze_graph(model_dir, output_node_names):\n",
        "    \"\"\"Extract the sub graph defined by the output nodes and convert\n",
        "    all its variables into constant\n",
        "    Args:\n",
        "        model_dir: the root folder containing the checkpoint state file\n",
        "        output_node_names: a string, containing all the output node's names,\n",
        "                            comma separated\n",
        "    \"\"\"\n",
        "    if not tf.io.gfile.exists(model_dir):\n",
        "        raise AssertionError(\n",
        "            \"Export directory doesn't exists. Please specify an export \"\n",
        "            \"directory: %s\" % model_dir)\n",
        "\n",
        "    if not output_node_names:\n",
        "        print(\"You need to supply the name of a node to --output_node_names.\")\n",
        "        return -1\n",
        "\n",
        "    # We retrieve our checkpoint fullpath\n",
        "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
        "    print(checkpoint)\n",
        "    input_checkpoint = checkpoint.model_checkpoint_path\n",
        "\n",
        "    # We precise the file fullname of our freezed graph\n",
        "    absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\n",
        "#     absolute_model_dir = 'model/'\n",
        "    output_graph = absolute_model_dir + \"/frozen_model.pb\"\n",
        "\n",
        "    # We clear devices to allow TensorFlow to control on which device it will load operations\n",
        "    clear_devices = True\n",
        "\n",
        "    # We start a session using a temporary fresh Graph\n",
        "    with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n",
        "        # We import the meta graph in the current default Graph\n",
        "        saver = tf.compat.v1.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
        "\n",
        "        # We restore the weights\n",
        "        saver.restore(sess, input_checkpoint)\n",
        "\n",
        "        # We use a built-in TF helper to export variables to constants\n",
        "        output_graph_def = tf.compat.v1.graph_util.convert_variables_to_constants(\n",
        "            sess, # The session is used to retrieve the weights\n",
        "            tf.compat.v1.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes\n",
        "            output_node_names.split(\",\") # The output node names are used to select the usefull nodes\n",
        "        )\n",
        "\n",
        "        # Finally we serialize and dump the output graph to the filesystem\n",
        "        with tf.io.gfile.GFile(output_graph, \"wb\") as f:\n",
        "            f.write(output_graph_def.SerializeToString())\n",
        "        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n",
        "\n",
        "    return\n",
        "\n",
        "freeze_graph('.', 'output')\n",
        "# !python freeze_graph.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_checkpoint_path: \"./model4\"\n",
            "all_model_checkpoint_paths: \"./model4\"\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from ./model4\n",
            "WARNING:tensorflow:From <ipython-input-15-756420e29a19>:43: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:359: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "INFO:tensorflow:Froze 8 variables.\n",
            "INFO:tensorflow:Converted 8 variables to const ops.\n",
            "1153 ops in the final graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cVA3TQWSb87Q",
        "colab": {}
      },
      "source": [
        "# !pip install pydrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nUB7sjPRicWh",
        "colab": {}
      },
      "source": [
        "# from pydrive.auth import GoogleAuth\n",
        "# from pydrive.drive import GoogleDrive\n",
        "# from google.colab import auth\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# # Authenticate and create the PyDrive client.\n",
        "# # This only needs to be done once in a notebook.\n",
        "# auth.authenticate_user()\n",
        "# gauth = GoogleAuth()\n",
        "# gauth.credentials = GoogleCredentials.get_application_default()\n",
        "# drive = GoogleDrive(gauth)\n",
        "\n",
        "# uploaded = drive.CreateFile({'title': 'poseLSTM2.data-00000-of-00001'})\n",
        "# uploaded.SetContentFile('model/poseLSTM2.data-00000-of-00001')\n",
        "# uploaded.Upload()\n",
        "# print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "\n",
        "# uploaded = drive.CreateFile({'title': 'poseLSTM2.index'})\n",
        "# uploaded.SetContentFile('model/poseLSTM2.index')\n",
        "# uploaded.Upload()\n",
        "# print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "\n",
        "# uploaded = drive.CreateFile({'title': 'poseLSTM2.meta'})\n",
        "# uploaded.SetContentFile('model/poseLSTM2.meta')\n",
        "# uploaded.Upload()\n",
        "# print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "\n",
        "# uploaded = drive.CreateFile({'title': 'checkpoint'})\n",
        "# uploaded.SetContentFile('model/checkpoint')\n",
        "# uploaded.Upload()\n",
        "# print('Uploaded file with ID {}'.format(uploaded.get('id')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bdMeTU1v3vbJ",
        "colab": {}
      },
      "source": [
        "# tf.saved_model.simple_save(sess,\n",
        "#                           export_dir='newpb3',\n",
        "#                           inputs={'Input': x},\n",
        "#                           outputs={'output': y})\n",
        "\n",
        "# #saved\n",
        "# uploaded = drive.CreateFile({'title': 'poseLSTM2.pb'})\n",
        "# uploaded.SetContentFile('newpb3/saved_model.pb')\n",
        "# uploaded.Upload()\n",
        "# print('Uploaded file with ID {}'.format(uploaded.get('id')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "35ABMFpQDawh",
        "colab": {}
      },
      "source": [
        "# import tensorflow.python.saved_model\n",
        "# from tensorflow.python.saved_model import tag_constants\n",
        "# from tensorflow.python.saved_model.signature_def_utils_impl import predict_signature_def\n",
        "\n",
        "# builder = tf.saved_model.builder.SavedModelBuilder('./SavedModel/')\n",
        "# signature = predict_signature_def(inputs={'input': x},\n",
        "#                                   outputs={'output': y})\n",
        "\n",
        "# builder.add_meta_graph_and_variables(sess,\n",
        "#                                    [tf.saved_model.tag_constants.SERVING],\n",
        "#                                    signature_def_map={'predict':signature},\n",
        "#                                     strip_default_attrs=True)\n",
        "# builder.save()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sxm6HoqHQn7O",
        "colab": {}
      },
      "source": [
        "# with tf.Session(graph=tf.Graph()) as sess:\n",
        "#     tf.saved_model.loader.load(sess, [\"serve\"], './SavedModel/')\n",
        "#     graph = tf.get_default_graph()\n",
        "#     print(graph.get_operations())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E6bM9AjkVbY8",
        "colab": {}
      },
      "source": [
        "# tf.train.write_graph(sess.graph.as_graph_def(), '.', 'model2.pbtxt', as_text=True)\n",
        "\n",
        "# from tensorflow.python.tools import freeze_graph\n",
        "\n",
        "# freeze_graph.freeze_graph('model2.pbtxt', \"\", False,\n",
        "#                           'model/checkpoint', \"output\",\n",
        "#                            \"save/restore_all\", \"save/Const:0\",\n",
        "#                            'Model.pb', True, \"\"\n",
        "#                          )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xcj96IOXm654",
        "colab": {}
      },
      "source": [
        "# frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
        "#         sess,\n",
        "#         sess.graph_def,\n",
        "#         ['output'])\n",
        "# with open('output_graph2.pb', 'wb') as f:\n",
        "#   f.write(frozen_graph_def.SerializeToString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g_ZZTOoa3mv0",
        "colab": {}
      },
      "source": [
        "# saver = tf.compat.v1.train.Saver()\n",
        "\n",
        "# # Later, launch the model, use the saver to restore variables from disk, and\n",
        "# # do some work with the model.\n",
        "# with tf.compat.v1.Session() as sess:\n",
        "#   # Restore variables from disk.\n",
        "#   saver.restore(sess, \"model3.ckpt\")\n",
        "#   print(\"Model restored.\")\n",
        "#   print(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2KRBnB467-xf",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "# files.download('model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "agyQaCZdfw9Y",
        "colab": {}
      },
      "source": [
        "# data = np.load('data.npy')\n",
        "# with tf.compat.v1.Session() as sess:\n",
        "#   y_out = sess.run(\n",
        "#     [pred],\n",
        "#     feed_dict={x: [data]}\n",
        "#   )\n",
        "#   print(y_out)\n",
        "#   print(np.argmax(y_out))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MGwwyK-9pjin",
        "colab": {}
      },
      "source": [
        "# !tf_upgrade_v2 \\\n",
        "#   --infile \"/content/drive/My DriveposeLSTM.ipynb \\\n",
        "#   --outfile poseLSTM2.ipynb \\\n",
        "#   --reportfile report.txt"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}